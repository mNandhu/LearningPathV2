[
    {
        "id": "intro_python",
        "name": "Python (programming language)",
        "text": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\nPython is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n\n\n== History ==\n\nPython was conceived in the late 1980s by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL, capable of exception handling and interfacing with the Amoeba operating system. Its implementation began in December 1989. Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his \"permanent vacation\" from his responsibilities as Python's \"benevolent dictator for life\" (BDFL), a title the Python community bestowed upon him to reflect his long-term commitment as the project's chief decision-maker (he has since come out of retirement and is self-titled \"BDFL-emeritus\"). In January 2019, active Python core developers elected a five-member Steering Council to lead the project.\nThe name Python is said to come from the British comedy series Monty Python's Flying Circus.\nPython 2.0 was released on 16 October 2000, with many major new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 2.7's end-of-life was initially set for 2015, then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3. It no longer receives security patches or updates. While Python 2.7 and older versions are officially unsupported, a different unofficial Python implementation, PyPy, continues to support Python 2, i.e. \"2.7.18+\" (plus 3.10), with the plus meaning (at least some) \"backported security updates\".\nPython 3.0 was released on 3 December 2008, with some new semantics and changed syntax. At least every Python release since (now unsupported) 3.5 has added some syntax to the language, and a few later releases have dropped outdated modules, and changed semantics, at least in a minor way.\nAs of 12 March 2025, Python 3.13 is the latest stable release. Python 3.13 currently receives full bug-fix and security updates, while Python 3.12—released in October 2023—will have active bug-fix support only until April 2025. Python 3.9 is the oldest supported version of Python (albeit in the 'security support' phase), due to Python 3.8 reaching end-of-life. Starting with 3.13, it and later versions have 2 years of full support (up from one and a half), followed by 3 years of security support (for same total support as before).\nSecurity updates were expedited in 2021 (and again twice in 2022, and more fixed in 2023 and in September 2024 for Python 3.12.6 down to 3.8.20), since all Python versions were insecure (including 2.7) because of security issues leading to possible remote code execution and web-cache poisoning.   \nPython 3.10 added the | union type operator and the match and case keywords (for structural pattern matching statements). 3.11 expanded exception handling functionality. Python 3.12 added the new keyword type. Notable changes in 3.11 from 3.10 include increased program execution speed and improved error reporting. Python 3.11 claims to be between 10 and 60% faster than Python 3.10, and Python 3.12 adds another 5% on top of that. It also has improved error messages (again improved in 3.14), and many other changes.\nPython 3.13 introduces more syntax for types, a new and improved interactive interpreter (REPL), featuring multi-line editing and color support; an incremental garbage collector (producing shorter pauses for collection in programs with a lot of objects, and addition to the improved speed in 3.11 and 3.12),  and an experimental just-in-time (JIT) compiler (such features, can/needs to be enabled specifically for the increase in speed), and an experimental free-threaded build mode, which disables the global interpreter lock (GIL), allowing threads to run more concurrently, that latter feature enabled with python3.13t or python3.13t.exe.\nPython 3.13 introduces some change in behavior, i.e. new \"well-defined semantics\", fixing bugs (plus many removals of deprecated classes, functions and methods, and removed some of the C API and outdated modules): \"The  [old] implementation of locals() and frame.f_locals is slow, inconsistent and buggy [and it] has many corner cases and oddities. Code that works around those may need to be changed. Code that uses locals() for simple templating, or print debugging, will continue to work correctly.\"\nPython 3.13 introduces the experimental free-threaded build mode, which disables the Global Interpreter Lock (GIL), a feature of CPython that previously prevented multiple threads from executing Python bytecode simultaneously. This optional build, introduced through PEP 703, enables better exploitation of multi-core CPUs. By allowing multiple threads to run Python code in parallel, the free-threaded mode addresses long-standing performance bottlenecks associated with the GIL, offering a new path for parallelism in Python without resorting to multiprocessing or external concurrency frameworks.\nSome (more) standard library modules and many deprecated classes, functions and methods, will be removed in Python 3.15 or 3.16.\nPython 3.11 adds Sigstore digital verification signatures for all CPython artifacts (in addition to PGP). Since use of PGP has been criticized by security practitioners,  Python is moving to Sigstore exclusively and dropping PGP from 3.14.\nPython 3.14 is now in alpha 3; regarding possible change to annotations: \"In Python 3.14, from __future__ import annotations will continue to work as it did before, converting annotations into strings.\"\nPEP 711 proposes PyBI: a standard format for distributing Python Binaries.\nPython 3.15 will \"Make UTF-8 mode default\", the mode exists in all current Python versions, but currently needs to be opted into. UTF-8 is already used, by default, on Windows (and elsewhere), for most things, but e.g. to open files it's not and enabling also makes code fully cross-platform, i.e. use UTF-8 for everything on all platforms.\n\n\n== Design philosophy and features ==\nPython is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming (including metaprogramming and metaobjects). Many other paradigms are supported via extensions, including design by contract and logic programming. Python is often referred to as a 'glue language' because it can seamlessly integrate components written in other languages.\nPython uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management. It uses dynamic name resolution (late binding), which binds method and variable names during program execution.\nIts design offers some support for functional programming in the Lisp tradition. It has filter,mapandreduce functions; list comprehensions, dictionaries, sets, and generator expressions. The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.\nIts core philosophy is summarized in the Zen of Python (PEP 20), which includes aphorisms such as:\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nReadability counts.\nHowever, Python features regularly violate these principles and have received criticism for adding unnecessary language bloat. Responses to these criticisms are that the Zen of Python is a guideline rather than a rule. The addition of some new features had been so controversial that Guido van Rossum resigned as Benevolent Dictator for Life following vitriol over the addition of the assignment expression operator in Python 3.8.\nNevertheless, rather than building all of its functionality into its core, Python was designed to be highly extensible via modules. This compact modularity has made it particularly popular as a means of adding programmable interfaces to existing applications. Van Rossum's vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which espoused the opposite approach.\nPython claims to strive for a simpler, less-cluttered syntax and grammar while giving developers a choice in their coding methodology. In contrast to Perl's \"there is more than one way to do it\" motto, Python embraces a \"there should be one—and preferably only one—obvious way to do it.\" philosophy. In practice, however, Python provides many ways to achieve the same task. There are, for example, at least three ways to format a string literal, with no certainty as to which one a programmer should use. Alex Martelli, a Fellow at the Python Software Foundation and Python book author, wrote: \"To describe something as 'clever' is not considered a compliment in the Python culture.\"\nPython's developers usually strive to avoid premature optimization and reject patches to non-critical parts of the CPython reference implementation that would offer marginal increases in speed at the cost of clarity. Execution speed can be improved by moving speed-critical functions to extension modules written in languages such as C, or by using a just-in-time compiler like PyPy. It is also possible to cross-compile to other languages, but it either doesn't provide the full speed-up that might be expected, since Python is a very dynamic language, or a restricted subset of Python is compiled, and possibly semantics are slightly changed.\nPython's developers aim for it to be fun to use. This is reflected in its name—a tribute to the British comedy group Monty Python—and in occasionally playful approaches to tutorials and reference materials, such as the use of the terms \"spam\" and \"eggs\" (a reference to a Monty Python sketch) in examples, instead of the often-used \"foo\" and \"bar\". A common neologism in the Python community is pythonic, which has a wide range of meanings related to program style. \"Pythonic\" code may use Python idioms well, be natural or show fluency in the language, or conform with Python's minimalist philosophy and emphasis on readability.\n\n\n== Syntax and semantics ==\n\nPython is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal.\n\n\n=== Indentation ===\n\nPython uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block. Thus, the program's visual structure accurately represents its semantic structure. This feature is sometimes termed the off-side rule. Some other languages use indentation this way; but in most, indentation has no semantic meaning. The recommended indent size is four spaces.\n\n\n=== Statements and control flow ===\nPython's statements include:\n\nThe assignment statement, using a single equals sign =\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else if)\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block\nThe while statement, which executes a block of code as long as its condition is true\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups); it also ensures that clean-up code in a finally block is always run regardless of how the block exits\nThe raise statement, used to raise a specified exception or re-raise a caught exception\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming\nThe def statement, which defines a function or method\nThe with statement, which encloses a code block within a context manager (for example, acquiring a lock before it is run, then releasing the lock; or opening and closing a file), allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try/finally idiom\nThe break statement, which exits a loop\nThe continue statement, which skips the rest of the current iteration and continues with the next\nThe del statement, which removes a variable—deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined\nThe pass statement, serving as a NOP, syntactically needed to create an empty code block\nThe assert statement, used in debugging to check for conditions that should apply\nThe yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines\nThe return statement, used to return a value from a function\nThe import and from statements, used to import modules whose functions or variables can be used in the current program\nThe match and case statements, an analog of the switch statement construct, that compares an expression against one or more cases as a control-of-flow measure.\nThe assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing—in contrast to statically-typed languages, where each variable may contain only a value of a certain type.\nPython does not support tail call optimization or first-class continuations, and, according to Van Rossum, it never will. However, better support for coroutine-like functionality is provided by extending Python's generators. Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python 2.5 on, it is possible to pass data back into a generator function; and from version 3.3, it can be passed through multiple stack levels.\n\n\n=== Expressions ===\nPython's expressions include:\n\nThe +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of divisions in Python: floor division (or integer division) // and floating-point / division. Python uses the ** operator for exponentiation.\nPython uses the + operator for string concatenation. Python uses the * operator for duplicating a string a specified number of times.\nThe @ infix operator is intended to be used by libraries such as NumPy for matrix multiplication.\nThe syntax :=, called the \"walrus operator\", was introduced in Python 3.8. It assigns values to variables as part of a larger expression.\nIn Python, == compares by value. Python's is operator may be used to compare object identities (comparison by reference), and comparisons may be chained—for example, a <= b <= c.\nPython uses and, or, and not as Boolean operators.\nPython has a type of expression named a list comprehension, and a more general expression named a generator expression.\nAnonymous functions are implemented using lambda expressions; however, there may be only one expression in each body.\nConditional expressions are written as x if c else y (different in order of operands from the c ? x : y operator common to many other languages).\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as keys of dictionaries, provided all of the tuple's elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. Thus, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5), which is then assigned back to t—thereby effectively \"modifying the contents\" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.\nPython features sequence unpacking where multiple expressions, each evaluating to anything that can be assigned (to a variable, writable property, etc.) are associated in an identical manner to that forming tuple literals—and, as a whole, are put on the left-hand side of the equal sign in an assignment statement. The statement expects an iterable object on the right-hand side of the equal sign that produces the same number of values as the provided writable expressions; when iterated through them, it assigns each of the produced values to the corresponding expression on the left.\nPython has a \"string format\" operator % that functions analogously to printf format strings in C—e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python 2.6+ and 3+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2). Python 3.6 added \"f-strings\": spam = \"blah\"; eggs = 2; f'spam={spam} eggs={eggs}'.\nStrings in Python can be concatenated by \"adding\" them (with the same operator as for adding integers and floats), e.g. \"spam\" + \"eggs\" returns \"spameggs\". If strings contain numbers, they are added as strings rather than integers, e.g. \"2\" + \"2\" returns \"22\".\nPython has various string literals:\nDelimited by single or double quotes; unlike in Unix shells, Perl, and Perl-influenced languages, single and double quotes work the same. Both use the backslash (\\) as an escape character. String interpolation became available in Python 3.6 as \"formatted string literals\".\nTriple-quoted (beginning and ending with three single or double quotes), which may span multiple lines and function like here documents in shells, Perl, and Ruby.\nRaw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. (Compare \"@-quoting\" in C#.)\nPython has array index and array slicing expressions in lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted—for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\n\nList comprehensions vs. for-loops\nConditional expressions vs. if blocks\nThe eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former is for expressions, the latter is for statements\nStatements cannot be a part of an expression—so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement.\n\n\n=== Methods ===\nMethods on objects are functions attached to the object's class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, Ruby). Python also provides methods, often called dunder methods (due to their names beginning and ending with double-underscores), to allow user-defined classes to modify how they are handled by native operations including length, comparison, in arithmetic operations and type conversion.\n\n\n=== Typing ===\n\nPython uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that it is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.\nPython allows programmers to define their own types using classes, most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.\nBefore version 3.0, Python had two kinds of classes (both using the same syntax):  old-style and new-style; current Python versions only support the semantics of the new style.\nPython supports optional type annotations. These annotations are not enforced by the language, but may be used by external tools such as mypy to catch errors. Mypy also supports a Python compiler called mypyc, which leverages type annotations for optimization.\n\n\n=== Arithmetic operations ===\nPython has the usual symbols for arithmetic operators (+, -, *, /), the floor division operator // and the modulo operation % (where the remainder can be negative, e.g. 4 % -3 == -2). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a matrix‑multiplication operator @ . These operators work like in traditional math; with the same precedence rules, the operators infix (+ and - can also be unary to represent positive and negative numbers respectively).\nThe division between integers produces floating-point results. The behavior of division has changed significantly over time:\n\nCurrent Python (i.e. since 3.0) changed / to always be floating-point division, e.g. 5/2 == 2.5.\nThe floor division // operator was introduced. So 7//3 == 2, -7//3 == -3, 7.5//3 == 2.0 and -7.5//3 == -3.0. Adding from __future__ import division causes a module used in Python 2.7 to use Python 3.0 rules for division (see above).\nIn Python terms, / is true division (or simply division), and // is floor division. / before version 3.0 is classic division.\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a + b)//b == a//b + 1 is always true. It also means that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.\nPython provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses round to even: round(1.5) and round(2.5) both produce 2. Versions before 3 used round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.\nPython allows Boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.\nPython uses arbitrary-precision arithmetic for all integer operations. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes. The Fraction class in the fractions module provides arbitrary precision for rational numbers.\nDue to Python's extensive mathematics library and the third-party library NumPy, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.\n\n\n=== Function syntax ===\nFunctions are created in Python using the def keyword. In Python, you define the function as if you were calling it, by typing the function name and then the attributes required. Here is an example of a function that will print whatever is given:If you want the attribute to have a set value if no value is given, use the variable-defining syntax inside the function definition.\n\n\n== Programming examples ==\n\"Hello, World!\" program:\n\nProgram to calculate the factorial of a positive integer:\n\n\n== Libraries ==\nPython's large standard library is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals, manipulating regular expressions, and unit testing.\nSome parts of the standard library are covered by specifications—for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333—but most are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\nAs of 13 March 2025, the Python Package Index (PyPI), the official repository for third-party Python software, contains over 614,339 packages with a wide range of functionality, including:\n\n\n== Development environments ==\n\nMost Python implementations (including CPython) include a read–eval–print loop (REPL), permitting them to function as a command line interpreter for which users enter statements sequentially and receive results immediately.\nPython also comes with an Integrated development environment (IDE) called IDLE, which is more beginner-oriented.\nOther shells, including IDLE and IPython, add further abilities such as improved auto-completion, session state retention, and syntax highlighting.\nAs well as standard desktop integrated development environments including PyCharm, IntelliJ Idea, Visual Studio Code etc, there are web browser-based IDEs, including SageMath, for developing science- and math-related programs; Jupyter Notebooks, an open-source interactive computing platform; PythonAnywhere, a browser-based IDE and hosting environment; and Canopy IDE, a commercial IDE emphasizing scientific computing.\n\n\n== Implementations ==\n\n\n=== Reference implementation ===\nCPython is the reference implementation of Python. It is written in C, meeting the C89 standard (Python 3.11 uses C11) with several select C99 features. CPython includes its own C extensions, but third-party extensions are not limited to older C versions—e.g. they can be implemented with C11 or C++. CPython compiles Python programs into an intermediate bytecode which is then executed by its virtual machine. CPython is distributed with a large standard library written in a mixture of C and native Python, and is available for many platforms, including Windows (starting with Python 3.9, the Python installer deliberately fails to install on Windows 7 and 8; Windows XP was supported until Python 3.5) and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python 3.9.1, with experimental installer), with unofficial support for VMS. Platform portability was one of its earliest priorities. (During Python 1 and 2 development, even OS/2 and Solaris were supported, but support has since been dropped for many platforms.)\nAll current Python versions (i.e. since 3.7) only support operating systems with multi-threading support.\n\n\n=== Other implementations ===\nAll alternative implementations have at least slightly different semantics (e.g. may have unordered dictionaries, unlike all current Python versions), e.g. with the larger Python ecosystem, such as with supporting the C Python API of with PyPy:\n\nPyPy is a fast, compliant interpreter of Python 2.7 and  3.10. Its just-in-time compiler often brings a significant speed improvement over CPython, but some libraries written in C cannot be used with it. It has e.g. RISC-V support.\nCodon is a language with an ahead-of-time (AOT) compiler, that (AOT) compiles a statically-typed Python-like language with \"syntax and semantics are nearly identical to Python's, there are some notable differences\" e.g. it uses 64-bit machine integers, for speed, not arbitrary like Python, and it claims speedups over CPython are usually on the order of 10–100x. It compiles to machine code (via LLVM) and supports native multithreading.  Codon can also compile to Python extension modules that can be imported and used from Python.\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the call stack in the same way, thus allowing massively concurrent programs. PyPy also has a stackless version.\nMicroPython and CircuitPython are Python 3 variants optimized for microcontrollers, including Lego Mindstorms EV3.\nPyston is a variant of the Python runtime that uses just-in-time compilation to speed up the execution of Python programs.\nCinder is a performance-oriented fork of CPython 3.8 that contains a number of optimizations, including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.\nSnek Embedded Computing Language (compatible with e.g. 8-bit AVR microcontrollers such as ATmega 328P-based Arduino, as well as larger ones compatible with MicroPython) \"is Python-inspired, but it is not Python. It is possible to write Snek programs that run under a full Python system, but most Python programs will not run under Snek.\" It is an imperative language not including OOP / classes, unlike Python, and simplifying to one number type with 32-bit single-precision (similar to JavaScript, except smaller).\n\n\n=== No longer supported implementations ===\nOther just-in-time Python compilers have been developed, but are now unsupported:\n\nGoogle began a project named Unladen Swallow in 2009, with the aim of speeding up the Python interpreter five-fold by using the LLVM, and of improving its multithreading ability to scale to thousands of cores, while ordinary implementations suffer from the global interpreter lock.\nPsyco is a discontinued just-in-time specializing compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialized for certain data types and is faster than the standard Python code. Psyco does not support Python 2.7 or later.\nPyS60 was a Python 2 interpreter for Series 60 mobile phones released by Nokia in 2005. It implemented many of the modules from the standard library and some additional modules for integrating with the Symbian operating system. The Nokia N900 also supports Python with GTK widget libraries, enabling programs to be written and run on the target device.\n\n\n=== Cross-compilers to other languages ===\nThere are several compilers/transpilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\n\nBrython, Transcrypt and Pyjs (latest release in 2012) compile Python to JavaScript.\nCython compiles (a superset of) Python to C. The resulting code is also usable with Python via direct C-level API calls into the Python interpreter.\nPyJL compiles/transpiles a subset of Python to \"human-readable, maintainable, and high-performance Julia source code\". Despite claiming high performance, no tool can claim to do that for arbitrary Python code; i.e. it's known not possible to compile to a faster language or machine code. Unless semantics of Python are changed, but in many cases speedup is possible with few or no changes in the Python code. The faster Julia source code can then be used from Python, or compiled to machine code, and based that way.\nNuitka compiles Python into C. It works with Python 3.4 to 3.12 (and 2.6 and 2.7), for Python's main supported platforms (and Windows 7 or even Windows XP) and for Android. It claims complete support for Python 3.10,  some support for 3.11 and 3.12  and experimental support for Python 3.13. It supports macOS including Apple Silicon-based.  It's a free compiler, though it also has commercial add-ons (e.g. for hiding source code).\nNumba is used from Python, as a tool (enabled by adding a decorator to relevant Python code), a JIT compiler that translates a subset of Python and NumPy code into fast machine code.\nPythran compiles a subset of Python 3 to C++ (C++11).\nRPython can be compiled to C, and is used to build the PyPy interpreter of Python.\nThe Python → 11l → C++ transpiler compiles a subset of Python 3 to C++ (C++17).\nSpecialized:\n\nMyHDL is a Python-based hardware description language (HDL), that converts MyHDL code to Verilog or VHDL code.\nOlder projects (or not to be used with Python 3.x and latest syntax):\n\nGoogle's Grumpy (latest release in 2017) transpiles Python 2 to Go.\nIronPython allows running Python 2.7 programs (and an alpha, released in 2021, is also available for \"Python 3.4, although features and behaviors from later versions may be included\") on the .NET Common Language Runtime.\nJython compiles Python 2.7 to Java bytecode, allowing the use of the Java libraries from a Python program.\nPyrex (latest release in 2010) and Shed Skin (latest release in 2013) compile to C and C++ respectively.\n\n\n=== Performance ===\nPerformance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy '13. Python's performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game.\nThere are some ways of optimizing performance due to the slower nature of Python being an interpreted language. These optimizations use different strategies or tools:\n\nJust-in-time compilation: Compiling code just before it is executed dynamically. These implementations are seen in libraries like Numba and PyPy.\nStatic compilation: Code gets compiled into machine code before execution. An example of this is Cython which compiles Python into C.\nConcurrency and Parallelism: Multiple tasks can be run at the same time. Python contains modules like `multiprocessing` to allow for this form of parallelism. Additionally, it helps to overcome limitations of the Global Interpreter Lock (GIL) in CPU tasks.\nEfficient Data Structures: Using data types such as Set for membership tests or deque from collections for queue operations can also improve performance.\n\n\n== Development ==\nPython's development is conducted largely through the Python Enhancement Proposal (PEP) process, the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions. Python coding style is covered in PEP 8. Outstanding PEPs are reviewed and commented on by the Python community and the steering council.\nEnhancement of the language corresponds with the development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language's development. Specific issues were originally discussed in the Roundup bug tracker hosted at by the foundation. In 2022, all issues and discussions were migrated to GitHub. Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017.\nCPython's public releases come in three types, distinguished by which part of the version number is incremented:\n\nBackward-incompatible versions, where code is expected to break and needs to be manually ported. The first part of the version number is incremented. These releases happen infrequently—version 3.0 was released 8 years after 2.0. According to Guido van Rossum, a version 4.0 is very unlikely to ever happen.\nMajor or \"feature\" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python 3.9, these releases are expected to happen annually. Each major version is supported by bug fixes for several years after its release.\nBug fix releases, which introduce no new features, occur about every 3 months and are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented.\nMany alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for each release, they are often delayed if the code is not ready. Python's development team monitors the state of the code by running the large unit test suite during development.\nThe major academic conference on Python is PyCon. There are also special Python mentoring programs, such as PyLadies.\nPython 3.12 removed wstr meaning Python extensions need to be modified, and 3.10 added pattern matching to the language.\nPython 3.12 dropped some outdated modules, and more will be dropped in the future, deprecated as of 3.13; already deprecated array 'u' format code will emit DeprecationWarning since 3.13 and will be removed in Python 3.16. The 'w' format code should be used instead. Part of ctypes is also deprecated and http.server.CGIHTTPRequestHandler will emit a DeprecationWarning, and will be removed in 3.15. Using that code already has a high potential for both security and functionality bugs. Parts of the typing module are deprecated, e.g. creating a typing.NamedTuple class using keyword arguments to denote the fields and such (and more) will be disallowed in Python 3.15.\n\n\n== API documentation generators ==\nTools that can generate documentation for Python API include pydoc (available as part of the standard library), Sphinx, Pdoc and its forks, Doxygen and Graphviz, among others.\n\n\n== Naming ==\nPython's name is derived from the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture; for example, the metasyntactic variables often used in Python literature are spam and eggs instead of the traditional foo and bar. The official Python documentation also contains various references to Monty Python routines. Users of Python are sometimes referred to as \"Pythonistas\".\nThe prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of Simple DirectMedia Layer to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python.\n\n\n== Popularity ==\nSince 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where as of December 2022 it was the most popular language (ahead of C, C++, and Java). It was selected as Programming Language of the Year (for \"the highest rise in ratings in a year\") in 2007, 2010, 2018, and 2020 (the only language to have done so four times as of 2020). In the TIOBE Index, monthly rankings are based on the volume of searches for programming languages on Google, Amazon, Wikipedia, Bing, and 20 other platforms. According to the accompanying graph, Python has shown a marked upward trend since the early 2000s, eventually passing long-established languages such as C, C++, and Java. This progression can be attributed to Python’s readable syntax, comprehensive standard library, and application in data science and machine learning fields.\n\nLarge organizations that use Python include Wikipedia, Google, Yahoo!, CERN, NASA, Facebook, Amazon, Instagram, Spotify, and some smaller entities like Industrial Light & Magic and ITA. The social news networking site Reddit was written mostly in Python. Organizations that partially use Python include Discord and Baidu.\n\n\n== Uses ==\n\nPython can serve as a scripting language for web applications, e.g. via mod_wsgi for the Apache webserver. With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle, and Zope support developers in the design and maintenance of complex applications. Pyjs and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as a data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.\nLibraries such as NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing, with specialized libraries such as Biopython and Astropy providing domain-specific functionality. SageMath is a computer algebra system with a notebook interface programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus. OpenCV has Python bindings with a rich set of features for computer vision and image processing.\nPython is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, scikit-learn and the Logic language ProbLog. As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing.\nThe combination of Python and Prolog has proved to be particularly useful for AI applications, with Prolog providing knowledge representation and reasoning capabilities. The Janus system, in particular, exploits the similarities between these two languages,\nin part because of their use of dynamic typing, and the simple recursive nature of their\ndata structures. Typical applications of this combination include  natural language processing, visual query\nanswering, geospatial reasoning, and handling of semantic web data.\nThe Natlog system, implemented in Python, uses Definite Clause Grammars (DCGs) as prompt generators for text-to-text generators like GPT3 and text-to-image generators like DALL-E or Stable Diffusion.\nPython can also be used for graphical user interface (GUI) by using libraries like Tkinter.\nPython is embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modelers like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP, Inkscape, Scribus and Paint Shop Pro, and musical notation programs like scorewriter and capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS. It has also been used in several video games, and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.\nMany operating systems include Python as a standard component. It ships with most Linux distributions, AmigaOS 4 (using Python 2.7), FreeBSD (as a package), NetBSD, and OpenBSD (as a package) and can be used from the command line (terminal). Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora Linux use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.\nPython is used extensively in the information security industry, including in exploit development.\nMost of the Sugar software for the One Laptop per Child XO, developed at Sugar Labs as of 2008, is written in Python. The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.\nLibreOffice includes Python and intends to replace Java with Python. Its Python Scripting Provider is a core feature since Version 4.0 from 7 February 2013.\n\n\n== Languages influenced by Python ==\nPython's design and philosophy have influenced many other programming languages:\n\nBoo uses indentation, a similar syntax, and a similar object model.\nCobra uses indentation and a similar syntax, and its Acknowledgements document lists Python first among languages that influenced it.\nCoffeeScript, a programming language that cross-compiles to JavaScript, has Python-inspired syntax.\nECMAScript–JavaScript borrowed iterators and generators from Python.\nGDScript, a scripting language very similar to Python, built-in to the Godot game engine.\nGo is designed for the \"speed of working in a dynamic language like Python\" and shares the same syntax for slicing arrays.\nGroovy was motivated by the desire to bring the Python design philosophy to Java.\nJulia was designed to be \"as usable for general programming as Python\".\nMojo is a non-strict superset of Python (e.g. still missing classes, and adding e.g. struct).\nNim uses indentation and similar syntax.\nRuby's creator, Yukihiro Matsumoto, has said: \"I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That's why I decided to design my own language.\"\nSwift, a programming language developed by Apple, has some Python-inspired syntax.\nKotlin blends Python and Java features, minimizing boilerplate code for enhanced developer efficiency.\nPython's development practices have also been emulated by other languages. For example, the practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python, a PEP) is also used in Tcl, Erlang, and Swift.\n\n\n== See also ==\n\nPython syntax and semantics\npip (package manager)\nList of programming languages\nHistory of programming languages\nComparison of programming languages\n\n\n== References ==\n\n\n=== Sources ===\n\"Python for Artificial Intelligence\". Python Wiki. 19 July 2012. Archived from the original on 1 November 2012. Retrieved 3 December 2012.\nPaine, Jocelyn, ed. (August 2005). \"AI in Python\". AI Expert Newsletter. Amzi!. Archived from the original on 26 March 2012. Retrieved 11 February 2012.\n\"PyAIML 0.8.5 : Python Package Index\". Pypi.python.org. Retrieved 17 July 2013.\nRussell, Stuart J. & Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, NJ: Prentice Hall. ISBN 978-0-13-604259-4.\n\n\n== Further reading ==\nDowney, Allen (July 2024). Think Python: How to Think Like a Computer Scientist (3rd ed.). O'Reilly Media. ISBN 978-1098155438.\nLutz, Mark (2013). Learning Python (5th ed.). O'Reilly Media. ISBN 978-0-596-15806-4.\nSummerfield, Mark (2009). Programming in Python 3 (2nd ed.). Addison-Wesley Professional. ISBN 978-0-321-68056-3.\nRamalho, Luciano (May 2022). Fluent Python. O'Reilly Media. ISBN 978-1-4920-5632-4.\n\n\n== External links ==\n\nOfficial website \nThe Python Tutorial"
    },
    {
        "id": "data_structures",
        "name": "Data structure",
        "text": "In computer science, a data structure is a data organization and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.\n\n\n== Usage ==\nData structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.\nDifferent types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.\nData structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.\n\n\n== Implementation ==\nData structures can be implemented using a variety of programming languages and techniques, but they all share the common goal of efficiently organizing and storing data. Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. This approach to data structuring has profound implications for the efficiency and scalability of algorithms. For instance, the contiguous memory allocation in arrays facilitates rapid access and modification operations, leading to optimized performance in sequential data processing scenarios. \nThe implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).\n\n\n== Examples ==\n\nThere are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:\n\nAn array is a number of elements in a specific order, typically all of the same type (depending on the language, individual elements may either all be forced to be the same type, or may be of almost any type). Elements are accessed using an integer index to specify which element is required. Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable.\nA linked list (also just called list) is a linear collection of data elements of any type, called nodes, where each node has itself a value, and points to the next node in the linked list. The principal advantage of a linked list over an array is that values can always be efficiently inserted and removed without relocating the rest of the list. Certain other operations, such as random access to a certain element, are however slower on lists than on arrays.\nA record (also called tuple or struct) is an aggregate data structure. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members. In the context of object-oriented programming, records are known as plain old data structures to distinguish them from objects.\nHash tables, also known as hash maps, are data structures that provide fast retrieval of values based on keys. They use a hashing function to map keys to indexes in an array, allowing for constant-time access in the average case. Hash tables are commonly used in dictionaries, caches, and database indexing. However, hash collisions can occur, which can impact their performance. Techniques like chaining and open addressing are employed to handle collisions.\nGraphs are collections of nodes connected by edges, representing relationships between entities. Graphs can be used to model social networks, computer networks, and transportation networks, among other things. They consist of vertices (nodes) and edges (connections between nodes). Graphs can be directed or undirected, and they can have cycles or be acyclic. Graph traversal algorithms include breadth-first search and depth-first search.\nStacks and queues are abstract data types that can be implemented using arrays or linked lists. A stack has two primary operations: push (adds an element to the top of the stack) and pop (removes the topmost element from the stack), that follow the Last In, First Out (LIFO) principle. Queues have two main operations: enqueue (adds an element to the rear of the queue) and dequeue (removes an element from the front of the queue) that follow the First In, First Out (FIFO) principle.\nTrees represent a hierarchical organization of elements. A tree consists of nodes connected by edges, with one node being the root and all other nodes forming subtrees. Trees are widely used in various algorithms and data storage scenarios. Binary trees (particularly heaps), AVL trees, and B-trees are some popular types of trees. They enable efficient and optimal searching, sorting, and hierarchical representation of data.\nA trie, or prefix tree, is a special type of tree used to efficiently retrieve strings. In a trie, each node represents a character of a string, and the edges between nodes represent the characters that connect them. This structure is especially useful for tasks like autocomplete, spell-checking, and creating dictionaries. Tries allow for quick searches and operations based on string prefixes.\n\n\n== Language support ==\nMost assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.\nMost programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.\nModern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.\nMany known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nPeter Brass, Advanced Data Structures, Cambridge University Press, 2008, ISBN 978-0521880374\nDonald Knuth, The Art of Computer Programming, vol. 1. Addison-Wesley, 3rd edition, 1997, ISBN 978-0201896831\nDinesh Mehta and Sartaj Sahni, Handbook of Data Structures and Applications, Chapman and Hall/CRC Press, 2004, ISBN 1584884355\nNiklaus Wirth, Algorithms and Data Structures, Prentice Hall, 1985, ISBN 978-0130220059\n\n\n== Further reading ==\nOpen Data Structures by Pat Morin\nG. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures - in Pascal and C, second edition, Addison-Wesley, 1991, ISBN 0-201-41607-7\nEllis Horowitz and Sartaj Sahni, Fundamentals of Data Structures in Pascal, Computer Science Press, 1984, ISBN 0-914894-94-3\n\n\n== External links ==\n\nDescriptions from the Dictionary of Algorithms and Data Structures\nData structures course\nAn Examination of Data Structures from .NET perspective\nSchaffer, C. Data Structures and Algorithm Analysis"
    },
    {
        "id": "algorithms",
        "name": "Algorithm",
        "text": "In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).\nIn contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results. For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation.\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\n\n\n== Etymology ==\nAround 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). In the early 12th century, Latin translations of said al-Khwarizmi texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Hereby, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\". Around 1230, the English word algorism is attested and then by Chaucer in 1391, English adopted the French term. In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, \"number\"; cf. \"arithmetic\"), the Latin word was altered to algorithmus.\n\n\n== Definition ==\n\nOne informal definition is \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure\nor cook-book recipe. In general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.\n\n\n== History ==\n\n\n=== Ancient algorithms ===\nStep-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), Chinese mathematics (around 200 BC and later), and Arabic mathematics (around 800 AD).\nThe earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.\nAlgorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,: Ch 9.2  and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).: Ch 9.1 Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\n\n\n=== Computers ===\n\n\n==== Weight-driven clocks ====\nBolter credits the invention of the weight-driven clock as \"the key invention [of Europe in the Middle Ages],\" specifically the verge escapement mechanism producing the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" in the 13th century and \"computational machines\"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called \"history's first programmer\".\n\n\n==== Electromechanical relay ====\nBell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and \"telephone switching technologies\" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.\nTelephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the \"burdensome\" use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".\n\n\n=== Formalization ===\n\nIn 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\n\n== Representations ==\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms.\n\n\n=== Turing machines ===\nThere are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called \"sets of quadruples\", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.\n\n\n=== Flowchart representation ===\nThe graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure.\n\n\n== Algorithmic analysis ==\n\nIt is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠\n  \n    \n      \n        O\n        (\n        1\n        )\n      \n    \n    {\\displaystyle O(1)}\n  \n⁠, otherwise ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠ is required.\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠\n  \n    \n      \n        O\n        (\n        log\n        ⁡\n        n\n        )\n      \n    \n    {\\displaystyle O(\\log n)}\n  \n⁠) outperforms a sequential search (cost ⁠\n  \n    \n      \n        O\n        (\n        n\n        )\n      \n    \n    {\\displaystyle O(n)}\n  \n⁠ ) when used for table lookups on sorted lists or arrays.\n\n\n=== Formal versus empirical ===\n\nThe analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many \"one-off\" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\nEmpirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.\n\n\n=== Execution efficiency ===\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\n\n== Design ==\n\nAlgorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.\n\n\n=== Structured programming ===\nPer the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.\n\n\n== Legal status ==\n\nBy themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\n\n== Classification ==\n\n\n=== By implementation ===\nRecursion\nA recursive algorithm invokes itself repeatedly until meeting a termination condition and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other. The Tower of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\nSerial, parallel or distributed\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.\nDeterministic or non-deterministic\nDeterministic algorithms solve the problem with exact decisions at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics.\nExact or approximate\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items, and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider the weights of items as well as their value.\nQuantum algorithm\nQuantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms that seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\n\n\n=== By design paradigm ===\nAnother way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:\n\nBrute-force or exhaustive search\nBrute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords.\nDivide and conquer\nA divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer, where an unordered list can be divided into segments containing one item and sorting of the entire list can be obtained by merging the segments. A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, which solves one smaller instance of itself, and uses the solution to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\nSearch and enumeration\nMany problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking.\nRandomized algorithm\nSuch algorithms make some choices randomly (or pseudo-randomly). They find approximate solutions when finding exact solutions may be impractical (see heuristic method below). For some problems, the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:\nMonte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.\nReduction of complexity\nThis technique transforms difficult problems into better-known problems solvable with (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm finds the median of an unsorted list by first sorting the list (the expensive portion), and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\nBack tracking\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\n\n\n=== Optimization problems ===\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\nLinear programming\nWhen searching for optimal solutions to a linear function bound by linear equality and inequality constraints, the constraints can be used directly to produce optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem also requires that any of the unknowns be integers, then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\nDynamic programming\nWhen a problem shows optimal substructures—meaning the optimal solution can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions. For example, Floyd–Warshall algorithm, the shortest path between a start and goal vertex in a weighted graph can be found using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. Unlike divide and conquer, dynamic programming subproblems often overlap. The difference between dynamic programming and simple recursion is the caching or memoization of recursive calls. When subproblems are independent and do not repeat, memoization does not help; hence dynamic programming is not applicable to all complex problems. Using memoization dynamic programming reduces the complexity of many problems from exponential to polynomial.\nThe greedy method\nGreedy algorithms, similarly to a dynamic programming, work by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution and improve it by making small modifications. For some problems, they always find the optimal solution but for others they may stop at local optima. The most popular use of greedy algorithms is finding minimal spanning trees of graphs without negative cycles. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\nThe heuristic method\nIn optimization problems, heuristic algorithms find solutions close to the optimal solution when finding the optimal solution is impractical. These algorithms get closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. They can ideally find a solution very close to the optimal solution in a relatively short time. These algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\n\n\n== Examples ==\n\nOne of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as:\nHigh-level description:\n\nIf a set of numbers is empty, then there is no highest number.\nAssume the first number in the set is the largest.\nFor each remaining number in the set: if this number is greater than the current largest, it becomes the new largest.\nWhen there are no unchecked numbers left in the set, consider the current largest number to be the largest in the set.\n(Quasi-)formal description:\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Bibliography ==\n\nZaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Algorithm\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nWeisstein, Eric W. \"Algorithm\". MathWorld.\nDictionary of Algorithms and Data Structures – National Institute of Standards and Technology\nAlgorithm repositories\nThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\nCollected Algorithms of the ACM – Associations for Computing Machinery\nThe Stanford GraphBase Archived December 6, 2015, at the Wayback Machine – Stanford University"
    },
    {
        "id": "calculus",
        "name": "Calculus",
        "text": "Calculus is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\nOriginally called infinitesimal calculus or \"the calculus of infinitesimals\", it has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus. They make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. It is the \"mathematical backbone\" for dealing with problems where variables change with time or another reference variable.\nInfinitesimal calculus was formulated separately in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus is widely used in science, engineering, biology, and even has applications in social science and other branches of math.\n\n\n== Etymology ==\n\nIn mathematics education, calculus is an abbreviation of both infinitesimal calculus and integral calculus, which denotes courses of elementary mathematical analysis. \nIn Latin, the word calculus means “small pebble”, (the diminutive of calx, meaning \"stone\"), a meaning which still persists in medicine. Because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to be the Latin word for calculation. In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.\nIn addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, sequent calculus, and process calculus. Furthermore, the term \"calculus\" has variously been applied in ethics and philosophy, for such systems as Bentham's felicific calculus, and the ethical calculus.\n\n\n== History ==\n\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India.\n\n\n=== Ancient precursors ===\n\n\n==== Egypt ====\nCalculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (c. 1820 BC), but the formulae are simple instructions, with no indication as to how they were obtained.\n\n\n==== Greece ====\n\nLaying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematician Eudoxus of Cnidus (c. 390–337 BC) developed  the method of exhaustion to prove the formulas for cone and pyramid volumes.\nDuring the Hellenistic period, this method was further developed by Archimedes (c. 287 – c. 212 BC), who combined it with a concept of the indivisibles—a precursor to infinitesimals—allowing him to solve several problems now treated by integral calculus. In The Method of Mechanical Theorems he describes, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines.\n\n\n==== China ====\nThe method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD to find the area of a circle. In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method that would later be called Cavalieri's principle to find the volume of a sphere.\n\n\n=== Medieval ===\n\n\n==== Middle East ====\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c. 965 – c. 1040 AD) derived a formula for the sum of fourth powers. He determined the equations to calculate the area enclosed by the curve represented by \n  \n    \n      \n        y\n        =\n        \n          x\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle y=x^{k}}\n  \n (which translates to the integral \n  \n    \n      \n        ∫\n        \n          x\n          \n            k\n          \n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int x^{k}\\,dx}\n  \n in contemporary notation), for any given non-negative integer value of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n.He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.\n\n\n==== India ====\nBhāskara II (c. 1114–1185) was acquainted with some ideas of differential calculus and suggested that the \"differential coefficient\" vanishes at an extremum value of the function. In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, if \n  \n    \n      \n        x\n        ≈\n        y\n      \n    \n    {\\displaystyle x\\approx y}\n  \n then \n  \n    \n      \n        sin\n        ⁡\n        (\n        y\n        )\n        −\n        sin\n        ⁡\n        (\n        x\n        )\n        ≈\n        (\n        y\n        −\n        x\n        )\n        cos\n        ⁡\n        (\n        y\n        )\n        .\n      \n    \n    {\\displaystyle \\sin(y)-\\sin(x)\\approx (y-x)\\cos(y).}\n  \n This can be interpreted as the discovery that cosine is the derivative of sine. In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics stated components of calculus. They studied series equivalent to the Maclaurin expansions of ⁠\n  \n    \n      \n        sin\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\sin(x)}\n  \n⁠, ⁠\n  \n    \n      \n        cos\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\cos(x)}\n  \n⁠, and ⁠\n  \n    \n      \n        arctan\n        ⁡\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\arctan(x)}\n  \n⁠ more than two hundred years before their introduction in Europe. According to Victor J. Katz they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\".\n\n\n=== Modern ===\nJohannes Kepler's work Stereometria Doliorum (1615) formed the basis of integral calculus. Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.\nSignificant work was a treatise, the origin being Kepler's methods, written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in The Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term. The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving predecessors to the second fundamental theorem of calculus around 1670.\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.\n\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton. He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today.: 51–52  The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics. A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\", a term that endured in English schools into the 19th century.: 100  The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.\n\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\n\n\n=== Foundations ===\nIn calculus, foundations refers to the rigorous development of the subject from axioms and definitions.  In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734.  Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities. The foundations of differential and integral calculus had been laid. In Cauchy's Cours d'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation. In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral. It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by Émile Borel, and used it to define integrals of all but the most pathological functions. Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus. There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations. Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold. The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.\n\n\n=== Significance ===\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles. The Hungarian polymath John von Neumann wrote of this work,\n\nThe calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization.: 341–453  Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure.: 685–700  More advanced applications include power series and Fourier series.\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.\n\n\n== Principles ==\n\n\n=== Limits and infinitesimals ===\n\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\".  For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols \n  \n    \n      \n        d\n        x\n      \n    \n    {\\displaystyle dx}\n  \n and \n  \n    \n      \n        d\n        y\n      \n    \n    {\\displaystyle dy}\n  \n were taken to be infinitesimal, and the derivative \n  \n    \n      \n        d\n        y\n        \n          /\n        \n        d\n        x\n      \n    \n    {\\displaystyle dy/dx}\n  \n was their ratio.\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.\n\n\n=== Differential calculus ===\n\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.: 32 \nIn more explicit terms the \"doubling function\" may be denoted by g(x) = 2x and the \"squaring function\" by f(x) = x2. The \"derivative\" now takes the function f(x), defined by the expression \"x2\", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function g(x) = 2x, as will turn out.\nIn Lagrange's notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called f is denoted by f′, pronounced \"f prime\" or \"f dash\". For instance, if f(x) = x2 is the squaring function, then f′(x) = 2x is its derivative (the doubling function g from above).\nIf the input of the function represents time, then the derivative represents change concerning time. For example, if f is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.: 18–20 \nIf a function is linear (that is if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent variable, y is the dependent variable, b is the y-intercept, and:\n\n  \n    \n      \n        m\n        =\n        \n          \n            rise\n            run\n          \n        \n        =\n        \n          \n            \n              \n                change in \n              \n              y\n            \n            \n              \n                change in \n              \n              x\n            \n          \n        \n        =\n        \n          \n            \n              Δ\n              y\n            \n            \n              Δ\n              x\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {\\text{rise}}{\\text{run}}}={\\frac {{\\text{change in }}y}{{\\text{change in }}x}}={\\frac {\\Delta y}{\\Delta x}}.}\n  \n\nThis gives an exact value for the slope of a straight line.: 6  If the graph of the function is not a straight line, however, then the change in y divided by the change in x varies. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, let f be a function, and fix a point a in the domain of f. (a, f(a)) is a point on the graph of the function. If h is a number close to zero, then a + h is a number close to a. Therefore, (a + h, f(a + h)) is close to (a, f(a)). The slope between these two points is\n\n  \n    \n      \n        m\n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            \n              (\n              a\n              +\n              h\n              )\n              −\n              a\n            \n          \n        \n        =\n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            h\n          \n        \n        .\n      \n    \n    {\\displaystyle m={\\frac {f(a+h)-f(a)}{(a+h)-a}}={\\frac {f(a+h)-f(a)}{h}}.}\n  \n\nThis expression is called a difference quotient. A line through two points on a curve is called a secant line, so m is the slope of the secant line between (a, f(a)) and (a + h, f(a + h)). The second line is only an approximation to the behavior of the function at the point  a because it does not account for what happens between  a and  a + h. It is not possible to discover the behavior at  a by setting h to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as h tends to zero, meaning that it considers the behavior of f for all small values of h and extracts a consistent value for the case when h equals zero:\n\n  \n    \n      \n        \n          lim\n          \n            h\n            →\n            0\n          \n        \n        \n          \n            \n              f\n              (\n              a\n              +\n              h\n              )\n              −\n              f\n              (\n              a\n              )\n            \n            \n              h\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\lim _{h\\to 0}{f(a+h)-f(a) \\over {h}}.}\n  \n\nGeometrically, the derivative is the slope of the tangent line to the graph of f at a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.: 61–63 \nHere is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  ′\n                \n                (\n                3\n                )\n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      (\n                      3\n                      +\n                      h\n                      \n                        )\n                        \n                          2\n                        \n                      \n                      −\n                      \n                        3\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      9\n                      +\n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                      −\n                      9\n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                \n                  \n                    \n                      6\n                      h\n                      +\n                      \n                        h\n                        \n                          2\n                        \n                      \n                    \n                    \n                      h\n                    \n                  \n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  lim\n                  \n                    h\n                    →\n                    0\n                  \n                \n                (\n                6\n                +\n                h\n                )\n              \n            \n            \n              \n              \n                \n                =\n                6\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f'(3)&=\\lim _{h\\to 0}{(3+h)^{2}-3^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}{9+6h+h^{2}-9 \\over {h}}\\\\&=\\lim _{h\\to 0}{6h+h^{2} \\over {h}}\\\\&=\\lim _{h\\to 0}(6+h)\\\\&=6\\end{aligned}}}\n  \n\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.: 63 \n\n\n=== Leibniz notation ===\n\nA common notation, introduced by Leibniz, for the derivative in the example above is\n\n  \n    \n      \n        \n          \n            \n              \n                y\n              \n              \n                \n                =\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n            \n              \n                \n                  \n                    \n                      d\n                      y\n                    \n                    \n                      d\n                      x\n                    \n                  \n                \n              \n              \n                \n                =\n                2\n                x\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}y&=x^{2}\\\\{\\frac {dy}{dx}}&=2x.\\end{aligned}}}\n  \n\nIn an approach based on limits, the symbol ⁠dy/ dx⁠ is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above.: 74  Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, dy being the infinitesimally small change in y caused by an infinitesimally small change  dx applied to x. We can also think of ⁠d/ dx⁠ as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        (\n        \n          x\n          \n            2\n          \n        \n        )\n        =\n        2\n        x\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}(x^{2})=2x.}\n  \n\nIn this usage, the dx in the denominator is read as \"with respect to x\".: 79  Another example of correct notation could be:\n\n  \n    \n      \n        \n          \n            \n              \n                g\n                (\n                t\n                )\n              \n              \n                \n                =\n                \n                  t\n                  \n                    2\n                  \n                \n                +\n                2\n                t\n                +\n                4\n              \n            \n            \n              \n                \n                  \n                    d\n                    \n                      d\n                      t\n                    \n                  \n                \n                g\n                (\n                t\n                )\n              \n              \n                \n                =\n                2\n                t\n                +\n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}g(t)&=t^{2}+2t+4\\\\{d \\over dt}g(t)&=2t+2\\end{aligned}}}\n  \n\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like  dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\n\n\n=== Integral calculus ===\n\nIntegral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration.: 508  The indefinite integral, also known as the antiderivative, is the inverse operation to the derivative.: 163–165  F is an indefinite integral of f when f is a derivative of F.  (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.: 282 \nA motivating example is the distance traveled in a given time.: 153  If the speed is constant, only multiplication is needed:\n\n  \n    \n      \n        \n          D\n          i\n          s\n          t\n          a\n          n\n          c\n          e\n        \n        =\n        \n          S\n          p\n          e\n          e\n          d\n        \n        ⋅\n        \n          T\n          i\n          m\n          e\n        \n      \n    \n    {\\displaystyle \\mathrm {Distance} =\\mathrm {Speed} \\cdot \\mathrm {Time} }\n  \n\nBut if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time.  For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles.  Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed.  Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.: 535   This connection between the area under a curve and the distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given period. If f(x) represents speed as it varies over time, the distance traveled between the times represented by  a and b is the area of the region between f(x) and the x-axis, between x = a and x = b.\nTo approximate that area, an intuitive method would be to divide up the distance between  a and b into several equal segments, the length of each segment represented by the symbol Δx. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base Δx and height h gives the distance (time Δx multiplied by speed h) traveled in that segment.   Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for Δx will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit as Δx approaches zero.: 512–522 \nThe symbol of integration is \n  \n    \n      \n        ∫\n      \n    \n    {\\displaystyle \\int }\n  \n, an elongated S chosen to suggest summation.: 529  The definite integral is written as:\n\n  \n    \n      \n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx}\n  \n\nand is read \"the integral from a to b of f-of-x with respect to x.\" The Leibniz notation dx is intended to suggest dividing the area under the curve into an infinite number of rectangles so that their width Δx becomes the infinitesimally small dx.: 44 \nThe indefinite integral, or antiderivative, is written:\n\n  \n    \n      \n        ∫\n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int f(x)\\,dx.}\n  \n\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant.: 326  Since the derivative of the function y = x2 + C, where C is any constant, is y′ = 2x, the antiderivative of the latter is given by:\n\n  \n    \n      \n        ∫\n        2\n        x\n        \n        d\n        x\n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        C\n        .\n      \n    \n    {\\displaystyle \\int 2x\\,dx=x^{2}+C.}\n  \n\nThe unspecified constant C present in the indefinite integral or antiderivative is known as the constant of integration.: 135 \n\n\n=== Fundamental theorem ===\n\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations.: 290  More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\nThe fundamental theorem of calculus states: If a function f is continuous on the interval [a, b] and if F is a function whose derivative is f on the interval (a, b), then\n\n  \n    \n      \n        \n          ∫\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        −\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  \n\nFurthermore, for every x in the interval (a, b),\n\n  \n    \n      \n        \n          \n            d\n            \n              d\n              x\n            \n          \n        \n        \n          ∫\n          \n            a\n          \n          \n            x\n          \n        \n        f\n        (\n        t\n        )\n        \n        d\n        t\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle {\\frac {d}{dx}}\\int _{a}^{x}f(t)\\,dt=f(x).}\n  \n\nThis realization, made by both Newton and Leibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work of Isaac Barrow, is difficult to determine because of the priority dispute between them.) The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulae for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.: 351–352 \n\n\n== Applications ==\n\nCalculus is used in every branch of the physical sciences,: 1  actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired. It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other. Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or, it can be used in probability theory to determine the expectation value of a continuous random variable given a probability density function.: 37  In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero-gravity environments.\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, and the potential energies due to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion, which states that the derivative of an object's momentum concerning time equals the net force upon it. Alternatively, Newton's second law can be expressed by saying that the net force equals the object's mass times its acceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus.: 52–55  Chemistry also uses calculus in determining reaction rates: 599  and in studying radioactive decay.: 814  In biology, population dynamics starts with reproduction and death rates to model population changes.: 631 \nGreen's theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing. For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel to maximize flow. Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly a cancerous tumor grows.\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.: 387 \n\n\n== See also ==\n\nGlossary of calculus\nList of calculus topics\nList of derivatives and integrals in alternative calculi\nList of differentiation identities\nPublications in calculus\nTable of integrals\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Calculus\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nWeisstein, Eric W. \"Calculus\". MathWorld.\nTopics on Calculus at PlanetMath.\nCalculus Made Easy (1914) by Silvanus P. Thompson Full text in PDF\nCalculus on In Our Time at the BBC\nCalculus.org: The Calculus page at University of California, Davis – contains resources and links to other sites\nEarliest Known Uses of Some of the Words of Mathematics: Calculus & Analysis\nThe Role of Calculus in College Mathematics Archived 26 July 2021 at the Wayback Machine from ERICDigests.org\nOpenCourseWare Calculus from the Massachusetts Institute of Technology\nInfinitesimal Calculus – an article on its historical development, in Encyclopedia of Mathematics, ed. Michiel Hazewinkel.\nDaniel Kleitman, MIT. \"Calculus for Beginners and Artists\".\nCalculus training materials at imomath.com\n(in English and Arabic) The Excursion of Calculus, 1772"
    },
    {
        "id": "linear_algebra",
        "name": "Linear algebra",
        "text": "Linear algebra is the branch of mathematics concerning linear equations such as\n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        ,\n      \n    \n    {\\displaystyle a_{1}x_{1}+\\cdots +a_{n}x_{n}=b,}\n  \n\nlinear maps such as\n\n  \n    \n      \n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n        )\n        ↦\n        \n          a\n          \n            1\n          \n        \n        \n          x\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            n\n          \n        \n        \n          x\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle (x_{1},\\ldots ,x_{n})\\mapsto a_{1}x_{1}+\\cdots +a_{n}x_{n},}\n  \n\nand their representations in vector spaces and through matrices.\nLinear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to function spaces.\nLinear algebra is also used in most sciences and fields of engineering because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.\n\n\n== History ==\n\nThe procedure (using counting rods) for solving simultaneous linear equations now called Gaussian elimination appears in the ancient Chinese mathematical text Chapter Eight: Rectangular Arrays of The Nine Chapters on the Mathematical Art. Its use is illustrated in eighteen problems, with two to five equations.\nSystems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry. In fact, in this new geometry, now called Cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.\nThe first systematic methods for solving linear systems used determinants and were first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's rule. Later, Gauss further described the method of elimination, which was initially listed as an advancement in geodesy.\nIn 1844 Hermann Grassmann published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for womb.\nLinear algebra grew with ideas noted in the complex plane. For instance, two numbers w and z in \n  \n    \n      \n        \n          C\n        \n      \n    \n    {\\displaystyle \\mathbb {C} }\n  \n have a difference w – z, and the line segments wz and 0(w − z) are of the same length and direction. The segments are equipollent. The four-dimensional system \n  \n    \n      \n        \n          H\n        \n      \n    \n    {\\displaystyle \\mathbb {H} }\n  \n of quaternions was discovered by W.R. Hamilton in 1843. The term vector was introduced as v = xi + yj + zk representing a point in space. The quaternion difference p – q also produces a segment equipollent to pq. Other hypercomplex number systems also used the idea of a linear space with a basis.\nArthur Cayley introduced matrix multiplication and the inverse matrix in 1856, making possible the general linear group. The mechanism of group representation became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nBenjamin Peirce published his Linear Associative Algebra (1872), and his son Charles Sanders Peirce extended the work later.\nThe telegraph required an explanatory system, and the 1873 publication by James Clerk Maxwell of A Treatise on Electricity and Magnetism instituted a field theory of forces and required differential geometry for expression. Linear algebra is flat differential geometry and serves in tangent spaces to manifolds. Electromagnetic symmetries of spacetime are expressed by the Lorentz transformations, and much of the history of linear algebra is the history of Lorentz transformations.\nThe first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century when many ideas and methods of previous centuries were generalized as abstract algebra. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modeling and simulations.\n\n\n== Vector spaces ==\n\nUntil the 19th century, linear algebra was introduced through systems of linear equations and matrices. In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.\nA vector space over a field F (often the field of the real numbers) is a set V equipped with two binary operations. Elements of V are called vectors, and elements of F are called scalars. The first operation, vector addition, takes any two vectors v and w and outputs a third vector v + w. The second operation, scalar multiplication, takes any scalar a and any vector v and outputs a new vector av. The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, u, v and w are arbitrary elements of V, and a and b are arbitrary scalars in the field F.)\n\nThe first four axioms mean that V is an abelian group under addition.\nAn element of a specific vector space may have various natures; for example, it could be a sequence, a function, a polynomial, or a matrix. Linear algebra is concerned with the properties of such objects that are common to all vector spaces.\n\n\n=== Linear maps ===\n\nLinear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F, a linear map (also called, in some contexts, linear transformation or linear mapping) is a map\n\n  \n    \n      \n        T\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle T:V\\to W}\n  \n\nThat is compatible with addition and scalar multiplication, that is\n\n  \n    \n      \n        T\n        (\n        \n          u\n        \n        +\n        \n          v\n        \n        )\n        =\n        T\n        (\n        \n          u\n        \n        )\n        +\n        T\n        (\n        \n          v\n        \n        )\n        ,\n        \n        T\n        (\n        a\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(\\mathbf {u} +\\mathbf {v} )=T(\\mathbf {u} )+T(\\mathbf {v} ),\\quad T(a\\mathbf {v} )=aT(\\mathbf {v} )}\n  \n\nfor any vectors u,v in V and scalar a in F.\nThis implies that for any vectors u, v in V and scalars a, b in F, one has\n\n  \n    \n      \n        T\n        (\n        a\n        \n          u\n        \n        +\n        b\n        \n          v\n        \n        )\n        =\n        T\n        (\n        a\n        \n          u\n        \n        )\n        +\n        T\n        (\n        b\n        \n          v\n        \n        )\n        =\n        a\n        T\n        (\n        \n          u\n        \n        )\n        +\n        b\n        T\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle T(a\\mathbf {u} +b\\mathbf {v} )=T(a\\mathbf {u} )+T(b\\mathbf {v} )=aT(\\mathbf {u} )+bT(\\mathbf {v} )}\n  \n\nWhen V = W are the same vector space, a linear map T : V → V is also known as a linear operator on V.\nA bijective linear map between two vector spaces (that is, every vector from the second space is associated with exactly one in the first) is an isomorphism. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm.\n\n\n=== Subspaces, span, and basis ===\n\nThe study of those subsets of vector spaces that are in themselves vector spaces under the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces. More precisely, a linear subspace of a vector space V over a field F is a subset W of V such that u + v and au are in W, for every u, v in W, and every a in F. (These conditions suffice for implying that W is a vector space.)\nFor example, given a linear map T : V → W, the image T(V) of V, and the inverse image T−1(0) of 0 (called kernel or null space), are linear subspaces of W and V, respectively.\nAnother important way of forming a subspace is to consider linear combinations of a set S of vectors: the set of all sums \n\n  \n    \n      \n        \n          a\n          \n            1\n          \n        \n        \n          \n            v\n          \n          \n            1\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        \n          \n            v\n          \n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            k\n          \n        \n        \n          \n            v\n          \n          \n            k\n          \n        \n        ,\n      \n    \n    {\\displaystyle a_{1}\\mathbf {v} _{1}+a_{2}\\mathbf {v} _{2}+\\cdots +a_{k}\\mathbf {v} _{k},}\n  \n\nwhere v1, v2, ..., vk are in S, and a1, a2, ..., ak are in F form a linear subspace called the span of S. The span of S is also the intersection of all linear subspaces containing S. In other words, it is the smallest (for the inclusion relation) linear subspace containing S.\nA set of vectors is linearly independent if none is in the span of the others. Equivalently, a set S of vectors is linearly independent if the only way to express the zero vector as a linear combination of elements of S is to take zero for every coefficient ai.\nA set of vectors that spans a vector space is called a spanning set or generating set. If a spanning set S is linearly dependent (that is not linearly independent), then some element w of S is in the span of the other elements of S, and the span would remain the same if one were to remove w from S. One may continue to remove elements of S until getting a linearly independent spanning set. Such a linearly independent set that spans a vector space V is called a basis of V. The importance of bases lies in the fact that they are simultaneously minimal-generating sets and maximal independent sets. More precisely, if S is a linearly independent set, and T is a spanning set such that S ⊆ T, then there is a basis B such that S ⊆ B ⊆ T.\nAny two bases of a vector space V have the same cardinality, which is called the dimension of V; this is the dimension theorem for vector spaces. Moreover, two vector spaces over the same field F are isomorphic if and only if they have the same dimension.\nIf any basis of V (and therefore every basis) has a finite number of elements, V is a finite-dimensional vector space. If U is a subspace of V, then dim U ≤ dim V. In the case where V is finite-dimensional, the equality of the dimensions implies U = V.\nIf U1 and U2 are subspaces of V, then\n\n  \n    \n      \n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        +\n        \n          U\n          \n            2\n          \n        \n        )\n        =\n        dim\n        ⁡\n        \n          U\n          \n            1\n          \n        \n        +\n        dim\n        ⁡\n        \n          U\n          \n            2\n          \n        \n        −\n        dim\n        ⁡\n        (\n        \n          U\n          \n            1\n          \n        \n        ∩\n        \n          U\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\dim(U_{1}+U_{2})=\\dim U_{1}+\\dim U_{2}-\\dim(U_{1}\\cap U_{2}),}\n  \n\nwhere U1 + U2 denotes the span of U1 ∪ U2.\n\n\n== Matrices ==\n\nMatrices allow explicit manipulation of finite-dimensional vector spaces and linear maps. Their theory is thus an essential part of linear algebra.\nLet V be a finite-dimensional vector space over a field F, and (v1, v2, ..., vm) be a basis of V (thus m is the dimension of V). By definition of a basis, the map\n\n  \n    \n      \n        \n          \n            \n              \n                (\n                \n                  a\n                  \n                    1\n                  \n                \n                ,\n                …\n                ,\n                \n                  a\n                  \n                    m\n                  \n                \n                )\n              \n              \n                \n                ↦\n                \n                  a\n                  \n                    1\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    1\n                  \n                \n                +\n                ⋯\n                \n                  a\n                  \n                    m\n                  \n                \n                \n                  \n                    v\n                  \n                  \n                    m\n                  \n                \n              \n            \n            \n              \n                \n                  F\n                  \n                    m\n                  \n                \n              \n              \n                \n                →\n                V\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}(a_{1},\\ldots ,a_{m})&\\mapsto a_{1}\\mathbf {v} _{1}+\\cdots a_{m}\\mathbf {v} _{m}\\\\F^{m}&\\to V\\end{aligned}}}\n  \n\nis a bijection from Fm, the set of the sequences of m elements of F, onto V. This is an isomorphism of vector spaces, if Fm is equipped with its standard structure of vector space, where vector addition and scalar multiplication are done component by component.\nThis isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinate vector (a1, ..., am) or by the column matrix\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1}\\\\\\vdots \\\\a_{m}\\end{bmatrix}}.}\n  \n\nIf W is another finite dimensional vector space (possibly the same), with a basis (w1, ..., wn), a linear map f from W to V is well defined by its values on the basis elements, that is (f(w1), ..., f(wn)). Thus, f is well represented by the list of the corresponding column matrices. That is, if \n\n  \n    \n      \n        f\n        (\n        \n          w\n          \n            j\n          \n        \n        )\n        =\n        \n          a\n          \n            1\n            ,\n            j\n          \n        \n        \n          v\n          \n            1\n          \n        \n        +\n        ⋯\n        +\n        \n          a\n          \n            m\n            ,\n            j\n          \n        \n        \n          v\n          \n            m\n          \n        \n        ,\n      \n    \n    {\\displaystyle f(w_{j})=a_{1,j}v_{1}+\\cdots +a_{m,j}v_{m},}\n  \n\nfor j = 1, ..., n, then f is represented by the matrix\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      1\n                      ,\n                      n\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      1\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    a\n                    \n                      m\n                      ,\n                      n\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\begin{bmatrix}a_{1,1}&\\cdots &a_{1,n}\\\\\\vdots &\\ddots &\\vdots \\\\a_{m,1}&\\cdots &a_{m,n}\\end{bmatrix}},}\n  \n\nwith m rows and n columns.\nMatrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing the same concepts.\nTwo matrices that encode the same linear transformation in different bases are called similar. It can be proved that two matrices are similar if and only if one can transform one into the other by elementary row and column operations. For a matrix representing a linear map from W to V, the row operations correspond to change of bases in V and the column operations correspond to change of bases in W. Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector spaces, this means that, for any linear map from W to V, there are bases such that a part of the basis of W is mapped bijectively on a part of the basis of V, and that the remaining basis elements of W, if any, are mapped to zero. Gaussian elimination is the basic algorithm for finding these elementary operations, and proving these results.\n\n\n== Linear systems ==\n\nA finite set of linear equations in a finite set of variables, for example, x1, x2, ..., xn, or x, y, ..., z is called a  system of linear equations or a linear system.\nSystems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory have been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.\nFor example, let\n\nbe a linear system.\nTo such a system, one may associate its matrix \n\n  \n    \n      \n        M\n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n              \n            \n          \n          ]\n        \n        .\n      \n    \n    {\\displaystyle M=\\left[{\\begin{array}{rrr}2&1&-1\\\\-3&-1&2\\\\-2&1&2\\end{array}}\\right].}\n  \n\nand its right member vector\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  8\n                \n              \n              \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  3\n                \n              \n            \n            ]\n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v} ={\\begin{bmatrix}8\\\\-11\\\\-3\\end{bmatrix}}.}\n  \n\nLet T be the linear transformation associated with the matrix M. A solution of the system (S) is a vector \n\n  \n    \n      \n        \n          X\n        \n        =\n        \n          \n            [\n            \n              \n                \n                  x\n                \n              \n              \n                \n                  y\n                \n              \n              \n                \n                  z\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {X} ={\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}}}\n  \n\nsuch that \n\n  \n    \n      \n        T\n        (\n        \n          X\n        \n        )\n        =\n        \n          v\n        \n        ,\n      \n    \n    {\\displaystyle T(\\mathbf {X} )=\\mathbf {v} ,}\n  \n\nthat is an element of the preimage of v by T.\nLet (S′) be the associated homogeneous system, where the right-hand sides of the equations are put to zero:\n\nThe solutions of (S′) are exactly the elements of the kernel of T or, equivalently, M.\nThe Gaussian-elimination consists of performing elementary row operations on the augmented matrix\n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  2\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n                \n                  8\n                \n              \n              \n                \n                  −\n                  3\n                \n                \n                  −\n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  11\n                \n              \n              \n                \n                  −\n                  2\n                \n                \n                  1\n                \n                \n                  2\n                \n                \n                  −\n                  3\n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}2&1&-1&8\\\\-3&-1&2&-11\\\\-2&1&2&-3\\end{array}}\\right]}\n  \n\nfor putting it in reduced row echelon form. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is \n\n  \n    \n      \n        \n          [\n          \n            \n            \n              \n                \n                  \n                    M\n                  \n                  \n                    \n                      v\n                    \n                  \n                \n              \n            \n            \n          \n          ]\n        \n        =\n        \n          [\n          \n            \n              \n                \n                  1\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  2\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n                \n                  0\n                \n                \n                  3\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n            \n          \n          ]\n        \n        ,\n      \n    \n    {\\displaystyle \\left[\\!{\\begin{array}{c|c}M&\\mathbf {v} \\end{array}}\\!\\right]=\\left[{\\begin{array}{rrr|r}1&0&0&2\\\\0&1&0&3\\\\0&0&1&-1\\end{array}}\\right],}\n  \n\nshowing that the system (S) has the unique solution\n\n  \n    \n      \n        \n          \n            \n              \n                x\n              \n              \n                \n                =\n                2\n              \n            \n            \n              \n                y\n              \n              \n                \n                =\n                3\n              \n            \n            \n              \n                z\n              \n              \n                \n                =\n                −\n                1.\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x&=2\\\\y&=3\\\\z&=-1.\\end{aligned}}}\n  \n\nIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks, kernels, matrix inverses.\n\n\n== Endomorphisms and square matrices ==\n\nA linear endomorphism is a linear map that maps a vector space V to itself. \nIf V has a basis of n elements, such an endomorphism is represented by a square matrix of size n.\nConcerning general linear maps, linear endomorphisms, and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations, coordinate changes, quadratic forms, and many other parts of mathematics.\n\n\n=== Determinant ===\n\nThe determinant of a square matrix A is defined to be\n\n  \n    \n      \n        \n          ∑\n          \n            σ\n            ∈\n            \n              S\n              \n                n\n              \n            \n          \n        \n        (\n        −\n        1\n        \n          )\n          \n            σ\n          \n        \n        \n          a\n          \n            1\n            σ\n            (\n            1\n            )\n          \n        \n        ⋯\n        \n          a\n          \n            n\n            σ\n            (\n            n\n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sum _{\\sigma \\in S_{n}}(-1)^{\\sigma }a_{1\\sigma (1)}\\cdots a_{n\\sigma (n)},}\n  \n\nwhere Sn is the group of all permutations of n elements, σ is a permutation, and (−1)σ the parity of the permutation. A matrix is invertible if and only if the determinant is invertible (i.e., nonzero if the scalars belong to a field).\nCramer's rule is a closed-form expression, in terms of determinants, of the solution of a system of n linear equations in n unknowns. Cramer's rule is useful for reasoning about the solution, but, except for n = 2 or 3, it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.\nThe determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense since this determinant is independent of the choice of the basis.\n\n\n=== Eigenvalues and eigenvectors ===\n\nIf f is a linear endomorphism of a vector space V over a field F, an eigenvector of f is a nonzero vector v of V such that f(v) = av for some scalar a in F. This scalar a is an eigenvalue of f.\nIf the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix z; the equation defining eigenvectors and eigenvalues becomes\n\n  \n    \n      \n        M\n        z\n        =\n        a\n        z\n        .\n      \n    \n    {\\displaystyle Mz=az.}\n  \n\nUsing the identity matrix I, whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten\n\n  \n    \n      \n        (\n        M\n        −\n        a\n        I\n        )\n        z\n        =\n        0.\n      \n    \n    {\\displaystyle (M-aI)z=0.}\n  \n\nAs z is supposed to be nonzero, this means that M – aI is a singular matrix, and thus that its determinant det (M − aI) equals zero. The eigenvalues are thus the roots of the polynomial\n\n  \n    \n      \n        det\n        (\n        x\n        I\n        −\n        M\n        )\n        .\n      \n    \n    {\\displaystyle \\det(xI-M).}\n  \n\nIf V is of dimension n, this is a monic polynomial of degree n, called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.\nIf a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said to be diagonalizable. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free, then the matrix is diagonalizable.\nA symmetric matrix is always diagonalizable. There are non-diagonalizable matrices, the simplest being\n\n  \n    \n      \n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle {\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}}}\n  \n\n(it cannot be diagonalizable since its square is the zero matrix, and the square of a nonzero diagonal matrix is never zero).\nWhen an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need to extend the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extension of the field of scalar for containing all eigenvalues and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.\n\n\n== Duality ==\n\nA linear form is a linear map from a vector space V over a field F to the field of scalars F, viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of V, and usually denoted V* or V′.\nIf v1, ..., vn is a basis of V (this implies that V is finite-dimensional), then one can define, for i = 1, ..., n, a linear map vi* such that vi*(vi) = 1 and vi*(vj) = 0 if j ≠ i. These linear maps form a basis of V*, called the dual basis of v1, ..., vn. (If V is not finite-dimensional, the vi* may be defined similarly; they are linearly independent, but do not form a basis.)\nFor v in V, the map\n\n  \n    \n      \n        f\n        →\n        f\n        (\n        \n          v\n        \n        )\n      \n    \n    {\\displaystyle f\\to f(\\mathbf {v} )}\n  \n\nis a linear form on V*. This defines the canonical linear map from V into (V*)*, the dual of V*, called the double dual or bidual of V. This canonical map is an isomorphism if V is finite-dimensional, and this allows identifying V with its bidual. (In the infinite-dimensional case, the canonical map is injective, but not surjective.)\nThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation\n\n  \n    \n      \n        ⟨\n        f\n        ,\n        \n          x\n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle f,\\mathbf {x} \\rangle }\n  \n\nfor denoting f(x).\n\n\n=== Dual map ===\n\nLet \n\n  \n    \n      \n        f\n        :\n        V\n        →\n        W\n      \n    \n    {\\displaystyle f:V\\to W}\n  \n\nbe a linear map. For every linear form h on W, the composite function h ∘ f is a linear form on V. This defines a linear map\n\n  \n    \n      \n        \n          f\n          \n            ∗\n          \n        \n        :\n        \n          W\n          \n            ∗\n          \n        \n        →\n        \n          V\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle f^{*}:W^{*}\\to V^{*}}\n  \n\nbetween the dual spaces, which is called the dual or the transpose of f.\nIf V and W are finite-dimensional, and M is the matrix of f in terms of some ordered bases, then the matrix of f* over the dual bases is the transpose MT of M, obtained by exchanging rows and columns.\nIf elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ,\n        M\n        \n          v\n        \n        ⟩\n        =\n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        M\n        ,\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}},M\\mathbf {v} \\rangle =\\langle h^{\\mathsf {T}}M,\\mathbf {v} \\rangle .}\n  \n\nTo highlight this symmetry, the two members of this equality are sometimes written \n\n  \n    \n      \n        ⟨\n        \n          h\n          \n            \n              T\n            \n          \n        \n        ∣\n        M\n        ∣\n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle h^{\\mathsf {T}}\\mid M\\mid \\mathbf {v} \\rangle .}\n  \n\n\n=== Inner-product spaces ===\n\nBesides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map.\n\n  \n    \n      \n        ⟨\n        ⋅\n        ,\n        ⋅\n        ⟩\n        :\n        V\n        ×\n        V\n        →\n        F\n      \n    \n    {\\displaystyle \\langle \\cdot ,\\cdot \\rangle :V\\times V\\to F}\n  \n\nthat satisfies the following three axioms for all vectors u, v, w in V and all scalars a in F:\n\nConjugate symmetry:\n\n  \n    \n      \n        ⟨\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        =\n        \n          \n            \n              ⟨\n              \n                v\n              \n              ,\n              \n                u\n              \n              ⟩\n            \n            ¯\n          \n        \n        .\n      \n    \n    {\\displaystyle \\langle \\mathbf {u} ,\\mathbf {v} \\rangle ={\\overline {\\langle \\mathbf {v} ,\\mathbf {u} \\rangle }}.}\n  \n\nIn \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  \n, it is symmetric.\nLinearity in the first argument:\n\n  \n    \n      \n        \n          \n            \n              \n                ⟨\n                a\n                \n                  u\n                \n                ,\n                \n                  v\n                \n                ⟩\n              \n              \n                \n                =\n                a\n                ⟨\n                \n                  u\n                \n                ,\n                \n                  v\n                \n                ⟩\n                .\n              \n            \n            \n              \n                ⟨\n                \n                  u\n                \n                +\n                \n                  v\n                \n                ,\n                \n                  w\n                \n                ⟩\n              \n              \n                \n                =\n                ⟨\n                \n                  u\n                \n                ,\n                \n                  w\n                \n                ⟩\n                +\n                ⟨\n                \n                  v\n                \n                ,\n                \n                  w\n                \n                ⟩\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\langle a\\mathbf {u} ,\\mathbf {v} \\rangle &=a\\langle \\mathbf {u} ,\\mathbf {v} \\rangle .\\\\\\langle \\mathbf {u} +\\mathbf {v} ,\\mathbf {w} \\rangle &=\\langle \\mathbf {u} ,\\mathbf {w} \\rangle +\\langle \\mathbf {v} ,\\mathbf {w} \\rangle .\\end{aligned}}}\n  \n\nPositive-definiteness:\n\n  \n    \n      \n        ⟨\n        \n          v\n        \n        ,\n        \n          v\n        \n        ⟩\n        ≥\n        0\n      \n    \n    {\\displaystyle \\langle \\mathbf {v} ,\\mathbf {v} \\rangle \\geq 0}\n  \n\nwith equality only for v = 0.\nWe can define the length of a vector v in V by\n\n  \n    \n      \n        ‖\n        \n          v\n        \n        \n          ‖\n          \n            2\n          \n        \n        =\n        ⟨\n        \n          v\n        \n        ,\n        \n          v\n        \n        ⟩\n        ,\n      \n    \n    {\\displaystyle \\|\\mathbf {v} \\|^{2}=\\langle \\mathbf {v} ,\\mathbf {v} \\rangle ,}\n  \n\nand we can prove the Cauchy–Schwarz inequality:\n\n  \n    \n      \n        \n          |\n        \n        ⟨\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        \n          |\n        \n        ≤\n        ‖\n        \n          u\n        \n        ‖\n        ⋅\n        ‖\n        \n          v\n        \n        ‖\n        .\n      \n    \n    {\\displaystyle |\\langle \\mathbf {u} ,\\mathbf {v} \\rangle |\\leq \\|\\mathbf {u} \\|\\cdot \\|\\mathbf {v} \\|.}\n  \n\nIn particular, the quantity\n\n  \n    \n      \n        \n          \n            \n              \n                |\n              \n              ⟨\n              \n                u\n              \n              ,\n              \n                v\n              \n              ⟩\n              \n                |\n              \n            \n            \n              ‖\n              \n                u\n              \n              ‖\n              ⋅\n              ‖\n              \n                v\n              \n              ‖\n            \n          \n        \n        ≤\n        1\n        ,\n      \n    \n    {\\displaystyle {\\frac {|\\langle \\mathbf {u} ,\\mathbf {v} \\rangle |}{\\|\\mathbf {u} \\|\\cdot \\|\\mathbf {v} \\|}}\\leq 1,}\n  \n\nand so we can call this quantity the cosine of the angle between the two vectors.\nTwo vectors are orthogonal if ⟨u, v⟩ = 0. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly easy to deal with, since if v = a1 v1 + ⋯ + an vn, then\n\n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n        =\n        ⟨\n        \n          v\n        \n        ,\n        \n          \n            v\n          \n          \n            i\n          \n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle a_{i}=\\langle \\mathbf {v} ,\\mathbf {v} _{i}\\rangle .}\n  \n\nThe inner product facilitates the construction of many useful concepts. For instance, given a transform T, we can define its Hermitian conjugate T* as the linear transform satisfying\n\n  \n    \n      \n        ⟨\n        T\n        \n          u\n        \n        ,\n        \n          v\n        \n        ⟩\n        =\n        ⟨\n        \n          u\n        \n        ,\n        \n          T\n          \n            ∗\n          \n        \n        \n          v\n        \n        ⟩\n        .\n      \n    \n    {\\displaystyle \\langle T\\mathbf {u} ,\\mathbf {v} \\rangle =\\langle \\mathbf {u} ,T^{*}\\mathbf {v} \\rangle .}\n  \n\nIf T satisfies TT* = T*T, we call T normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V.\n\n\n== Relationship with geometry ==\nThere is a strong relationship between linear algebra and geometry, which started with the introduction by René Descartes, in 1637, of Cartesian coordinates. In this new (at that time) geometry, now called Cartesian geometry, points are represented by Cartesian coordinates, which are sequences of three real numbers (in the case of the usual three-dimensional space). The basic objects of geometry, which are lines and planes are represented by linear equations. Thus, computing intersections of lines and planes amounts to solving systems of linear equations. This was one of the main motivations for developing linear algebra.\nMost geometric transformation, such as translations, rotations, reflections, rigid motions, isometries, and projections transform lines into lines. It follows that they can be defined, specified, and studied in terms of linear maps. This is also the case of homographies and Möbius transformations when considered as transformations of a projective space.\nUntil the end of the 19th century, geometric spaces were defined by axioms relating points, lines, and planes (synthetic geometry). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, Projective space and Affine space). It has been shown that the two approaches are essentially equivalent. In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including finite fields.\nPresently, most textbooks introduce geometric spaces from linear algebra, and geometry is often presented, at the elementary level, as a subfield of linear algebra.\n\n\n== Usage and applications ==\nLinear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.\n\n\n=== Functional analysis ===\nFunctional analysis studies function spaces. These are vector spaces with additional structure, such as Hilbert spaces. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, quantum mechanics (wave functions) and Fourier analysis (orthogonal basis).\n\n\n=== Scientific computation ===\nNearly all scientific computations involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. BLAS and LAPACK are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, to adapt them to the specificities of the computer (cache size, number of available cores, ...).\nSince the 1960s there have been processors with specialized instructions for optimizing the operations of linear algebra, optional array processors under the control of a conventional processor, supercomputers designed for array processing and conventional processors augmented with vector registers.\nSome contemporary processors, typically graphics processing units (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.\n\n\n=== Geometry of ambient space ===\nThe modeling of ambient space is based on geometry. Sciences concerned with this space use geometry widely. This is the case with mechanics and robotics, for describing rigid body dynamics; geodesy for describing Earth shape; perspectivity, computer vision, and computer graphics, for describing the relationship between a scene and its plane representation; and many other scientific domains.\nIn all these applications, synthetic geometry is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with coordinates. This requires the heavy use of linear algebra.\n\n\n=== Study of complex systems ===\n\nMost physical phenomena are modeled by partial differential equations. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting cells. For linear systems this interaction involves linear functions. For nonlinear systems, this interaction is often approximated by linear functions.This is called a linear model or first-order approximation. Linear models are frequently used for complex nonlinear real-world systems because they make parametrization more manageable. In both cases, very large matrices are generally involved. Weather forecasting (or more specifically, parametrization for atmospheric modeling) is a typical example of a real-world application, where the whole Earth atmosphere is divided into cells of, say, 100 km of width and 100 km of height.\n\n\n=== Fluid mechanics, fluid dynamics, and thermal energy systems ===\n\nLinear algebra, a branch of mathematics dealing with vector spaces and linear mappings between these spaces, plays a critical role in various engineering disciplines, including fluid mechanics, fluid dynamics, and thermal energy systems. Its application in these fields is multifaceted and indispensable for solving complex problems.\nIn fluid mechanics, linear algebra is integral to understanding and solving problems related to the behavior of fluids. It assists in the modeling and simulation of fluid flow, providing essential tools for the analysis of fluid dynamics problems. For instance, linear algebraic techniques are used to solve systems of differential equations that describe fluid motion. These equations, often complex and non-linear, can be linearized using linear algebra methods, allowing for simpler solutions and analyses.\nIn the field of fluid dynamics, linear algebra finds its application in computational fluid dynamics (CFD), a branch that uses numerical analysis and data structures to solve and analyze problems involving fluid flows. CFD relies heavily on linear algebra for the computation of fluid flow and heat transfer in various applications. For example, the Navier–Stokes equations, fundamental in fluid dynamics, are often solved using techniques derived from linear algebra. This includes the use of matrices and vectors to represent and manipulate fluid flow fields.\nFurthermore, linear algebra plays a crucial role in thermal energy systems, particularly in power systems analysis. It is used to model and optimize the generation, transmission, and distribution of electric power. Linear algebraic concepts such as matrix operations and eigenvalue problems are employed to enhance the efficiency, reliability, and economic performance of power systems. The application of linear algebra in this context is vital for the design and operation of modern power systems, including renewable energy sources and smart grids.\nOverall, the application of linear algebra in fluid mechanics, fluid dynamics, and thermal energy systems is an example of the profound interconnection between mathematics and engineering. It provides engineers with the necessary tools to model, analyze, and solve complex problems in these domains, leading to advancements in technology and industry.\n\n\n== Extensions and generalizations ==\nThis section presents several related topics that do not appear generally in elementary textbooks on linear algebra but are commonly considered, in advanced mathematics, as parts of linear algebra.\n\n\n=== Module theory ===\n\nThe existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a ring R, and this gives the structure called a module over R, or R-module.\nThe concepts of linear independence, span, basis, and linear maps (also called module homomorphisms) are defined for modules exactly as for vector spaces, with the essential difference that, if R is not a field, there are modules that do not have any basis. The modules that have a basis are the free modules, and those that are spanned by a finite set are the finitely generated modules. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that determinants exist only if the ring is commutative, and that a square matrix over a commutative ring is invertible only if its determinant has a multiplicative inverse in the ring.\nVector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a cokernel of a homomorphism of free modules.\nModules over the integers can be identified with abelian groups, since the multiplication by an integer may be identified as a repeated addition. Most of the theory of abelian groups may be extended to modules over a principal ideal domain. In particular, over a principal ideal domain, every submodule of a free module is free, and the fundamental theorem of finitely generated abelian groups may be extended straightforwardly to finitely generated modules over a principal ring.\nThere are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a computational complexity that is much higher than similar algorithms over a field. For more details, see Linear equation over a ring.\n\n\n=== Multilinear algebra and tensors ===\n\nIn multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of several different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space V* consisting of linear maps f : V → F where F is the field of scalars. Multilinear maps T : Vn → F can be described via tensor products of elements of V*.\nIf, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V, the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).\n\n\n=== Topological vector spaces ===\n\nVector spaces that are not finite-dimensional often require additional structure to be tractable. A normed vector space is a vector space along with a function called a norm, which measures the \"size\" of elements. The norm induces a metric, which measures the distance between elements, and induces a topology, which allows for a definition of continuous maps. The metric also allows for a definition of limits and completeness – a normed vector space that is complete is known as a Banach space. A complete metric space along with the additional structure of an inner product (a conjugate symmetric sesquilinear form) is known as a Hilbert space, which is in some sense a particularly well-behaved Banach space. Functional analysis applies the methods of linear algebra alongside those of mathematical analysis to study various function spaces; the central objects of study in functional analysis are Lp spaces, which are Banach spaces, and especially the L2 space of square-integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods.\n\n\n== See also ==\nFundamental matrix (computer vision)\nGeometric algebra\nLinear programming\nLinear regression, a statistical estimation method\nNumerical linear algebra\nOutline of linear algebra\nTransformation matrix\n\n\n== Explanatory notes ==\n\n\n== Citations ==\n\n\n== General and cited sources ==\n\n\n== Further reading ==\n\n\n=== History ===\nFearnley-Sander, Desmond, \"Hermann Grassmann and the Creation of Linear Algebra\", American Mathematical Monthly 86 (1979), pp. 809–817.\nGrassmann, Hermann (1844), Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert, Leipzig: O. Wigand\n\n\n=== Introductory textbooks ===\nAnton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International\nBanerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics, Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN 978-1420095388\nBretscher, Otto (2004), Linear Algebra with Applications (3rd ed.), Prentice Hall, ISBN 978-0-13-145334-0\nFarin, Gerald; Hansford, Dianne (2004), Practical Linear Algebra: A Geometry Toolbox, AK Peters, ISBN 978-1-56881-234-2\nHefferon, Jim (2020). Linear Algebra (4th ed.). Ann Arbor, Michigan: Orthogonal Publishing. ISBN 978-1-944325-11-4. OCLC 1178900366. OL 30872051M.\nKolman, Bernard; Hill, David R. (2007), Elementary Linear Algebra with Applications (9th ed.), Prentice Hall, ISBN 978-0-13-229654-0\nLay, David C. (2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN 978-0-321-28713-7\nLeon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall, ISBN 978-0-13-185785-8\nMurty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry, World Scientific Publishing, ISBN 978-981-4366-62-5. Chapter 1: Systems of Simultaneous Linear Equations\nNoble, B. & Daniel, J.W. (2nd Ed. 1977) [1], Pearson Higher Education, ISBN 978-0130413437.\nPoole, David (2010), Linear Algebra: A Modern Introduction (3rd ed.), Cengage – Brooks/Cole, ISBN 978-0-538-73545-2\nRicardo, Henry (2010), A Modern Introduction To Linear Algebra (1st ed.), CRC Press, ISBN 978-1-4398-0040-9\nSadun, Lorenzo (2008), Applied Linear Algebra: the decoupling principle (2nd ed.), AMS, ISBN 978-0-8218-4441-0\nStrang, Gilbert (2016), Introduction to Linear Algebra (5th ed.), Wellesley-Cambridge Press, ISBN 978-09802327-7-6\nThe Manga Guide to Linear Algebra (2012), by Shin Takahashi, Iroha Inoue and Trend-Pro Co., Ltd., ISBN 978-1-59327-413-9\n\n\n=== Advanced textbooks ===\nBhatia, Rajendra (November 15, 1996), Matrix Analysis, Graduate Texts in Mathematics, Springer, ISBN 978-0-387-94846-1\nDemmel, James W. (August 1, 1997), Applied Numerical Linear Algebra, SIAM, ISBN 978-0-89871-389-3\nDym, Harry (2007), Linear Algebra in Action, AMS, ISBN 978-0-8218-3813-6\nGantmacher, Felix R. (2005), Applications of the Theory of Matrices, Dover Publications, ISBN 978-0-486-44554-0\nGantmacher, Felix R. (1990), Matrix Theory Vol. 1 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-1376-8\nGantmacher, Felix R. (2000), Matrix Theory Vol. 2 (2nd ed.), American Mathematical Society, ISBN 978-0-8218-2664-5\nGelfand, Israel M. (1989), Lectures on Linear Algebra, Dover Publications, ISBN 978-0-486-66082-0\nGlazman, I. M.; Ljubic, Ju. I. (2006), Finite-Dimensional Linear Analysis, Dover Publications, ISBN 978-0-486-45332-3\nGolan, Johnathan S. (January 2007), The Linear Algebra a Beginning Graduate Student Ought to Know (2nd ed.), Springer, ISBN 978-1-4020-5494-5\nGolan, Johnathan S. (August 1995), Foundations of Linear Algebra, Kluwer, ISBN 0-7923-3614-3\nGreub, Werner H. (October 16, 1981), Linear Algebra, Graduate Texts in Mathematics (4th ed.), Springer, ISBN 978-0-8018-5414-9\nHoffman, Kenneth; Kunze, Ray (1971), Linear algebra (2nd ed.), Englewood Cliffs, N.J.: Prentice-Hall, Inc., MR 0276251\nHalmos, Paul R. (August 20, 1993), Finite-Dimensional Vector Spaces, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-90093-3\nFriedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (September 7, 2018), Linear Algebra (5th ed.), Pearson, ISBN 978-0-13-486024-4\nHorn, Roger A.; Johnson, Charles R. (February 23, 1990), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6\nHorn, Roger A.; Johnson, Charles R. (June 24, 1994), Topics in Matrix Analysis, Cambridge University Press, ISBN 978-0-521-46713-1\nLang, Serge (March 9, 2004), Linear Algebra, Undergraduate Texts in Mathematics (3rd ed.), Springer, ISBN 978-0-387-96412-6\nMarcus, Marvin; Minc, Henryk (2010), A Survey of Matrix Theory and Matrix Inequalities, Dover Publications, ISBN 978-0-486-67102-4\nMeyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra, Society for Industrial and Applied Mathematics (SIAM), ISBN 978-0-89871-454-8, archived from the original on October 31, 2009\nMirsky, L. (1990), An Introduction to Linear Algebra, Dover Publications, ISBN 978-0-486-66434-7\nShafarevich, I. R.; Remizov, A. O (2012), Linear Algebra and Geometry, Springer, ISBN 978-3-642-30993-9\nShilov, Georgi E. (June 1, 1977), Linear algebra, Dover Publications, ISBN 978-0-486-63518-7\nShores, Thomas S. (December 6, 2006), Applied Linear Algebra and Matrix Analysis, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-33194-2\nSmith, Larry (May 28, 1998), Linear Algebra, Undergraduate Texts in Mathematics, Springer, ISBN 978-0-387-98455-1\nTrefethen, Lloyd N.; Bau, David (1997), Numerical Linear Algebra, SIAM, ISBN 978-0-898-71361-9\n\n\n=== Study guides and outlines ===\nLeduc, Steven A. (May 1, 1996), Linear Algebra (Cliffs Quick Review), Cliffs Notes, ISBN 978-0-8220-5331-6\nLipschutz, Seymour; Lipson, Marc (December 6, 2000), Schaum's Outline of Linear Algebra (3rd ed.), McGraw-Hill, ISBN 978-0-07-136200-9\nLipschutz, Seymour (January 1, 1989), 3,000 Solved Problems in Linear Algebra, McGraw–Hill, ISBN 978-0-07-038023-3\nMcMahon, David (October 28, 2005), Linear Algebra Demystified, McGraw–Hill Professional, ISBN 978-0-07-146579-3\nZhang, Fuzhen (April 7, 2009), Linear Algebra: Challenging Problems for Students, The Johns Hopkins University Press, ISBN 978-0-8018-9125-0\n\n\n== External links ==\n\n\n=== Online Resources ===\n\nMIT Linear Algebra Video Lectures, a series of 34 recorded lectures by Professor Gilbert Strang (Spring 2010)\nInternational Linear Algebra Society\n\"Linear algebra\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\nLinear Algebra on MathWorld\nMatrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics\nEarliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols\nEssence of linear algebra, a video presentation from 3Blue1Brown of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view\n\n\n=== Online books ===\nBeezer, Robert A. (2009) [2004]. A First Course in Linear Algebra. Gainesville, Florida: University Press of Florida. ISBN 9781616100049.\nConnell, Edwin H. (2004) [1999]. Elements of Abstract and Linear Algebra. University of Miami, Coral Gables, Florida: Self-published.\nHefferon, Jim (2020). Linear Algebra (4th ed.). Ann Arbor, Michigan: Orthogonal Publishing. ISBN 978-1-944325-11-4. OCLC 1178900366. OL 30872051M.\nMargalit, Dan; Rabinoff, Joseph (2019). Interactive Linear Algebra. Georgia Institute of Technology, Atlanta, Georgia: Self-published.\nMatthews, Keith R. (2013) [1991]. Elementary Linear Algebra. University of Queensland, Brisbane, Australia: Self-published.\nMikaelian, Vahagn H. (2020) [2017]. Linear Algebra: Theory and Algorithms. Yerevan, Armenia: Self-published – via ResearchGate.\nSharipov, Ruslan, Course of linear algebra and multidimensional geometry\nTreil, Sergei, Linear Algebra Done Wrong"
    },
    {
        "id": "discrete_math",
        "name": "Discrete mathematics",
        "text": "Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic. By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".\nThe set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.\nResearch in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in \"discrete\" steps and store data in \"discrete\" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems.\nAlthough the main objects of study in discrete mathematics are discrete objects, analytic methods from \"continuous\" mathematics are often employed as well.\nIn university curricula, discrete mathematics appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, like precalculus in this respect.\nThe Fulkerson Prize is awarded for outstanding papers in discrete mathematics.\n\n\n== Topics ==\n\n\n=== Theoretical computer science ===\n\nTheoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical computer science is the study of algorithms and data structures. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time, space, and other resources taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems and representations of geometrical objects, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.\n\n\n=== Information theory ===\n\nInformation theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.\n\n\n=== Logic ===\n\nLogic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce's law (((P→Q)→P)→P) is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has accumulated to automated theorem proving and formal verification of software.\nLogical formulas are discrete structures, as are proofs, which form finite trees or, more generally, directed acyclic graph structures (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: true and false, but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied, e.g. infinitary logic.\n\n\n=== Set theory ===\n\nSet theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.\nIn discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.\n\n\n=== Combinatorics ===\n\nCombinatorics studies the ways in which discrete structures can be combined or arranged.\nEnumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.\nAnalytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\nTopological combinatorics concerns the use of techniques from topology and algebraic topology/combinatorial topology in combinatorics.\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.\nOrder theory is the study of partially ordered sets, both finite and infinite.\n\n\n=== Graph theory ===\n\nGraph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right. Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory and topological graph theory has close links to topology. There are also continuous graphs; however, for the most part, research in graph theory falls within the domain of discrete mathematics.\n\n\n=== Number theory ===\n\nNumber theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography and cryptanalysis, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.\n\n\n=== Algebraic structures ===\n\nAlgebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: Boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.\n\n\n=== Discrete analogues of continuous mathematics ===\nThere are many concepts and theories in continuous mathematics which have discrete versions, such as discrete calculus, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, discrete optimization, discrete probability theory, discrete probability distribution, difference equations, discrete dynamical systems, and discrete vector measures.\n\n\n==== Calculus of finite differences, discrete analysis, and discrete calculus ====\nIn discrete calculus and the calculus of finite differences, a function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance, where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals, there are discrete transforms for discrete functions or digital signals. As well as discrete metric spaces, there are more general discrete topological spaces, finite metric spaces, finite topological spaces.\nThe time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical systems.\n\n\n==== Discrete geometry ====\nDiscrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane.\nIn algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form \n  \n    \n      \n        V\n        (\n        x\n        −\n        c\n        )\n        ⊂\n        Spec\n        ⁡\n        K\n        [\n        x\n        ]\n        =\n        \n          \n            A\n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle V(x-c)\\subset \\operatorname {Spec} K[x]=\\mathbb {A} ^{1}}\n  \n for \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n a field can be studied either as \n  \n    \n      \n        Spec\n        ⁡\n        K\n        [\n        x\n        ]\n        \n          /\n        \n        (\n        x\n        −\n        c\n        )\n        ≅\n        Spec\n        ⁡\n        K\n      \n    \n    {\\displaystyle \\operatorname {Spec} K[x]/(x-c)\\cong \\operatorname {Spec} K}\n  \n, a point, or as the spectrum \n  \n    \n      \n        Spec\n        ⁡\n        K\n        [\n        x\n        \n          ]\n          \n            (\n            x\n            −\n            c\n            )\n          \n        \n      \n    \n    {\\displaystyle \\operatorname {Spec} K[x]_{(x-c)}}\n  \n of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.\n\n\n==== Discrete modelling ====\nIn applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation. Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.\n\n\n== Challenges ==\n\nThe history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).\nIn logic, the second problem on David Hilbert's list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gödel's second incompleteness theorem, proved in 1931, showed that this was not possible – at least not within arithmetic itself. Hilbert's tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.\nThe need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. The telecommunications industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.\nComputational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.\nSeveral fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.\nCurrently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.\n\n\n== See also ==\n\nOutline of discrete mathematics\nCyberchase, a show that teaches discrete mathematics to children\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\nDiscrete mathematics Archived 2011-08-29 at the Wayback Machine at the utk.edu Mathematics Archives, providing links to syllabi, tutorials, programs, etc.\nIowa Central: Electrical Technologies Program Discrete mathematics for Electrical engineering."
    },
    {
        "id": "probability",
        "name": "Probability theory",
        "text": "Error fetching article: Page id \"proability theory\" does not match any pages. Try another id!"
    },
    {
        "id": "statistics",
        "name": "Statistics",
        "text": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.\nWhen census data (comprising every member of the target population) cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.\nStatistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\n== Introduction ==\n\n\"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.\nStatistics is regarded as a body of science or a branch of mathematics. It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics was once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.\nSome consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.\nThe word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works. In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.\n\n\n== Statistical data ==\n\n\n=== Data collection ===\n\n\n==== Sampling ====\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.\nTo use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\n\n\n==== Experimental and observational studies ====\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\n\n===== Experiments =====\nThe basic steps of a statistical experiment are:\n\nPlanning the research, including finding the number of replicates of the study, using the following information:  preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.\nDesign of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.\nPerforming the experiment following the experimental protocol and analyzing the data following the experimental protocol.\nFurther examining the data set in secondary analyses, to suggest new hypotheses for future study.\nDocumenting and presenting the results of the study.\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.\n\n\n===== Observational study =====\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group. A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n\n\n=== Types of data ===\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998), van den Berg (1991).)\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\": 82 \n\n\n== Methods ==\n\n\n=== Descriptive statistics ===\n\nA descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.\n\n\n=== Inferential statistics ===\n\nStatistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\n\n\n==== Terminology and theory of inferential statistics ====\n\n\n===== Statistics, estimators and pivotal quantities =====\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\n\n===== Null hypothesis and alternative hypothesis =====\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\nWhat statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.\n\n\n===== Error =====\nWorking from a null hypothesis, two broad categories of error are recognized:\n\nType I errors where the null hypothesis is falsely rejected, giving a \"false positive\".\nType II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed, giving a \"false negative\".\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\nA statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\nMeasurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\n===== Interval estimation =====\n\nMost studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\n\n===== Significance =====\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\nThe standard approach is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\nAlthough in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nA difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to include the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to report confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.\nFallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is the probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.\nRejecting the null hypothesis does not automatically prove the alternative hypothesis.\nAs everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.\n\n\n===== Examples =====\nSome well-known statistical tests and procedures are:\n\n\n=== Exploratory data analysis ===\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n=== Mathematical statistics ===\n\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.\n\n\n== History ==\n\nFormal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels. Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.\nAlthough the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science. The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt. Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nThe mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi. This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis. The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages. The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others. Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment, the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things. Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.\nThe second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments, where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information. He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\". In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway, a concept in sexual selection about a positive feedback runaway effect found in evolution.\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.\n\n\n== Applications ==\n\n\n=== Applied statistics, theoretical statistics and mathematical statistics ===\nApplied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\nStatistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.\n\n\n=== Machine learning and data mining ===\nMachine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.\n\n\n=== Statistics in academia ===\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.\nA typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation. Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.\n\n\n=== Statistical computing ===\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n\n\n=== Business statistics ===\n\nIn business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management. Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\nA typical \"Business Statistics\" course is intended for business majors, and covers descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\n\n\n== Specialized disciplines ==\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.\n\n\n== Misuse ==\n\nMisuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter. A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics, by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias. Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias. Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs. Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented. To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole. According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:\n\nWho says so? (Does he/she have an axe to grind?)\nHow does he/she know? (Does he/she have the resources to know the facts?)\nWhat's missing? (Does he/she give us a complete picture?)\nDid someone change the subject? (Does he/she offer us the right answer to the wrong problem?)\nDoes it make sense? (Is his/her conclusion logical and consistent with what we already know?)\n\n\n=== Misinterpretation: correlation ===\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.\n\n\n== See also ==\n\nFoundations and major areas of statistics\n\n\n== References ==\n\n\n== Further reading ==\n\nLydia Denworth, \"A Significant Problem: Standard scientific methods are under fire. Will anything change?\", Scientific American, vol. 321, no. 4 (October 2019), pp. 62–67. \"The use of p values for nearly a century [since 1925] to determine statistical significance of experimental results has contributed to an illusion of certainty and [to] reproducibility crises in many scientific fields. There is growing determination to reform statistical analysis... Some [researchers] suggest changing statistical methods, whereas others would do away with a threshold for defining \"significant\" results\". (p. 63.)\nBarbara Illowsky; Susan Dean (2014). Introductory Statistics. OpenStax CNX. ISBN 978-1938168208.\nStockburger, David W. \"Introductory Statistics: Concepts, Models, and Applications\". Missouri State University (3rd Web ed.). Archived from the original on 28 May 2020.\nOpenIntro Statistics Archived 2019-06-16 at the Wayback Machine, 3rd edition by Diez, Barr, and Cetinkaya-Rundel\nStephen Jones, 2010. Statistics in Psychology: Explanations without Equations. Palgrave Macmillan. ISBN 978-1137282392.\nCohen, J (1990). \"Things I have learned (so far)\" (PDF). American Psychologist. 45 (12): 1304–1312. doi:10.1037/0003-066x.45.12.1304. S2CID 7180431. Archived from the original (PDF) on 2017-10-18.\nGigerenzer, G (2004). \"Mindless statistics\". Journal of Socio-Economics. 33 (5): 587–606. doi:10.1016/j.socec.2004.09.033.\nIoannidis, J.P.A. (2005). \"Why most published research findings are false\". PLOS Medicine. 2 (4): 696–701. doi:10.1371/journal.pmed.0040168. PMC 1855693. PMID 17456002.\n\n\n== External links ==\n\n(Electronic Version): TIBCO Software Inc. (2020). Data Science Textbook.\nOnline Statistics Education: An Interactive Multimedia Course of Study. Developed by Rice University (Lead Developer), University of Houston Clear Lake, Tufts University, and National Science Foundation.\nUCLA Statistical Computing Resources (archived 17 July 2006)\nPhilosophy of Statistics from the Stanford Encyclopedia of Philosophy"
    },
    {
        "id": "machine_learning",
        "name": "Machine learning",
        "text": "Error fetching article: Page id \"machine ;earning\" does not match any pages. Try another id!"
    },
    {
        "id": "classical_mechanics",
        "name": "Classical mechanics",
        "text": "Classical mechanics is a physical theory describing the motion of objects such as projectiles, parts of machinery, spacecraft, planets, stars, and galaxies. The development of classical mechanics involved substantial change in the methods and philosophy of physics. The qualifier classical distinguishes this type of mechanics from physics developed after the revolutions in physics of the early 20th century, all of which revealed limitations in classical mechanics.\nThe earliest formulation of classical mechanics is often referred to as Newtonian mechanics. It consists of the physical concepts based on the 17th century foundational works of Sir Isaac Newton, and the mathematical methods invented by Newton, Gottfried Wilhelm Leibniz, Leonhard Euler and others to describe the motion of bodies under the influence of forces. Later, methods based on energy were developed by Euler, Joseph-Louis Lagrange, William Rowan Hamilton and others, leading to the development of analytical mechanics (which includes Lagrangian mechanics and Hamiltonian mechanics). These advances, made predominantly in the 18th and 19th centuries, extended beyond earlier works; they are, with some modification, used in all areas of modern physics.\nIf the present state of an object that obeys the laws of classical mechanics is known, it is possible to determine how it will move in the future, and how it has moved in the past. Chaos theory shows that the long term predictions of classical mechanics are not reliable. Classical mechanics provides accurate results when studying objects that are not extremely massive and have speeds not approaching the speed of light. With objects  about the size of an atom's diameter, it becomes necessary to use quantum mechanics. To describe velocities approaching the speed of light, special relativity is needed. In cases where objects become extremely massive, general relativity becomes applicable. Some modern sources include relativistic mechanics in classical physics, as representing the field in its most developed and accurate form.\n\n\n== Branches ==\n\n\n=== Traditional division ===\nClassical mechanics was traditionally divided into three main branches. \nStatics is the branch of classical mechanics that is concerned with the analysis of force and torque acting on a physical system that does not experience an acceleration, but rather is in equilibrium with its environment.  Kinematics describes the motion of points, bodies (objects), and systems of bodies (groups of objects) without considering the forces that cause them to move. Kinematics, as a field of study, is often referred to as the \"geometry of motion\" and is occasionally seen as a branch of mathematics. Dynamics goes beyond merely describing objects' behavior and also considers the forces which explain it.\nSome authors (for example, Taylor (2005) and Greenwood (1997)) include special relativity within classical dynamics.\n\n\n=== Forces vs. energy ===\nAnother division is based on the choice of mathematical formalism. Classical mechanics can be mathematically presented in multiple different ways. The physical content of these different formulations is the same, but they provide different insights and facilitate different types of calculations. While the term \"Newtonian mechanics\" is sometimes used as a synonym for non-relativistic classical physics, it can also refer to a particular formalism based on Newton's laws of motion. Newtonian mechanics in this sense emphasizes force as a vector quantity.\nIn contrast, analytical mechanics uses scalar properties of motion representing the system as a whole—usually its kinetic energy and potential energy. The equations of motion are derived from the scalar quantity by some underlying principle about the scalar's variation. Two dominant branches of analytical mechanics are Lagrangian mechanics, which uses generalized coordinates and corresponding generalized velocities in configuration space, and Hamiltonian mechanics, which uses coordinates and corresponding momenta in phase space. Both formulations are equivalent by a Legendre transformation on the generalized coordinates, velocities and momenta; therefore, both contain the same information for describing the dynamics of a system. There are other formulations such as Hamilton–Jacobi theory, Routhian mechanics, and Appell's equation of motion. All equations of motion for particles and fields, in any formalism, can be derived from the widely applicable result called the principle of least action. One result is Noether's theorem, a statement which connects conservation laws to their associated symmetries.\n\n\n=== By region of application ===\nAlternatively, a division can be made by region of application:\n\nCelestial mechanics, relating to stars, planets and other celestial bodies\nContinuum mechanics, for materials modelled as a continuum, e.g., solids and fluids (i.e., liquids and gases).\nRelativistic mechanics (i.e. including the special and general theories of relativity), for bodies whose speed is close to the speed of light.\nStatistical mechanics, which provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic or bulk thermodynamic properties of materials.\n\n\n== Description of objects and their motion ==\n\nFor simplicity, classical mechanics often models real-world objects as point particles, that is, objects with negligible size. The motion of a point particle is determined by a small number of parameters: its position, mass, and the forces applied to it. Classical mechanics also describes the more complex motions of extended non-pointlike objects. Euler's laws provide extensions to Newton's laws in this area. The concepts of angular momentum rely on the same calculus used to describe one-dimensional motion. The rocket equation extends the notion of rate of change of an object's momentum to include the effects of an object \"losing mass\". (These generalizations/extensions are derived from Newton's laws, say, by decomposing a solid body into a collection of points.)\nIn reality, the kind of objects that classical mechanics can describe always have a non-zero size. (The behavior of very small particles, such as the electron, is more accurately described by quantum mechanics.) Objects with non-zero size have more complicated behavior than hypothetical point particles, because of the additional degrees of freedom, e.g., a baseball can spin while it is moving. However, the results for point particles can be used to study such objects by treating them as composite objects, made of a large number of collectively acting point particles. The center of mass of a composite object behaves like a point particle.\nClassical mechanics assumes that matter and energy have definite, knowable attributes such as location in space and speed. Non-relativistic mechanics also assumes that forces act instantaneously (see also Action at a distance).\n\n\n=== Kinematics ===\n\nThe position of a point particle is defined in relation to a coordinate system centered on an arbitrary fixed reference point in space called the origin O. A simple coordinate system might describe the position of a particle P with a vector notated by an arrow labeled r that points from the origin O to point P. In general, the point particle does not need to be stationary relative to O. In cases where P is moving relative to O, r is defined as a function of t, time. In pre-Einstein relativity (known as Galilean relativity), time is considered an absolute, i.e., the time interval that is observed to elapse between any given pair of events is the same for all observers. In addition to relying on absolute time, classical mechanics assumes Euclidean geometry for the structure of space.\n\n\n==== Velocity and speed ====\n\nThe velocity, or the rate of change of displacement with time, is defined as the derivative of the position with respect to time:\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        \n      \n    \n    {\\displaystyle \\mathbf {v} ={\\mathrm {d} \\mathbf {r}  \\over \\mathrm {d} t}\\,\\!}\n  \n.\nIn classical mechanics, velocities are directly additive and subtractive. For example, if one car travels east at 60 km/h and passes another car traveling in the same direction at 50 km/h, the slower car perceives the faster car as traveling east at 60 − 50 = 10 km/h. However, from the perspective of the faster car, the slower car is moving 10 km/h to the west, often denoted as −10 km/h where the sign implies opposite direction. Velocities are directly additive as vector quantities; they must be dealt with using vector analysis.\nMathematically, if the velocity of the first object in the previous discussion is denoted by the vector u = ud and the velocity of the second object by the vector v = ve, where u is the speed of the first object, v is the speed of the second object, and d and e are unit vectors in the directions of motion of each object respectively, then the velocity of the first object as seen by the second object is:\n\n  \n    \n      \n        \n          \n            u\n          \n          ′\n        \n        =\n        \n          u\n        \n        −\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=\\mathbf {u} -\\mathbf {v} \\,.}\n  \n\nSimilarly, the first object sees the velocity of the second object as:\n\n  \n    \n      \n        \n          \n            v\n            ′\n          \n        \n        =\n        \n          v\n        \n        −\n        \n          u\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {v'} =\\mathbf {v} -\\mathbf {u} \\,.}\n  \n\nWhen both objects are moving in the same direction, this equation can be simplified to:\n\n  \n    \n      \n        \n          \n            u\n          \n          ′\n        \n        =\n        (\n        u\n        −\n        v\n        )\n        \n          d\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {u} '=(u-v)\\mathbf {d} \\,.}\n  \n\nOr, by ignoring direction, the difference can be given in terms of speed only:\n\n  \n    \n      \n        \n          u\n          ′\n        \n        =\n        u\n        −\n        v\n        \n        .\n      \n    \n    {\\displaystyle u'=u-v\\,.}\n  \n\n\n==== Acceleration ====\n\nThe acceleration, or rate of change of velocity, is the derivative of the velocity with respect to time (the second derivative of the position with respect to time):\n\n  \n    \n      \n        \n          a\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                \n                  d\n                  \n                    2\n                  \n                \n              \n              \n                r\n              \n            \n            \n              \n                d\n              \n              \n                t\n                \n                  2\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {a} ={\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}={\\mathrm {d^{2}} \\mathbf {r}  \\over \\mathrm {d} t^{2}}.}\n  \n\nAcceleration represents the velocity's change over time.  Velocity can change in magnitude, direction, or both. Occasionally, a decrease in the magnitude of velocity \"v\" is referred to as deceleration, but generally any change in the velocity over time, including deceleration, is referred to as acceleration.\n\n\n==== Frames of reference ====\n\nWhile the position, velocity and acceleration of a particle can be described with respect to any observer in any state of motion, classical mechanics assumes the existence of a special family of reference frames in which the mechanical laws of nature take a comparatively simple form. These special reference frames are called inertial frames. An inertial frame is an idealized frame of reference within which an object with zero net force acting upon it moves with a constant velocity; that is, it is either at rest or moving uniformly in a straight line. In an inertial frame Newton's law of motion, \n  \n    \n      \n        F\n        =\n        m\n        a\n      \n    \n    {\\displaystyle F=ma}\n  \n, is valid.: 185 \nNon-inertial reference frames accelerate in relation to another inertial frame. A body rotating with respect to an inertial frame is not an inertial frame. When viewed from an inertial frame, particles in the non-inertial frame appear to move in ways not explained by forces from existing fields in the reference frame. Hence, it appears that there are other forces that enter the equations of motion solely as a result of the relative acceleration. These forces are referred to as fictitious forces, inertia forces, or pseudo-forces.\nConsider two reference frames S and S'. For observers in each of the reference frames an event has space-time coordinates of (x,y,z,t) in frame S and (x',y',z',t') in frame S'. Assuming time is measured the same in all reference frames, if we require x = x' when t = 0, then the relation between the space-time coordinates of the same event observed from the reference frames S' and S, which are moving at a relative velocity u in the x direction, is:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  x\n                  ′\n                \n              \n              \n                \n                =\n                x\n                −\n                t\n                u\n                ,\n              \n            \n            \n              \n                \n                  y\n                  ′\n                \n              \n              \n                \n                =\n                y\n                ,\n              \n            \n            \n              \n                \n                  z\n                  ′\n                \n              \n              \n                \n                =\n                z\n                ,\n              \n            \n            \n              \n                \n                  t\n                  ′\n                \n              \n              \n                \n                =\n                t\n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}x'&=x-tu,\\\\y'&=y,\\\\z'&=z,\\\\t'&=t.\\end{aligned}}}\n  \n\nThis set of formulas defines a group transformation known as the Galilean transformation (informally, the Galilean transform). This group is a limiting case of the Poincaré group used in special relativity. The limiting case applies when the velocity u is very small compared to c, the speed of light.\nThe transformations have the following consequences:\n\nv′ = v − u (the velocity v′ of a particle from the perspective of S′ is slower by u than its velocity v from the perspective of S)\na′ = a (the acceleration of a particle is the same in any inertial reference frame)\nF′ = F (the force on a particle is the same in any inertial reference frame)\nthe  speed of light is not a constant in classical mechanics, nor does the special position given to the speed of light in relativistic mechanics have a counterpart in classical mechanics.\nFor some problems, it is convenient to use rotating coordinates (reference frames). Thereby one can either keep a mapping to a convenient inertial frame, or introduce additionally a fictitious centrifugal force and Coriolis force.\n\n\n== Newtonian mechanics ==\n\nA force in physics is any action that causes an object's velocity to change; that is, to accelerate.  A force originates from within a field, such as an electro-static field (caused by static electrical charges), electro-magnetic field (caused by moving charges), or gravitational field (caused by mass), among others.\nNewton was the first to mathematically express the relationship between force and momentum. Some physicists interpret Newton's second law of motion as a definition of force and mass, while others consider it a fundamental postulate, a law of nature. Either interpretation has the same mathematical consequences, historically known as \"Newton's Second Law\":\n\n  \n    \n      \n        \n          F\n        \n        =\n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              \n                d\n              \n              (\n              m\n              \n                v\n              \n              )\n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} ={\\mathrm {d} \\mathbf {p}  \\over \\mathrm {d} t}={\\mathrm {d} (m\\mathbf {v} ) \\over \\mathrm {d} t}.}\n  \n\nThe quantity mv is called the (canonical) momentum. The net force on a particle is thus equal to the rate of change of the momentum of the particle with time. Since the definition of acceleration is a = dv/dt, the second law can be written in the simplified and more familiar form:\n\n  \n    \n      \n        \n          F\n        \n        =\n        m\n        \n          a\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =m\\mathbf {a} \\,.}\n  \n\nSo long as the force acting on a particle is known, Newton's second law is sufficient to describe the motion of a particle. Once independent relations for each force acting on a particle are available, they can be substituted into Newton's second law to obtain an ordinary differential equation, which is called the equation of motion.\nAs an example, assume that friction is the only force acting on the particle, and that it may be modeled as a function of the velocity of the particle, for example:\n\n  \n    \n      \n        \n          \n            F\n          \n          \n            \n              R\n            \n          \n        \n        =\n        −\n        λ\n        \n          v\n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {F} _{\\rm {R}}=-\\lambda \\mathbf {v} \\,,}\n  \n\nwhere λ is a positive constant, the negative sign states that the force is opposite the sense of the velocity. Then the equation of motion is\n\n  \n    \n      \n        −\n        λ\n        \n          v\n        \n        =\n        m\n        \n          a\n        \n        =\n        m\n        \n          \n            \n              \n                d\n              \n              \n                v\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle -\\lambda \\mathbf {v} =m\\mathbf {a} =m{\\mathrm {d} \\mathbf {v}  \\over \\mathrm {d} t}\\,.}\n  \n\nThis can be integrated to obtain\n\n  \n    \n      \n        \n          v\n        \n        =\n        \n          \n            v\n          \n          \n            0\n          \n        \n        \n          e\n          \n            \n              −\n              λ\n              t\n            \n            \n              /\n            \n            \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {v} =\\mathbf {v} _{0}e^{{-\\lambda t}/{m}}}\n  \n\nwhere v0 is the initial velocity. This means that the velocity of this particle decays exponentially to zero as time progresses. In this case, an equivalent viewpoint is that the kinetic energy of the particle is absorbed by friction (which converts it to heat energy in accordance with the conservation of energy), and the particle is slowing down. This expression can be further integrated to obtain the position r of the particle as a function of time.\nImportant forces include the gravitational force and the Lorentz force for electromagnetism. In addition, Newton's third law can sometimes be used to deduce the forces acting on a particle: if it is known that particle A exerts a force F on another particle B, it follows that B must exert an equal and opposite reaction force, −F, on A. The strong form of Newton's third law requires that F and −F act along the line connecting A and B, while the weak form does not. Illustrations of the weak form of Newton's third law are often found for magnetic forces.\n\n\n=== Work and energy ===\n\nIf a constant force F is applied to a particle that makes a displacement Δr, the work done by the force is defined as the scalar product of the force and displacement vectors:\n\n  \n    \n      \n        W\n        =\n        \n          F\n        \n        ⋅\n        Δ\n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\mathbf {F} \\cdot \\Delta \\mathbf {r} \\,.}\n  \n\nMore generally, if the force varies as a function of position as the particle moves from r1 to r2 along a path C, the work done on the particle is given by the line integral\n\n  \n    \n      \n        W\n        =\n        \n          ∫\n          \n            C\n          \n        \n        \n          F\n        \n        (\n        \n          r\n        \n        )\n        ⋅\n        \n          d\n        \n        \n          r\n        \n        \n        .\n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} (\\mathbf {r} )\\cdot \\mathrm {d} \\mathbf {r} \\,.}\n  \n\nIf the work done in moving the particle from r1 to r2 is the same no matter what path is taken, the force is said to be conservative. Gravity is a conservative force, as is the force due to an idealized spring, as given by Hooke's law. The force due to friction is non-conservative.\nThe kinetic energy Ek of a particle of mass m travelling at speed v is given by\n\n  \n    \n      \n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          v\n          \n            2\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle E_{\\mathrm {k} }={\\tfrac {1}{2}}mv^{2}\\,.}\n  \n\nFor extended objects composed of many particles, the kinetic energy of the composite body is the sum of the kinetic energies of the particles.\nThe work–energy theorem states that for a particle of constant mass m, the total work W done on the particle as it moves from position r1 to r2 is equal to the change in kinetic energy Ek of the particle:\n\n  \n    \n      \n        W\n        =\n        Δ\n        \n          E\n          \n            \n              k\n            \n          \n        \n        =\n        \n          E\n          \n            \n              \n                k\n                \n                  2\n                \n              \n            \n          \n        \n        −\n        \n          E\n          \n            \n              \n                k\n                \n                  1\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        m\n        \n          (\n          \n            \n              v\n              \n                2\n              \n              \n                \n                2\n              \n            \n            −\n            \n              v\n              \n                1\n              \n              \n                \n                2\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle W=\\Delta E_{\\mathrm {k} }=E_{\\mathrm {k_{2}} }-E_{\\mathrm {k_{1}} }={\\tfrac {1}{2}}m\\left(v_{2}^{\\,2}-v_{1}^{\\,2}\\right).}\n  \n\nConservative forces can be expressed as the gradient of a scalar function, known as the potential energy and denoted Ep:\n\n  \n    \n      \n        \n          F\n        \n        =\n        −\n        \n          ∇\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\,.}\n  \n\nIf all the forces acting on a particle are conservative, and Ep is the total potential energy (which is defined as a work of involved forces to rearrange mutual positions of bodies), obtained by summing the potential energies corresponding to each force\n\n  \n    \n      \n        \n          F\n        \n        ⋅\n        Δ\n        \n          r\n        \n        =\n        −\n        \n          ∇\n        \n        \n          E\n          \n            \n              p\n            \n          \n        \n        ⋅\n        Δ\n        \n          r\n        \n        =\n        −\n        Δ\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {F} \\cdot \\Delta \\mathbf {r} =-\\mathbf {\\nabla } E_{\\mathrm {p} }\\cdot \\Delta \\mathbf {r} =-\\Delta E_{\\mathrm {p} }\\,.}\n  \n\nThe decrease in the potential energy is equal to the increase in the kinetic energy\n\n  \n    \n      \n        −\n        Δ\n        \n          E\n          \n            \n              p\n            \n          \n        \n        =\n        Δ\n        \n          E\n          \n            \n              k\n            \n          \n        \n        ⇒\n        Δ\n        (\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        )\n        =\n        0\n        \n        .\n      \n    \n    {\\displaystyle -\\Delta E_{\\mathrm {p} }=\\Delta E_{\\mathrm {k} }\\Rightarrow \\Delta (E_{\\mathrm {k} }+E_{\\mathrm {p} })=0\\,.}\n  \n\nThis result is known as conservation of energy and states that the total energy,\n\n  \n    \n      \n        ∑\n        E\n        =\n        \n          E\n          \n            \n              k\n            \n          \n        \n        +\n        \n          E\n          \n            \n              p\n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\sum E=E_{\\mathrm {k} }+E_{\\mathrm {p} }\\,,}\n  \n\nis constant in time. It is often useful, because many commonly encountered forces are conservative.\n\n\n== Lagrangian mechanics ==\n\nLagrangian mechanics is a formulation of classical mechanics founded on the stationary-action principle (also known as the principle of least action). It was introduced by the Italian-French mathematician and astronomer Joseph-Louis Lagrange in his presentation to the Turin Academy of Science in 1760 culminating in his 1788 grand opus, Mécanique analytique. Lagrangian mechanics describes a mechanical system as a pair \n  \n    \n      \n        (\n        M\n        ,\n        L\n        )\n      \n    \n    {\\textstyle (M,L)}\n  \n consisting of a configuration space \n  \n    \n      \n        M\n      \n    \n    {\\textstyle M}\n  \n and a smooth function \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n  \n within that space called a Lagrangian. For many systems, \n  \n    \n      \n        L\n        =\n        T\n        −\n        V\n        ,\n      \n    \n    {\\textstyle L=T-V,}\n  \n where \n  \n    \n      \n        T\n      \n    \n    {\\textstyle T}\n  \n and \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n are the kinetic and potential energy of the system, respectively. The stationary action principle requires that the action functional of the system derived from \n  \n    \n      \n        L\n      \n    \n    {\\textstyle L}\n  \n must remain at a stationary point (a maximum, minimum, or saddle) throughout the time evolution of the system. This constraint allows the calculation of the equations of motion of the system using Lagrange's equations.\n\n\n== Hamiltonian mechanics ==\n\nHamiltonian mechanics emerged in 1833 as a reformulation of Lagrangian mechanics. Introduced by Sir William Rowan Hamilton, Hamiltonian mechanics replaces (generalized) velocities \n  \n    \n      \n        \n          \n            \n              \n                q\n                ˙\n              \n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\dot {q}}^{i}}\n  \n used in Lagrangian mechanics with (generalized) momenta. Both theories provide interpretations of classical mechanics and describe the same physical phenomena. Hamiltonian mechanics has a close relationship with geometry (notably, symplectic geometry and Poisson structures) and serves as a link between classical and quantum mechanics.\nIn this formalism, the dynamics of a system are governed by Hamilton's equations, which express the time derivatives of position and momentum variables in terms of partial derivatives of a function called the Hamiltonian:\n\n  \n    \n      \n        \n          \n            \n              \n                d\n              \n              \n                q\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              \n                p\n              \n            \n          \n        \n        ,\n        \n        \n          \n            \n              \n                d\n              \n              \n                p\n              \n            \n            \n              \n                d\n              \n              t\n            \n          \n        \n        =\n        −\n        \n          \n            \n              ∂\n              \n                \n                  H\n                \n              \n            \n            \n              ∂\n              \n                q\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {\\mathrm {d} {\\boldsymbol {q}}}{\\mathrm {d} t}}={\\frac {\\partial {\\mathcal {H}}}{\\partial {\\boldsymbol {p}}}},\\quad {\\frac {\\mathrm {d} {\\boldsymbol {p}}}{\\mathrm {d} t}}=-{\\frac {\\partial {\\mathcal {H}}}{\\partial {\\boldsymbol {q}}}}.}\n  \n\nThe Hamiltonian is the Legendre transform of the Lagrangian, and in many situations of physical interest it is equal to the total energy of the system.\n\n\n== Limits of validity ==\n\nMany branches of classical mechanics are simplifications or approximations of more accurate forms; two of the most accurate being general relativity and relativistic statistical mechanics. Geometric optics is an approximation to the quantum theory of light, and does not have a superior \"classical\" form.\nWhen both quantum mechanics and classical mechanics cannot apply, such as at the quantum level with many degrees of freedom, quantum field theory (QFT) is of use. QFT deals with small distances, and large speeds with many degrees of freedom as well as the possibility of any change in the number of particles throughout the interaction. When treating large degrees of freedom at the macroscopic level, statistical mechanics becomes useful. Statistical mechanics describes the behavior of large (but countable) numbers of particles and their interactions as a whole at the macroscopic level. Statistical mechanics is mainly used in thermodynamics for systems that lie outside the bounds of the assumptions of classical thermodynamics. In the case of high velocity objects approaching the speed of light, classical mechanics is enhanced by special relativity. In case that objects become extremely heavy (i.e., their Schwarzschild radius is not negligibly small for a given application), deviations from Newtonian mechanics become apparent and can be quantified by using the parameterized post-Newtonian formalism. In that case, general relativity (GR) becomes applicable. However, until now there is no theory of quantum gravity unifying GR and QFT in the sense that it could be used when objects become extremely small and heavy.[4][5]\n\n\n=== Newtonian approximation to special relativity ===\nIn special relativity, the momentum of a particle is given by\n\n  \n    \n      \n        \n          p\n        \n        =\n        \n          \n            \n              m\n              \n                v\n              \n            \n            \n              1\n              −\n              \n                \n                  \n                    v\n                    \n                      2\n                    \n                  \n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle \\mathbf {p} ={\\frac {m\\mathbf {v} }{\\sqrt {1-{\\frac {v^{2}}{c^{2}}}}}}\\,,}\n  \n\nwhere m is the particle's rest mass, v its velocity, v is the modulus of v, and c is the speed of light.\nIf v is very small compared to c, v2/c2 is approximately zero, and so\n\n  \n    \n      \n        \n          p\n        \n        ≈\n        m\n        \n          v\n        \n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {p} \\approx m\\mathbf {v} \\,.}\n  \n\nThus the Newtonian equation p = mv is an approximation of the relativistic equation for bodies moving with low speeds compared to the speed of light.\nFor example, the relativistic cyclotron frequency of a cyclotron, gyrotron, or high voltage magnetron is given by\n\n  \n    \n      \n        f\n        =\n        \n          f\n          \n            \n              c\n            \n          \n        \n        \n          \n            \n              m\n              \n                0\n              \n            \n            \n              \n                m\n                \n                  0\n                \n              \n              +\n              \n                \n                  T\n                  \n                    c\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle f=f_{\\mathrm {c} }{\\frac {m_{0}}{m_{0}+{\\frac {T}{c^{2}}}}}\\,,}\n  \n\nwhere fc is the classical frequency of an electron (or other charged particle) with kinetic energy T and (rest) mass m0 circling in a magnetic field. The (rest) mass of an electron is 511 keV. So the frequency correction is 1% for a magnetic vacuum tube with a 5.11 kV direct current accelerating voltage.\n\n\n=== Classical approximation to quantum mechanics ===\nThe ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is\n\n  \n    \n      \n        λ\n        =\n        \n          \n            h\n            p\n          \n        \n      \n    \n    {\\displaystyle \\lambda ={\\frac {h}{p}}}\n  \n\nwhere h is the Planck constant and p is the momentum.\nAgain, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.\nMore practical examples of the failure of classical mechanics on an engineering scale are conduction by quantum tunneling in tunnel diodes and very narrow transistor gates in integrated circuits.\nClassical mechanics is the same extreme high frequency approximation as geometric optics. It is more often accurate because it describes particles and bodies with rest mass. These have more momentum and therefore shorter De Broglie wavelengths than massless particles, such as light, with the same kinetic energies.\n\n\n== History ==\n\nThe study of the motion of bodies is an ancient one, making classical mechanics one of the oldest and largest subjects in science, engineering, and technology. The development of classical mechanics lead to the development of many areas of mathematics.: 54  \nSome Greek philosophers of antiquity, among them Aristotle, founder of Aristotelian physics, may have been the first to maintain the idea that \"everything happens for a reason\" and that theoretical principles can assist in the understanding of nature. While to a modern reader, many of these preserved ideas come forth as eminently reasonable, there is a conspicuous lack of both mathematical theory and controlled experiment, as we know it. These later became decisive factors in forming modern science, and their early application came to be known as classical mechanics. In his Elementa super demonstrationem ponderum, medieval mathematician Jordanus de Nemore introduced the concept of \"positional gravity\" and the use of component forces.\n\nThe first published causal explanation of the motions of planets was Johannes Kepler's Astronomia nova, published in 1609. He concluded, based on Tycho Brahe's observations on the orbit of Mars, that the planet's orbits were ellipses. This break with ancient thought was happening around the same time that Galileo was proposing abstract mathematical laws for the motion of objects. He may (or may not) have performed the famous experiment of dropping two cannonballs of different weights from the tower of Pisa, showing that they both hit the ground at the same time. The reality of that particular experiment is disputed, but he did carry out quantitative experiments by rolling balls on an inclined plane. His theory of accelerated motion was derived from the results of such experiments and forms a cornerstone of classical mechanics. In 1673 Christiaan Huygens described in his Horologium Oscillatorium the first two laws of motion. The work is also the first modern treatise in which a physical problem (the accelerated motion of a falling body) is idealized by a set of parameters then analyzed mathematically and constitutes one of the seminal works of applied mathematics. \n\nNewton founded his principles of natural philosophy on three proposed laws of motion: the law of inertia, his second law of acceleration (mentioned above), and the law of action and reaction; and hence laid the foundations for classical mechanics. Both Newton's second and third laws were given the proper scientific and mathematical treatment in Newton's Philosophiæ Naturalis Principia Mathematica. Here they are distinguished from earlier attempts at explaining similar phenomena, which were either incomplete, incorrect, or given little accurate mathematical expression. Newton also enunciated the principles of conservation of momentum and angular momentum. In mechanics, Newton was also the first to provide the first correct scientific and mathematical formulation of gravity in Newton's law of universal gravitation. The combination of Newton's laws of motion and gravitation provides the fullest and most accurate description of classical mechanics. He demonstrated that these laws apply to everyday objects as well as to celestial objects. In particular, he obtained a theoretical explanation of Kepler's laws of motion of the planets.\nNewton had previously invented the calculus; however, the Principia was formulated entirely in terms of long-established geometric methods in emulation of Euclid. Newton, and most of his contemporaries, with the notable exception of Huygens, worked on the assumption that classical mechanics would be able to explain all phenomena, including light, in the form of geometric optics. Even when discovering the so-called Newton's rings (a wave interference phenomenon) he maintained his own corpuscular theory of light.\n\nAfter Newton, classical mechanics became a principal field of study in mathematics as well as physics. Mathematical formulations progressively allowed finding solutions to a far greater number of problems. The first notable mathematical treatment was in 1788 by Joseph Louis Lagrange. Lagrangian mechanics was in turn re-formulated in 1833 by William Rowan Hamilton.\n\nSome difficulties were discovered in the late 19th century that could only be resolved by more modern physics. Some of these difficulties related to compatibility with electromagnetic theory, and the famous Michelson–Morley experiment. The resolution of these problems led to the special theory of relativity, often still considered a part of classical mechanics.\nA second set of difficulties were related to thermodynamics. When combined with thermodynamics, classical mechanics leads to the Gibbs paradox of classical statistical mechanics, in which entropy is not a well-defined quantity. Black-body radiation was not explained without the introduction of quanta. As experiments reached the atomic level, classical mechanics failed to explain, even approximately, such basic things as the energy levels and sizes of atoms and the photo-electric effect. The effort at resolving these problems led to the development of quantum mechanics.\nSince the end of the 20th century, classical mechanics in physics has no longer been an independent theory. Instead, classical mechanics is now considered an approximate theory to the more general quantum mechanics. Emphasis has shifted to understanding the fundamental forces of nature as in the Standard Model and its more modern extensions into a unified theory of everything. Classical mechanics is a theory useful for the study of the motion of non-quantum mechanical, low-energy particles in weak gravitational fields.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nAlonso, M.; Finn, J. (1992). Fundamental University Physics. Addison-Wesley.\nFeynman, Richard (1999). The Feynman Lectures on Physics. Perseus Publishing. ISBN 978-0-7382-0092-7.\nFeynman, Richard; Phillips, Richard (1998). Six Easy Pieces. Perseus Publishing. ISBN 978-0-201-32841-7.\nGoldstein, Herbert; Charles P. Poole; John L. Safko (2002). Classical Mechanics (3rd ed.). Addison Wesley. ISBN 978-0-201-65702-9.\nKibble, Tom W.B.; Berkshire, Frank H. (2004). Classical Mechanics (5th ed.). Imperial College Press. ISBN 978-1-86094-424-6.\nKleppner, D.; Kolenkow, R.J. (1973). An Introduction to Mechanics. McGraw-Hill. ISBN 978-0-07-035048-9.\nLandau, L.D.; Lifshitz, E.M. (1972). Course of Theoretical Physics, Vol. 1 – Mechanics. Franklin Book Company. ISBN 978-0-08-016739-8.\nMorin, David (2008). Introduction to Classical Mechanics: With Problems and Solutions (1st ed.). Cambridge: Cambridge University Press. ISBN 978-0-521-87622-3.\nGerald Jay Sussman; Jack Wisdom (2001). Structure and Interpretation of Classical Mechanics. MIT Press. ISBN 978-0-262-19455-6.\nO'Donnell, Peter J. (2015). Essential Dynamics and Relativity. CRC Press. ISBN 978-1-4665-8839-4.\nThornton, Stephen T.; Marion, Jerry B. (2003). Classical Dynamics of Particles and Systems (5th ed.). Brooks Cole. ISBN 978-0-534-40896-1.\n\n\n== External links ==\n\nCrowell, Benjamin. Light and Matter (an introductory text, uses algebra with optional sections involving calculus)\nFitzpatrick, Richard. Classical Mechanics (uses calculus)\nHoiland, Paul (2004). Preferred Frames of Reference & Relativity\nHorbatsch, Marko, \"Classical Mechanics Course Notes\".\nRosu, Haret C., \"Classical Mechanics\". Physics Education. 1999. [arxiv.org : physics/9909035]\nShapiro, Joel A. (2003). Classical Mechanics\nSussman, Gerald Jay & Wisdom, Jack &  Mayer, Meinhard E. (2001). Structure and Interpretation of Classical Mechanics\nTong, David. Classical Dynamics (Cambridge lecture notes on Lagrangian and Hamiltonian formalism)\nKinematic Models for Design Digital Library (KMODDL) Movies and photos of hundreds of working mechanical-systems models at Cornell University. Also includes an e-book library of classic texts on mechanical design and engineering.\nMIT OpenCourseWare 8.01: Classical Mechanics Free videos of actual course lectures with links to lecture notes, assignments and exams.\nAlejandro A. Torassa, On Classical Mechanics"
    },
    {
        "id": "genetics",
        "name": "Genetics",
        "text": "Error fetching article: \"genetic\" may refer to: \nGenetics\ngenes\nGenetic disorder\nmutation\nHeredity\nGenetic recombination\nGenetic relationship (linguistics)\nGenetic algorithm\nGenetic memory (disambiguation)"
    },
    {
        "id": "molecular_biology",
        "name": "Molecular Biology",
        "text": "Molecular biology  is a branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including biomolecular synthesis, modification, mechanisms, and interactions.\nThough cells and other microscopic structures had been observed in living organisms as early as the 18th century, a detailed understanding of the mechanisms and interactions governing their behavior did not emerge until the 20th century, when technologies used in physics and chemistry had advanced sufficiently to permit their application in the biological sciences. The term 'molecular biology' was first used in 1945 by the English physicist William Astbury, who described it as an approach focused on discerning the underpinnings of biological phenomena—i.e. uncovering the physical and chemical structures and properties of biological molecules, as well as their interactions with other molecules and how these interactions explain observations of so-called classical biology, which instead studies biological processes at larger scales and higher levels of organization. In 1953, Francis Crick, James Watson, Rosalind Franklin, and their colleagues at the Medical Research Council Unit, Cavendish Laboratory, were the first to describe the double helix model for the chemical structure of deoxyribonucleic acid (DNA), which is often considered a landmark event for the nascent field because it provided a physico-chemical basis by which to understand the previously nebulous idea of nucleic acids as the primary substance of biological inheritance. They proposed this structure based on previous research done by Franklin, which was conveyed to them by Maurice Wilkins and Max Perutz. Their work led to the discovery of DNA in other microorganisms, plants, and animals.\nThe field of molecular biology includes techniques which enable scientists to learn about molecular processes. These techniques are used to efficiently target new drugs, diagnose disease, and better understand cell physiology. Some clinical research and medical therapies arising from molecular biology are covered under gene therapy, whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine.\n\n\n== History of molecular biology ==\n\nMolecular biology sits at the intersection of biochemistry and genetics; as these scientific disciplines emerged and evolved in the 20th century, it became clear that they both sought to determine the molecular mechanisms which underlie vital cellular functions. Advances in molecular biology have been closely related to the development of new technologies and their optimization. Molecular biology has been elucidated by the work of many scientists, and thus the history of the field depends on an understanding of these scientists and their experiments.\nThe field of genetics arose from attempts to understand the set of rules underlying reproduction and heredity, and the nature of the hypothetical units of heredity known as genes. Gregor Mendel pioneered this work in 1866, when he first described the laws of inheritance he observed in his studies of mating crosses in pea plants. One such law of genetic inheritance is the law of segregation, which states that diploid individuals with two alleles for a particular gene will pass one of these alleles to their offspring. Because of his critical work, the study of genetic inheritance is commonly referred to as Mendelian genetics.\nA major milestone in molecular biology was the discovery of the structure of DNA. This work began in 1869 by Friedrich Miescher, a Swiss biochemist who first proposed a structure called nuclein, which we now know to be (deoxyribonucleic acid), or DNA. He discovered this unique substance by studying the components of pus-filled bandages, and noting the unique properties of the \"phosphorus-containing substances\". Another notable contributor to the DNA model was Phoebus Levene, who proposed the \"polynucleotide model\" of DNA in 1919 as a result of his biochemical experiments on yeast. In 1950, Erwin Chargaff expanded on the work of Levene and elucidated a few critical properties of nucleic acids: first, the sequence of nucleic acids varies across species. Second, the total concentration of purines (adenine and guanine) is always equal to the total concentration of pyrimidines (cysteine and thymine). This is now known as Chargaff's rule. In 1953, James Watson and Francis Crick published the double helical structure of DNA, based on the X-ray crystallography work done by Rosalind Franklin which was conveyed to them by Maurice Wilkins and Max Perutz. Watson and Crick described the structure of DNA and conjectured about the implications of this unique structure for possible mechanisms of DNA replication. Watson and Crick were awarded the Nobel Prize in Physiology or Medicine in 1962, along with Wilkins, for proposing a model of the structure of DNA.\nIn 1961, it was demonstrated that when a gene encodes a protein, three sequential bases of a gene's DNA specify each successive amino acid of the protein. Thus the genetic code is a triplet code, where each triplet (called a codon) specifies a particular amino acid. Furthermore, it was shown that the codons do not overlap with each other in the DNA sequence encoding a protein, and that each sequence is read from a fixed starting point.\nDuring 1962–1964, through the use of conditional lethal mutants of a bacterial virus, fundamental advances were made in our understanding of the functions and interactions of the proteins employed in the machinery of DNA replication, DNA repair, DNA recombination, and in the assembly of molecular structures.\n\n\n== Griffith's experiment ==\n\nIn 1928, Frederick Griffith, encountered a virulence property in pneumococcus bacteria, which was killing lab rats. According to Mendel, prevalent at that time, gene transfer could occur only from parent to daughter cells. Griffith advanced another theory, stating that gene transfer occurring in member of same generation is known as horizontal gene transfer (HGT). This phenomenon is now referred to as genetic transformation.\nGriffith's experiment addressed the pneumococcus bacteria, which had two different strains, one virulent and smooth and one avirulent and rough. The smooth strain had glistering appearance owing to the presence of a type of specific polysaccharide – a polymer of glucose and glucuronic acid capsule. Due to this polysaccharide layer of bacteria, a host's immune system cannot recognize the bacteria and it kills the host. The other, avirulent, rough strain lacks this polysaccharide capsule and has a dull, rough appearance.\nPresence or absence of capsule in the strain, is known to be genetically determined. Smooth and rough strains occur in several different type such as S-I, S-II, S-III, etc. and R-I, R-II, R-III, etc. respectively. All this subtypes of S and R bacteria differ with each other in antigen type they produce.\n\n\n== Avery–MacLeod–McCarty experiment ==\n\nThe Avery–MacLeod–McCarty experiment was a landmark study conducted in 1944 that demonstrated that DNA, not protein as previously thought, carries genetic information in bacteria. Oswald Avery, Colin Munro MacLeod, and Maclyn McCarty used an extract from a strain of pneumococcus that could cause pneumonia in mice. They showed that genetic transformation in the bacteria could be accomplished by injecting them with purified DNA from the extract. They discovered that when they digested the DNA in the extract with DNase, transformation of harmless bacteria into virulent ones was lost. This provided strong evidence that DNA was the genetic material, challenging the prevailing belief that proteins were responsible. It laid the basis for the subsequent discovery of its structure by Watson and Crick.\n\n\n== Hershey–Chase experiment ==\n\nConfirmation that DNA is the genetic material which is cause of infection came from the Hershey–Chase experiment. They used E.coli and bacteriophage for the experiment. This experiment is also known as blender experiment, as kitchen blender was used as a major piece of apparatus. Alfred Hershey and Martha Chase demonstrated that the DNA injected by a phage particle into a bacterium contains all information required to synthesize progeny phage particles. They used radioactivity to tag the bacteriophage's protein coat with radioactive sulphur and DNA with radioactive phosphorus, into two different test tubes respectively. After mixing bacteriophage and E.coli into the test tube, the incubation period starts in which phage transforms the genetic material in the E.coli cells. Then the mixture is blended or agitated, which separates the phage from E.coli cells. The whole mixture is centrifuged and the pellet which contains E.coli cells was checked and the supernatant was discarded. The E.coli cells showed radioactive phosphorus, which indicated that the transformed material was DNA not the protein coat.\nThe transformed DNA gets attached to the DNA of E.coli and radioactivity is only seen onto the bacteriophage's DNA. This mutated DNA can be passed to the next generation and the theory of Transduction came into existence. Transduction is a process in which the bacterial DNA carry the fragment of bacteriophages and pass it on the next generation. This is also a type of horizontal gene transfer.\n\n\n== Meselson–Stahl experiment ==\n\nThe Meselson-Stahl experiment was a landmark experiment in molecular biology that provided evidence for the semiconservative replication of DNA. Conducted in 1958 by Matthew Meselson and Franklin Stahl, the experiment involved growing E. coli bacteria in a medium containing heavy isotope of nitrogen (15N) for several generations. This caused all the newly synthesized bacterial DNA to be incorporated with the heavy isotope.\nAfter allowing the bacteria to replicate in a medium containing normal nitrogen (14N), samples were taken at various time points. These samples were then subjected to centrifugation in a density gradient, which separated the DNA molecules based on their density.\nThe results showed that after one generation of replication in the 14N medium, the DNA formed a band of intermediate density between that of pure 15N DNA and pure 14N DNA. This supported the semiconservative DNA replication proposed by Watson and Crick, where each strand of the parental DNA molecule serves as a template for the synthesis of a new complementary strand, resulting in two daughter DNA molecules, each consisting of one parental and one newly synthesized strand.\nThe Meselson-Stahl experiment provided compelling evidence for the semiconservative replication of DNA, which is fundamental to the understanding of genetics and molecular biology.\n\n\n== Modern molecular biology ==\nIn the early 2020s, molecular biology entered a golden age defined by both vertical and horizontal technical development. Vertically, novel technologies are allowing for real-time monitoring of biological processes at the atomic level. Molecular biologists today have access to increasingly affordable sequencing data at increasingly higher depths, facilitating the development of novel genetic manipulation methods in new non-model organisms. Likewise, synthetic molecular biologists will drive the industrial production of small and macro molecules through the introduction of exogenous metabolic pathways in various prokaryotic and eukaryotic cell lines.\nHorizontally, sequencing data is becoming more affordable and used in many different scientific fields. This will drive the development of industries in developing nations and increase accessibility to individual researchers. Likewise, CRISPR-Cas9 gene editing experiments can now be conceived and implemented by individuals for under $10,000 in novel organisms, which will drive the development of industrial and medical applications.\n\n\n== Relationship to other biological sciences ==\n\nThe following list describes a viewpoint on the interdisciplinary relationships between molecular biology and other related fields.\n\nMolecular biology is the study of the molecular underpinnings of the biological phenomena, focusing on molecular synthesis, modification, mechanisms and interactions.\nBiochemistry is the study of the chemical substances and vital processes occurring in living organisms. Biochemists focus heavily on the role, function, and structure of biomolecules such as proteins, lipids, carbohydrates and nucleic acids.\nGenetics is the study of how genetic differences affect organisms. Genetics attempts to predict how mutations, individual genes and genetic interactions can affect the expression of a phenotype\nWhile researchers practice techniques specific to molecular biology, it is common to combine these with methods from genetics and biochemistry. Much of molecular biology is quantitative, and recently a significant amount of work has been done using computer science techniques such as bioinformatics and computational biology. Molecular genetics, the study of gene structure and function, has been among the most prominent sub-fields of molecular biology since the early 2000s. Other branches of biology are informed by molecular biology, by either directly studying the interactions of molecules in their own right such as in cell biology and developmental biology, or indirectly, where molecular techniques are used to infer historical attributes of populations or species, as in fields in evolutionary biology such as population genetics and phylogenetics. There is also a long tradition of studying biomolecules \"from the ground up\", or molecularly, in biophysics.\n\n\n== Techniques of molecular biology ==\n\n\n=== Molecular cloning ===\n\nMolecular cloning is used to isolate and then transfer a DNA sequence of interest into a plasmid vector. This recombinant DNA technology was first developed in the 1960s. In this technique, a DNA sequence coding for a protein of interest is cloned using polymerase chain reaction (PCR), and/or restriction enzymes, into a plasmid (expression vector). The plasmid vector usually has at least 3 distinctive features: an origin of replication, a multiple cloning site (MCS), and a selective marker (usually antibiotic resistance). Additionally, upstream of the MCS are the promoter regions and the transcription start site, which regulate the expression of cloned gene.\nThis plasmid can be inserted into either bacterial or animal cells. Introducing DNA into bacterial cells can be done by transformation via uptake of naked DNA, conjugation via cell-cell contact or by transduction via viral vector. Introducing DNA into eukaryotic cells, such as animal cells, by physical or chemical means is called transfection. Several different transfection techniques are available, such as calcium phosphate transfection, electroporation, microinjection and liposome transfection. The plasmid may be integrated into the genome, resulting in a stable transfection, or may remain independent of the genome and expressed temporarily, called a transient transfection.\nDNA coding for a protein of interest is now inside a cell, and the protein can now be expressed. A variety of systems, such as inducible promoters and specific cell-signaling factors, are available to help express the protein of interest at high levels. Large quantities of a protein can then be extracted from the bacterial or eukaryotic cell. The protein can be tested for enzymatic activity under a variety of situations, the protein may be crystallized so its tertiary structure can be studied, or, in the pharmaceutical industry, the activity of new drugs against the protein can be studied.\n\n\n=== Polymerase chain reaction ===\n\nPolymerase chain reaction (PCR) is an extremely versatile technique for copying DNA. In brief, PCR allows a specific DNA sequence to be copied or modified in predetermined ways. The reaction is extremely powerful and under perfect conditions could amplify one DNA molecule to become 1.07 billion molecules in less than two hours. PCR has many applications, including the study of gene expression, the detection of pathogenic microorganisms, the detection of genetic mutations, and the introduction of mutations to DNA. The PCR technique can be used to introduce restriction enzyme sites to ends of DNA molecules, or to mutate particular bases of DNA, the latter is a method referred to as site-directed mutagenesis. PCR can also be used to determine whether a particular DNA fragment is found in a cDNA library. PCR has many variations, like reverse transcription PCR (RT-PCR) for amplification of RNA, and, more recently, quantitative PCR which allow for quantitative measurement of DNA or RNA molecules.\n\n\n=== Gel electrophoresis ===\n\nGel electrophoresis is a technique which separates molecules by their size using an agarose or polyacrylamide gel. This technique is one of the principal tools of molecular biology. The basic principle is that DNA fragments can be separated by applying an electric current across the gel - because the DNA backbone contains negatively charged phosphate groups, the DNA will migrate through the agarose gel towards the positive end of the current. Proteins can also be separated on the basis of size using an SDS-PAGE gel, or on the basis of size and their electric charge by using what is known as a 2D gel electrophoresis.\n\n\n=== The Bradford protein assay ===\n\nThe Bradford assay is a molecular biology technique which enables the fast, accurate quantitation of protein molecules utilizing the unique properties of a dye called Coomassie Brilliant Blue G-250. Coomassie Blue undergoes a visible color shift from reddish-brown to bright blue upon binding to protein. In its unstable, cationic state, Coomassie Blue has a background wavelength of 465 nm and gives off a reddish-brown color. When Coomassie Blue binds to protein in an acidic solution, the background wavelength shifts to 595 nm and the dye gives off a bright blue color. Proteins in the assay bind Coomassie blue in about 2 minutes, and the protein-dye complex is stable for about an hour, although it is recommended that absorbance readings are taken within 5 to 20 minutes of reaction initiation. The concentration of protein in the Bradford assay can then be measured using a visible light spectrophotometer, and therefore does not require extensive equipment.\nThis method was developed in 1975 by Marion M. Bradford, and has enabled significantly faster, more accurate protein quantitation compared to previous methods: the Lowry procedure and the biuret assay. Unlike the previous methods, the Bradford assay is not susceptible to interference by several non-protein molecules, including ethanol, sodium chloride, and magnesium chloride. However, it is susceptible to influence by strong alkaline buffering agents, such as sodium dodecyl sulfate (SDS).\n\n\n=== Macromolecule blotting and probing ===\nThe terms northern, western and eastern blotting are derived from what initially was a molecular biology joke that played on the term Southern blotting, after the technique described by Edwin Southern for the hybridisation of blotted DNA. Patricia Thomas, developer of the RNA blot which then became known as the northern blot, actually did not use the term.\n\n\n==== Southern blotting ====\n\nNamed after its inventor, biologist Edwin Southern, the Southern blot is a method for probing for the presence of a specific DNA sequence within a DNA sample. DNA samples before or after restriction enzyme (restriction endonuclease) digestion are separated by gel electrophoresis and then transferred to a membrane by blotting via capillary action. The membrane is then exposed to a labeled DNA probe that has a complement base sequence to the sequence on the DNA of interest. Southern blotting is less commonly used in laboratory science due to the capacity of other techniques, such as PCR, to detect specific DNA sequences from DNA samples. These blots are still used for some applications, however, such as measuring transgene copy number in transgenic mice or in the engineering of gene knockout embryonic stem cell lines.\n\n\n==== Northern blotting ====\n\nThe northern blot is used to study the presence of specific RNA molecules as relative comparison among a set of different samples of RNA. It is essentially a combination of denaturing RNA gel electrophoresis, and a blot. In this process RNA is separated based on size and is then transferred to a membrane that is then probed with a labeled complement of a sequence of interest. The results may be visualized through a variety of ways depending on the label used; however, most result in the revelation of bands representing the sizes of the RNA detected in sample. The intensity of these bands is related to the amount of the target RNA in the samples analyzed. The procedure is commonly used to study when and how much gene expression is occurring by measuring how much of that RNA is present in different samples, assuming that no post-transcriptional regulation occurs and that the levels of mRNA reflect proportional levels of the corresponding protein being produced. It is one of the most basic tools for determining at what time, and under what conditions, certain genes are expressed in living tissues.\n\n\n==== Western blotting ====\n\nA western blot is a technique by which specific proteins can be detected from a mixture of proteins. Western blots can be used to determine the size of isolated proteins, as well as to quantify their expression. In western blotting, proteins are first separated by size, in a thin gel sandwiched between two glass plates in a technique known as SDS-PAGE. The proteins in the gel are then transferred to a polyvinylidene fluoride (PVDF), nitrocellulose, nylon, or other support membrane. This membrane can then be probed with solutions of antibodies. Antibodies that specifically bind to the protein of interest can then be visualized by a variety of techniques, including colored products, chemiluminescence, or autoradiography. Often, the antibodies are labeled with enzymes. When a chemiluminescent substrate is exposed to the enzyme it allows detection. Using western blotting techniques allows not only detection but also quantitative analysis. Analogous methods to western blotting can be used to directly stain specific proteins in live cells or tissue sections.\n\n\n==== Eastern blotting ====\n\nThe eastern blotting technique is used to detect post-translational modification of proteins. Proteins blotted on to the PVDF or nitrocellulose membrane are probed for modifications using specific substrates.\n\n\n=== Microarrays ===\n\nA DNA microarray is a collection of spots attached to a solid support such as a microscope slide where each spot contains one or more single-stranded DNA oligonucleotide fragments. Arrays make it possible to put down large quantities of very small (100 micrometre diameter) spots on a single slide. Each spot has a DNA fragment molecule that is complementary to a single DNA sequence. A variation of this technique allows the gene expression of an organism at a particular stage in development to be qualified (expression profiling). In this technique the RNA in a tissue is isolated and converted to labeled complementary DNA (cDNA). This cDNA is then hybridized to the fragments on the array and visualization of the hybridization can be done. Since multiple arrays can be made with exactly the same position of fragments, they are particularly useful for comparing the gene expression of two different tissues, such as a healthy and cancerous tissue. Also, one can measure what genes are expressed and how that expression changes with time or with other factors.\nThere are many different ways to fabricate microarrays; the most common are silicon chips, microscope slides with spots of ~100 micrometre diameter, custom arrays, and arrays with larger spots on porous membranes (macroarrays). There can be anywhere from 100 spots to more than 10,000 on a given array. Arrays can also be made with molecules other than DNA.\n\n\n=== Allele-specific oligonucleotide ===\n\nAllele-specific oligonucleotide (ASO) is a technique that allows detection of single base mutations without the need for PCR or gel electrophoresis. Short (20–25 nucleotides in length), labeled probes are exposed to the non-fragmented target DNA, hybridization occurs with high specificity due to the short length of the probes and even a single base change will hinder hybridization. The target DNA is then washed and the unhybridized probes are removed. The target DNA is then analyzed for the presence of the probe via radioactivity or fluorescence. In this experiment, as in most molecular biology techniques, a control must be used to ensure successful experimentation.\nIn molecular biology, procedures and technologies are continually being developed and older technologies abandoned. For example, before the advent of DNA gel electrophoresis (agarose or polyacrylamide), the size of DNA molecules was typically determined by rate sedimentation in sucrose gradients, a slow and labor-intensive technique requiring expensive instrumentation; prior to sucrose gradients, viscometry was used. Aside from their historical interest, it is often worth knowing about older technology, as it is occasionally useful to solve another new problem for which the newer technique is inappropriate.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\n\n Media related to Molecular biology at Wikimedia Commons"
    },
    {
        "id": "world_war_1",
        "name": "World War I",
        "text": "World War II or the Second World War (1 September 1939 – 2 September 1945) was a global conflict between two coalitions: the Allies and the Axis powers. Nearly all of the world's countries participated, with many nations mobilising all resources in pursuit of total war. Tanks and aircraft played major roles, enabling the strategic bombing of cities and delivery of the first and only nuclear weapons ever used in war. World War II was the deadliest conflict in history, resulting in 70 to 85 million deaths, more than half of which were civilians. Millions died in genocides, including the Holocaust, and by massacres, starvation, and disease. After the Allied victory, Germany, Austria, Japan, and Korea were occupied, and German and Japanese leaders were tried for war crimes.\nThe causes of World War II included unresolved tensions in the aftermath of World War I and the rises of fascism in Europe and militarism in Japan. Key events preceding the war included Japan's invasion of Manchuria in 1931, the Spanish Civil War, the outbreak of the Second Sino-Japanese War in 1937, and Germany's annexations of Austria and the Sudetenland. World War II is generally considered to have begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland, after which the United Kingdom and France declared war on Germany. Poland was divided between Germany and the Soviet Union under the Molotov–Ribbentrop Pact. In 1940, the Soviets annexed the Baltic states and parts of Finland and Romania. After the fall of France in June 1940, the war continued mainly between Germany and the British Empire, with fighting in the Balkans, Mediterranean, and Middle East, the aerial Battle of Britain and the Blitz, and naval Battle of the Atlantic. Through campaigns and treaties, Germany gained control of much of continental Europe and formed the Axis alliance with Italy, Japan, and other countries. In June 1941, Germany led an invasion of the Soviet Union, opening the Eastern Front and initially making large territorial gains.\nIn December 1941, Japan attacked American and British territories in Asia and the Pacific, including at Pearl Harbor in Hawaii, leading the United States to enter the war against Japan and Germany. Japan conquered much of coastal China and Southeast Asia, but its advances in the Pacific were halted in June 1942 at the Battle of Midway. In late 1942, Axis forces were defeated in North Africa and at Stalingrad in the Soviet Union, and in 1943 their continued defeats on the Eastern Front, an Allied invasion of Italy, and Allied offensives in the Pacific forced them into retreat on all fronts. In 1944, the Western Allies invaded France at Normandy as the Soviet Union recaptured its pre-war territory and the U.S. crippled Japan's navy and captured key Pacific islands. The war in Europe concluded with the liberation of German-occupied territories; invasions of Germany by the Western Allies and the Soviet Union, which culminated in the fall of Berlin to Soviet troops; and Germany's unconditional surrender on 8 May 1945. On 6 and 9 August, the U.S. dropped atomic bombs on Hiroshima and Nagasaki in Japan. Faced with an imminent Allied invasion, the prospect of further atomic bombings, and a Soviet declaration of war and invasion of Manchuria, Japan announced its unconditional surrender on 15 August, and signed a surrender document on 2 September 1945.\nWorld War II transformed the political, economic, and social structures of the world, and established the foundation of international relations for the rest of the 20th century and into the 21st century. The United Nations was created to foster international cooperation and prevent future conflicts, with the victorious great powers—China, France, the Soviet Union, the UK, and the U.S.—becoming the permanent members of its security council. The Soviet Union and U.S. emerged as rival global superpowers, setting the stage for the half-century Cold War. In the wake of Europe's devastation, the influence of its great powers waned, triggering the decolonisation of Africa and Asia. Many countries whose industries had been damaged moved towards economic recovery and expansion.\n\n\n== Start and end dates ==\n\nWorld War II began in Europe on 1 September 1939 with the German invasion of Poland and the United Kingdom and France's declaration of war on Germany two days later on 3 September 1939. Dates for the beginning of the Pacific War include the start of the Second Sino-Japanese War on 7 July 1937, or the earlier Japanese invasion of Manchuria, on 19 September 1931. Others follow the British historian A. J. P. Taylor, who stated that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars became World War II in 1941. Other proposed starting dates for World War II include the Italian invasion of Abyssinia on 3 October 1935. The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939. Others view the Spanish Civil War as the start or prelude to World War II.\nThe exact date of the war's end also is not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 15 August 1945 (V-J Day), rather than with the formal surrender of Japan on 2 September 1945, which officially ended the war in Asia. A peace treaty between Japan and the Allies was signed in 1951. A 1990 treaty regarding Germany's future allowed the reunification of East and West Germany to take place and resolved most post–World War II issues. No formal peace treaty between Japan and the Soviet Union was ever signed, although the state of war between the two countries was terminated by the Soviet–Japanese Joint Declaration of 1956, which also restored full diplomatic relations between them.\n\n\n== Background ==\n\n\n=== Aftermath of World War I ===\n\nWorld War I had radically altered the political European map with the defeat of the Central Powers—including Austria-Hungary, Germany, Bulgaria, and the Ottoman Empire—and the 1917 Bolshevik seizure of power in Russia, which led to the founding of the Soviet Union. Meanwhile, the victorious Allies of World War I, such as France, Belgium, Italy, Romania, and Greece, gained territory, and new nation-states were created out of the dissolution of the Austro-Hungarian, Ottoman, and Russian Empires.\nTo prevent a future world war, the League of Nations was established in 1920 by the Paris Peace Conference. The organisation's primary goals were to prevent armed conflict through collective security, military, and naval disarmament, as well as settling international disputes through peaceful negotiations and arbitration.\nDespite strong pacifist sentiment after World War I, irredentist and revanchist nationalism had emerged in several European states. These sentiments were especially marked in Germany because of the significant territorial, colonial, and financial losses imposed by the Treaty of Versailles. Under the treaty, Germany lost around 13 percent of its home territory and all its overseas possessions, while German annexation of other states was prohibited, reparations were imposed, and limits were placed on the size and capability of the country's armed forces.\n\n\n=== Germany and Italy ===\nThe German Empire was dissolved in the German revolution of 1918–1919, and a democratic government, later known as the Weimar Republic, was created. The interwar period saw strife between supporters of the new republic and hardline opponents on both the political right and left. Italy, as an Entente ally, had made some post-war territorial gains; however, Italian nationalists were angered that the promises made by the United Kingdom and France to secure Italian entrance into the war were not fulfilled in the peace settlement. From 1922 to 1925, the Fascist movement led by Benito Mussolini seized power in Italy with a nationalist, totalitarian, and class collaborationist agenda that abolished representative democracy, repressed socialist, left-wing, and liberal forces, and pursued an aggressive expansionist foreign policy aimed at making Italy a world power, promising the creation of a \"New Roman Empire\".\n\nAdolf Hitler, after an unsuccessful attempt to overthrow the German government in 1923, eventually became the chancellor of Germany in 1933 when President Paul von Hindenburg and the Reichstag appointed him. Following Hindenburg's death in 1934, Hitler proclaimed himself Führer of Germany and abolished democracy, espousing a radical, racially motivated revision of the world order, and soon began a massive rearmament campaign. France, seeking to secure its alliance with Italy, allowed Italy a free hand in Ethiopia, which Italy desired as a colonial possession. The situation was aggravated in early 1935 when the Territory of the Saar Basin was legally reunited with Germany, and Hitler repudiated the Treaty of Versailles, accelerated his rearmament programme, and introduced conscription.\n\n\n=== European treaties ===\nThe United Kingdom, France and Italy formed the Stresa Front in April 1935 in order to contain Germany, a key step towards military globalisation; however, that June, the United Kingdom made an independent naval agreement with Germany, easing prior restrictions. The Soviet Union, concerned by Germany's goals of capturing vast areas of Eastern Europe, drafted a treaty of mutual assistance with France. Before taking effect, though, the Franco-Soviet pact was required to go through the bureaucracy of the League of Nations, which rendered it essentially toothless. The United States, concerned with events in Europe and Asia, passed the Neutrality Act in August of the same year.\nHitler defied the Versailles and Locarno Treaties by remilitarising the Rhineland in March 1936, encountering little opposition due to the policy of appeasement. In October 1936, Germany and Italy formed the Rome–Berlin Axis. A month later, Germany and Japan signed the Anti-Comintern Pact, which Italy joined the following year.\n\n\n=== Asia ===\nThe Kuomintang party in China launched a unification campaign against regional warlords and nominally unified China in the mid-1920s, but was soon embroiled in a civil war against its former Chinese Communist Party (CCP) allies and new regional warlords. In 1931, an increasingly militaristic Empire of Japan, which had long sought influence in China as the first step of what its government saw as the country's right to rule Asia, staged the Mukden incident as a pretext to invade Manchuria and establish the puppet state of Manchukuo.\nChina appealed to the League of Nations to stop the Japanese invasion of Manchuria. Japan withdrew from the League of Nations after being condemned for its incursion into Manchuria. The two nations then fought several battles, in Shanghai, Rehe and Hebei, until the Tanggu Truce was signed in 1933. Thereafter, Chinese volunteer forces continued the resistance to Japanese aggression in Manchuria, and Chahar and Suiyuan. After the 1936 Xi'an Incident, the Kuomintang and CCP forces agreed on a ceasefire to present a united front to oppose Japan.\n\n\n== Pre-war events ==\n\n\n=== Italian invasion of Ethiopia (1935) ===\n\nThe Second Italo-Ethiopian War was a brief colonial war that began in October 1935 and ended in May 1936. The war began with the invasion of the Ethiopian Empire (also known as Abyssinia) by the armed forces of the Kingdom of Italy (Regno d'Italia), which was launched from Italian Somaliland and Eritrea. The war resulted in the military occupation of Ethiopia and its annexation into the newly created colony of Italian East Africa (Africa Orientale Italiana); in addition it exposed the weakness of the League of Nations as a force to preserve peace. Both Italy and Ethiopia were member nations, but the League did little when the former clearly violated Article X of the League's Covenant. The United Kingdom and France supported imposing sanctions on Italy for the invasion, but the sanctions were not fully enforced and failed to end the Italian invasion. Italy subsequently dropped its objections to Germany's goal of absorbing Austria.\n\n\n=== Spanish Civil War (1936–1939) ===\n\nWhen civil war broke out in Spain, Hitler and Mussolini lent military support to the Nationalist rebels, led by General Francisco Franco. Italy supported the Nationalists to a greater extent than the Nazis: Mussolini sent more than 70,000 ground troops, 6,000 aviation personnel, and 720 aircraft to Spain. The Soviet Union supported the existing government of the Spanish Republic. More than 30,000 foreign volunteers, known as the International Brigades, also fought against the Nationalists. Both Germany and the Soviet Union used this proxy war as an opportunity to test in combat their most advanced weapons and tactics. The Nationalists won the civil war in April 1939; Franco, now dictator, remained officially neutral during World War II but generally favoured the Axis. His greatest collaboration with Germany was the sending of volunteers to fight on the Eastern Front.\n\n\n=== Japanese invasion of China (1937) ===\n\nIn July 1937, Japan captured the former Chinese imperial capital of Peking after instigating the Marco Polo Bridge incident, which culminated in the Japanese campaign to invade all of China. The Soviets quickly signed a non-aggression pact with China to lend materiel support, effectively ending China's prior cooperation with Germany. From September to November, the Japanese attacked Taiyuan, engaged the Kuomintang Army around Xinkou, and fought Communist forces in Pingxingguan. Generalissimo Chiang Kai-shek deployed his best army to defend Shanghai, but after three months of fighting, Shanghai fell. The Japanese continued to push Chinese forces back, capturing the capital Nanking in December 1937. After the fall of Nanking, tens or hundreds of thousands of Chinese civilians and disarmed combatants were murdered by the Japanese.\nIn March 1938, Nationalist Chinese forces won their first major victory at Taierzhuang, but then the city of Xuzhou was taken by the Japanese in May. In June 1938, Chinese forces stalled the Japanese advance by flooding the Yellow River; this manoeuvre bought time for the Chinese to prepare their defences at Wuhan, but the city was taken by October. Japanese military victories did not bring about the collapse of Chinese resistance that Japan had hoped to achieve; instead, the Chinese government relocated inland to Chongqing and continued the war.\n\n\n=== Soviet–Japanese border conflicts ===\n\nIn the mid-to-late 1930s, Japanese forces in Manchukuo had sporadic border clashes with the Soviet Union and Mongolia. The Japanese doctrine of Hokushin-ron, which emphasised Japan's expansion northward, was favoured by the Imperial Army during this time. This policy would prove difficult to maintain in light of the Japanese defeat at Khalkin Gol in 1939, the ongoing Second Sino-Japanese War and ally Nazi Germany pursuing neutrality with the Soviets. Japan and the Soviet Union eventually signed a Neutrality Pact in April 1941, and Japan adopted the doctrine of Nanshin-ron, promoted by the Navy, which took its focus southward and eventually led to war with the United States and the Western Allies.\n\n\n=== European occupations and agreements ===\n\nIn Europe, Germany and Italy were becoming more aggressive. In March 1938, Germany annexed Austria, again provoking little response from other European powers. Encouraged, Hitler began pressing German claims on the Sudetenland, an area of Czechoslovakia with a predominantly ethnic German population. Soon the United Kingdom and France followed the appeasement policy of British Prime Minister Neville Chamberlain and conceded this territory to Germany in the Munich Agreement, which was made against the wishes of the Czechoslovak government, in exchange for a promise of no further territorial demands. Soon afterwards, Germany and Italy forced Czechoslovakia to cede additional territory to Hungary, and Poland annexed the Trans-Olza region of Czechoslovakia.\nAlthough all of Germany's stated demands had been satisfied by the agreement, privately Hitler was furious that British interference had prevented him from seizing all of Czechoslovakia in one operation. In subsequent speeches Hitler attacked British and Jewish \"war-mongers\" and in January 1939 secretly ordered a major build-up of the German navy to challenge British naval supremacy. In March 1939, Germany invaded the remainder of Czechoslovakia and subsequently split it into the German Protectorate of Bohemia and Moravia and a pro-German client state, the Slovak Republic. Hitler also delivered an ultimatum to Lithuania on 20 March 1939, forcing the concession of the Klaipėda Region, formerly the German Memelland.\n\nGreatly alarmed and with Hitler making further demands on the Free City of Danzig, the United Kingdom and France guaranteed their support for Polish independence; when Italy conquered Albania in April 1939, the same guarantee was extended to the Kingdoms of Romania and Greece. Shortly after the Franco-British pledge to Poland, Germany and Italy formalised their own alliance with the Pact of Steel. Hitler accused the United Kingdom and Poland of trying to \"encircle\" Germany and renounced the Anglo-German Naval Agreement and the German–Polish declaration of non-aggression.\nThe situation became a crisis in late August as German troops continued to mobilise against the Polish border. On 23 August the Soviet Union signed a non-aggression pact with Germany, after tripartite negotiations for a military alliance between France, the United Kingdom, and Soviet Union had stalled. This pact had a secret protocol that defined German and Soviet \"spheres of influence\" (western Poland and Lithuania for Germany; eastern Poland, Finland, Estonia, Latvia and Bessarabia for the Soviet Union), and raised the question of continuing Polish independence. The pact neutralised the possibility of Soviet opposition to a campaign against Poland and assured that Germany would not have to face the prospect of a two-front war, as it had in World War I. Immediately afterwards, Hitler ordered the attack to proceed on 26 August, but upon hearing that the United Kingdom had concluded a formal mutual assistance pact with Poland and that Italy would maintain neutrality, he decided to delay it.\nIn response to British requests for direct negotiations to avoid war, Germany made demands on Poland, which served as a pretext to worsen relations. On 29 August, Hitler demanded that a Polish plenipotentiary immediately travel to Berlin to negotiate the handover of Danzig, and to allow a plebiscite in the Polish Corridor in which the German minority would vote on secession. The Poles refused to comply with the German demands, and on the night of 30–31 August in a confrontational meeting with the British ambassador Nevile Henderson, Ribbentrop declared that Germany considered its claims rejected.\n\n\n== Course of the war ==\n\n\n=== War breaks out in Europe (1939–1940) ===\n\nOn 1 September 1939, Germany invaded Poland after having staged several false flag border incidents as a pretext to initiate the invasion. The first German attack of the war came against the Polish defences at Westerplatte. The United Kingdom responded with an ultimatum for Germany to cease military operations, and on 3 September, after the ultimatum was ignored, Britain and France declared war on Germany. During the Phoney War period, the alliance provided no direct military support to Poland, outside of a cautious French probe into the Saarland. The Western Allies also began a naval blockade of Germany, which aimed to damage the country's economy and war effort. Germany responded by ordering U-boat warfare against Allied merchant and warships, which would later escalate into the Battle of the Atlantic.\nOn 8 September, German troops reached the suburbs of Warsaw. The Polish counter-offensive to the west halted the German advance for several days, but it was outflanked and encircled by the Wehrmacht. Remnants of the Polish army broke through to besieged Warsaw. On 17 September 1939, two days after signing a cease-fire with Japan, the Soviet Union invaded Poland under the supposed pretext that the Polish state had ceased to exist. On 27 September, the Warsaw garrison surrendered to the Germans, and the last large operational unit of the Polish Army surrendered on 6 October. Despite the military defeat, Poland never surrendered; instead, it formed the Polish government-in-exile and a clandestine state apparatus remained in occupied Poland. A significant part of Polish military personnel evacuated to Romania and Latvia; many of them later fought against the Axis in other theatres of the war.\nGermany annexed western Poland and occupied central Poland; the Soviet Union annexed eastern Poland; small shares of Polish territory were transferred to Lithuania and Slovakia. On 6 October, Hitler made a public peace overture to the United Kingdom and France but said that the future of Poland was to be determined exclusively by Germany and the Soviet Union. The proposal was rejected and Hitler ordered an immediate offensive against France, which was postponed until the spring of 1940 due to bad weather.\n\nAfter the outbreak of war in Poland, Stalin threatened Estonia, Latvia, and Lithuania with military invasion, forcing the three Baltic countries to sign pacts allowing the creation of Soviet military bases in these countries; in October 1939, significant Soviet military contingents were moved there. Finland refused to sign a similar pact and rejected ceding part of its territory to the Soviet Union. The Soviet Union invaded Finland in November 1939, and was subsequently expelled from the League of Nations for this crime of aggression. Despite overwhelming numerical superiority, Soviet military success during the Winter War was modest, and the Finno-Soviet war ended in March 1940 with some Finnish concessions of territory.\nIn June 1940, the Soviet Union occupied the entire territories of Estonia, Latvia and Lithuania, as well as the Romanian regions of Bessarabia, Northern Bukovina, and the Hertsa region. In August 1940, Hitler imposed the Second Vienna Award on Romania which led to the transfer of Northern Transylvania to Hungary. In September 1940, Bulgaria demanded Southern Dobruja from Romania with German and Italian support, leading to the Treaty of Craiova. The loss of one-third of Romania's 1939 territory caused a coup against King Carol II, turning Romania into a fascist dictatorship under Marshal Ion Antonescu, with a course set towards the Axis in the hopes of a German guarantee. Meanwhile, German-Soviet political relations and economic co-operation gradually stalled, and both states began preparations for war.\n\n\n=== Western Europe (1940–1941) ===\n\nIn April 1940, Germany invaded Denmark and Norway to protect shipments of iron ore from Sweden, which the Allies were attempting to cut off. Denmark capitulated after six hours, and despite Allied support, Norway was conquered within two months. British discontent over the Norwegian campaign led to the resignation of Prime Minister Neville Chamberlain, who was replaced by Winston Churchill on 10 May 1940.\nOn the same day, Germany launched an offensive against France. To circumvent the strong Maginot Line fortifications on the Franco-German border, Germany directed its attack at the neutral nations of Belgium, the Netherlands, and Luxembourg. The Germans carried out a flanking manoeuvre through the Ardennes region, which was mistakenly perceived by the Allies as an impenetrable natural barrier against armoured vehicles. By successfully implementing new Blitzkrieg tactics, the Wehrmacht rapidly advanced to the Channel and cut off the Allied forces in Belgium, trapping the bulk of the Allied armies in a cauldron on the Franco-Belgian border near Lille. The United Kingdom was able to evacuate a significant number of Allied troops from the continent by early June, although they had to abandon almost all their equipment.\nOn 10 June, Italy invaded France, declaring war on both France and the United Kingdom. The Germans turned south against the weakened French army, and Paris fell to them on 14 June. Eight days later France signed an armistice with Germany; it was divided into German and Italian occupation zones, and an unoccupied rump state under the Vichy Regime, which, though officially neutral, was generally aligned with Germany. France kept its fleet, which the United Kingdom attacked on 3 July in an attempt to prevent its seizure by Germany.\nThe air Battle of Britain began in early July with Luftwaffe attacks on shipping and harbours. The German campaign for air superiority started in August but its failure to defeat RAF Fighter Command forced the indefinite postponement of the proposed German invasion of Britain. The German strategic bombing offensive intensified with night attacks on London and other cities in the Blitz, but largely ended in May 1941 after failing to significantly disrupt the British war effort.\nUsing newly captured French ports, the German Navy enjoyed success against an over-extended Royal Navy, using U-boats against British shipping in the Atlantic. The British Home Fleet scored a significant victory on 27 May 1941 by sinking the German battleship Bismarck.\nIn November 1939, the United States was assisting China and the Western Allies, and had amended the Neutrality Act to allow \"cash and carry\" purchases by the Allies. In 1940, following the German capture of Paris, the size of the United States Navy was significantly increased. In September the United States further agreed to a trade of American destroyers for British bases. Still, a large majority of the American public continued to oppose any direct military intervention in the conflict well into 1941. In December 1940, Roosevelt accused Hitler of planning world conquest and ruled out any negotiations as useless, calling for the United States to become an \"arsenal of democracy\" and promoting Lend-Lease programmes of military and humanitarian aid to support the British war effort; Lend-Lease was later extended to the other Allies, including the Soviet Union after it was invaded by Germany. The United States started strategic planning to prepare for a full-scale offensive against Germany.\nAt the end of September 1940, the Tripartite Pact formally united Japan, Italy, and Germany as the Axis powers. The Tripartite Pact stipulated that any country—with the exception of the Soviet Union—that attacked any Axis Power would be forced to go to war against all three. The Axis expanded in November 1940 when Hungary, Slovakia, and Romania joined. Romania and Hungary later made major contributions to the Axis war against the Soviet Union, in Romania's case partially to recapture territory ceded to the Soviet Union.\n\n\n=== Mediterranean (1940–1941) ===\n\nIn early June 1940, the Italian Regia Aeronautica attacked and besieged Malta, a British possession. From late summer to early autumn, Italy conquered British Somaliland and made an incursion into British-held Egypt. In October, Italy attacked Greece, but the attack was repulsed with heavy Italian casualties; the campaign ended within months with minor territorial changes. To assist Italy and prevent Britain from gaining a foothold, Germany prepared to invade the Balkans, which would threaten Romanian oil fields and strike against British dominance of the Mediterranean.\n\nIn December 1940, British Empire forces began counter-offensives against Italian forces in Egypt and Italian East Africa. The offensives were successful; by early February 1941, Italy had lost control of eastern Libya, and large numbers of Italian troops had been taken prisoner. The Italian Navy also suffered significant defeats, with the Royal Navy putting three Italian battleships out of commission after a carrier attack at Taranto, and neutralising several more warships at the Battle of Cape Matapan.\nItalian defeats prompted Germany to deploy an expeditionary force to North Africa; at the end of March 1941, Rommel's Afrika Korps launched an offensive which drove back Commonwealth forces. In less than a month, Axis forces advanced to western Egypt and besieged the port of Tobruk.\nBy late March 1941, Bulgaria and Yugoslavia signed the Tripartite Pact; however, the Yugoslav government was overthrown two days later by pro-British nationalists. Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month. The airborne invasion of the Greek island of Crete at the end of May completed the German conquest of the Balkans. Partisan warfare subsequently broke out against the Axis occupation of Yugoslavia, which continued until the end of the war.\nIn the Middle East in May, Commonwealth forces quashed an uprising in Iraq which had been supported by German aircraft from bases within Vichy-controlled Syria. Between June and July, British-led forces invaded and occupied the French possessions of Syria and Lebanon, assisted by the Free French.\n\n\n=== Axis attack on the Soviet Union (1941) ===\n\nWith the situation in Europe and Asia relatively stable, Germany, Japan, and the Soviet Union made preparations for war. With the Soviets wary of mounting tensions with Germany, and the Japanese planning to take advantage of the European War by seizing resource-rich European possessions in Southeast Asia, the two powers signed the Soviet–Japanese Neutrality Pact in April 1941. By contrast, the Germans were steadily making preparations for an attack on the Soviet Union, massing forces on the Soviet border.\nHitler believed that the United Kingdom's refusal to end the war was based on the hope that the United States and the Soviet Union would enter the war against Germany sooner or later. On 31 July 1940, Hitler decided that the Soviet Union should be eliminated and aimed for the conquest of Ukraine, the Baltic states and Byelorussia. However, other senior German officials like Ribbentrop saw an opportunity to create a Euro-Asian bloc against the British Empire by inviting the Soviet Union into the Tripartite Pact. In November 1940, negotiations took place to determine if the Soviet Union would join the pact. The Soviets showed some interest but asked for concessions from Finland, Bulgaria, Turkey, and Japan that Germany considered unacceptable. On 18 December 1940, Hitler issued the directive to prepare for an invasion of the Soviet Union.\nOn 22 June 1941, Germany, supported by Italy and Romania, invaded the Soviet Union in Operation Barbarossa, with Germany accusing the Soviets of plotting against them; they were joined shortly by Finland and Hungary. The primary targets of this surprise offensive were the Baltic region, Moscow and Ukraine, with the ultimate goal of ending the 1941 campaign near the Arkhangelsk-Astrakhan line—from the Caspian to the White Seas. Hitler's objectives were to eliminate the Soviet Union as a military power, exterminate Communism, generate Lebensraum (\"living space\") by dispossessing the native population, and guarantee access to the strategic resources needed to defeat Germany's remaining rivals.\nAlthough the Red Army was preparing for strategic counter-offensives before the war, Operation Barbarossa forced the Soviet supreme command to adopt strategic defence. During the summer, the Axis made significant gains into Soviet territory, inflicting immense losses in both personnel and materiel. By mid-August, however, the German Army High Command decided to suspend the offensive of a considerably depleted Army Group Centre, and to divert the 2nd Panzer Group to reinforce troops advancing towards central Ukraine and Leningrad. The Kiev offensive was overwhelmingly successful, resulting in encirclement and elimination of four Soviet armies, and made possible further advance into Crimea and industrially-developed Eastern Ukraine (the First Battle of Kharkov).\n\nThe diversion of three-quarters of the Axis troops and the majority of their air forces from France and the central Mediterranean to the Eastern Front prompted the United Kingdom to reconsider its grand strategy. In July, the UK and the Soviet Union formed a military alliance against Germany and in August, the United Kingdom and the United States jointly issued the Atlantic Charter, which outlined British and American goals for the post-war world. In late August the British and Soviets invaded neutral Iran to secure the Persian Corridor, Iran's oil fields, and preempt any Axis advances through Iran toward the Baku oil fields or India.\nBy October, Axis powers had achieved operational objectives in Ukraine and the Baltic region, with only the sieges of Leningrad and Sevastopol continuing. A major offensive against Moscow was renewed; after two months of fierce battles in increasingly harsh weather, the German army almost reached the outer suburbs of Moscow, where the exhausted troops were forced to suspend the offensive. Large territorial gains were made by Axis forces, but their campaign had failed to achieve its main objectives: two key cities remained in Soviet hands, the Soviet capability to resist was not broken, and the Soviet Union retained a considerable part of its military potential. The blitzkrieg phase of the war in Europe had ended.\nBy early December, freshly mobilised reserves allowed the Soviets to achieve numerical parity with Axis troops. This, as well as intelligence data which established that a minimal number of Soviet troops in the East would be sufficient to deter any attack by the Japanese Kwantung Army, allowed the Soviets to begin a massive counter-offensive that started on 5 December all along the front and pushed German troops 100–250 kilometres (62–155 mi) west.\n\n\n=== War breaks out in the Pacific (1941) ===\n\nFollowing the Japanese false flag Mukden incident in 1931, the Japanese shelling of the American gunboat USS Panay in 1937, and the 1937–1938 Nanjing Massacre, Japanese-American relations deteriorated. In 1939, the United States notified Japan that it would not be extending its trade treaty and American public opinion opposing Japanese expansionism led to a series of economic sanctions—the Export Control Acts—which banned U.S. exports of chemicals, minerals and military parts to Japan, and increased economic pressure on the Japanese regime. During 1939 Japan launched its first attack against Changsha, but was repulsed by late September. Despite several offensives by both sides, by 1940 the war between China and Japan was at a stalemate. To increase pressure on China by blocking supply routes, and to better position Japanese forces in the event of a war with the Western powers, Japan invaded and occupied northern Indochina in September 1940.\nChinese nationalist forces launched a large-scale counter-offensive in early 1940. In August, Chinese communists launched an offensive in Central China; in retaliation, Japan instituted harsh measures in occupied areas to reduce human and material resources for the communists. Continued antipathy between Chinese communist and nationalist forces culminated in armed clashes in January 1941, effectively ending their co-operation. In March, the Japanese 11th army attacked the headquarters of the Chinese 19th army but was repulsed during Battle of Shanggao. In September, Japan attempted to take the city of Changsha again and clashed with Chinese nationalist forces.\nGerman successes in Europe prompted Japan to increase pressure on European governments in Southeast Asia. The Dutch government agreed to provide Japan with oil supplies from the Dutch East Indies, but negotiations for additional access to their resources ended in failure in June 1941. In July 1941 Japan sent troops to southern Indochina, thus threatening British and Dutch possessions in the Far East. The United States, the United Kingdom, and other Western governments reacted to this move with a freeze on Japanese assets and a total oil embargo. At the same time, Japan was planning an invasion of the Soviet Far East, intending to take advantage of the German invasion in the west, but abandoned the operation after the sanctions.\nSince early 1941, the United States and Japan had been engaged in negotiations in an attempt to improve their strained relations and end the war in China. During these negotiations, Japan advanced a number of proposals which were dismissed by the Americans as inadequate. At the same time the United States, the United Kingdom, and the Netherlands engaged in secret discussions for the joint defence of their territories, in the event of a Japanese attack against any of them. Roosevelt reinforced the Philippines (an American protectorate scheduled for independence in 1946) and warned Japan that the United States would react to Japanese attacks against any \"neighboring countries\".\n\nFrustrated at the lack of progress and feeling the pinch of the American–British–Dutch sanctions, Japan prepared for war. Emperor Hirohito, after initial hesitation about Japan's chances of victory, began to favour Japan's entry into the war. As a result, Prime Minister Fumimaro Konoe resigned. Hirohito refused the recommendation to appoint Prince Naruhiko Higashikuni in his place, choosing War Minister Hideki Tojo instead. On 3 November, Nagano explained in detail the plan of the attack on Pearl Harbor to the Emperor. On 5 November, Hirohito approved in imperial conference the operations plan for the war. On 20 November, the new government presented an interim proposal as its final offer. It called for the end of American aid to China and for lifting the embargo on the supply of oil and other resources to Japan. In exchange, Japan promised not to launch any attacks in Southeast Asia and to withdraw its forces from southern Indochina. The American counter-proposal of 26 November required that Japan evacuate all of China without conditions and conclude non-aggression pacts with all Pacific powers. That meant Japan was essentially forced to choose between abandoning its ambitions in China, or seizing the natural resources it needed in the Dutch East Indies by force; the Japanese military did not consider the former an option, and many officers considered the oil embargo an unspoken declaration of war.\nJapan planned to seize European colonies in Asia to create a large defensive perimeter stretching into the Central Pacific. The Japanese would then be free to exploit the resources of Southeast Asia while exhausting the over-stretched Allies by fighting a defensive war. To prevent American intervention while securing the perimeter, it was further planned to neutralise the United States Pacific Fleet and the American military presence in the Philippines from the outset. On 7 December 1941 (8 December in Asian time zones), Japan attacked British and American holdings with near-simultaneous offensives against Southeast Asia and the Central Pacific. These included an attack on the American fleets at Pearl Harbor and the Philippines, as well as invasions of Guam, Wake Island, Malaya, Thailand, and Hong Kong.\nThese attacks led the United States, United Kingdom, China, Australia, and several other states to formally declare war on Japan, whereas the Soviet Union, being heavily involved in large-scale hostilities with European Axis countries, maintained its neutrality agreement with Japan. Germany, followed by the other Axis states, declared war on the United States in solidarity with Japan, citing as justification the American attacks on German war vessels that had been ordered by Roosevelt.\n\n\n=== Axis advance stalls (1942–1943) ===\nOn 1 January 1942, the Allied Big Four—the Soviet Union, China, the United Kingdom, and the United States—and 22 smaller or exiled governments issued the Declaration by United Nations, thereby affirming the Atlantic Charter and agreeing not to sign a separate peace with the Axis powers.\nDuring 1942, Allied officials debated on the appropriate grand strategy to pursue. All agreed that defeating Germany was the primary objective. The Americans favoured a straightforward, large-scale attack on Germany through France. The Soviets demanded a second front. The British argued that military operations should target peripheral areas to wear out German strength, leading to increasing demoralisation, and bolstering resistance forces; Germany itself would be subject to a heavy bombing campaign. An offensive against Germany would then be launched primarily by Allied armour, without using large-scale armies. Eventually, the British persuaded the Americans that a landing in France was infeasible in 1942 and they should instead focus on driving the Axis out of North Africa.\nAt the Casablanca Conference in early 1943, the Allies reiterated the statements issued in the 1942 Declaration and demanded the unconditional surrender of their enemies. The British and Americans agreed to continue to press the initiative in the Mediterranean by invading Sicily to fully secure the Mediterranean supply routes. Although the British argued for further operations in the Balkans to bring Turkey into the war, in May 1943, the Americans extracted a British commitment to limit Allied operations in the Mediterranean to an invasion of the Italian mainland, and to invade France in 1944.\n\n\n=== Pacific (1942–1943) ===\n\nBy the end of April 1942, Japan and its ally Thailand had almost conquered Burma, Malaya, the Dutch East Indies, Singapore, and Rabaul, inflicting severe losses on Allied troops and taking a large number of prisoners. Despite stubborn resistance by Filipino and U.S. forces, the Philippine Commonwealth was eventually captured in May 1942, forcing its government into exile. On 16 April, in Burma, 7,000 British soldiers were encircled by the Japanese 33rd Division during the Battle of Yenangyaung and rescued by the Chinese 38th Division. Japanese forces also achieved naval victories in the South China Sea, Java Sea, and Indian Ocean, and bombed the Allied naval base at Darwin, Australia. In January 1942, the only Allied success against Japan was a Chinese victory at Changsha. These easy victories over the unprepared U.S. and European opponents left Japan overconfident, and overextended.\nIn early May 1942, Japan initiated operations to capture Port Moresby by amphibious assault and thus sever communications and supply lines between the United States and Australia. The planned invasion was thwarted when an Allied task force, centred on two American fleet carriers, fought Japanese naval forces to a draw in the Battle of the Coral Sea. Japan's next plan, motivated by the earlier Doolittle Raid, was to seize Midway Atoll and lure American carriers into battle to be eliminated; as a diversion, Japan would also send forces to occupy the Aleutian Islands in Alaska. In mid-May, Japan started the Zhejiang-Jiangxi campaign in China, with the goal of inflicting retribution on the Chinese who aided the surviving American airmen in the Doolittle Raid by destroying Chinese air bases and fighting against the Chinese 23rd and 32nd Army Groups. In early June, Japan put its operations into action, but the Americans had broken Japanese naval codes in late May and were fully aware of the plans and order of battle, and used this knowledge to achieve a decisive victory at Midway over the Imperial Japanese Navy.\nWith its capacity for aggressive action greatly diminished as a result of the Midway battle, Japan attempted to capture Port Moresby by an overland campaign in the Territory of Papua. The Americans planned a counterattack against Japanese positions in the southern Solomon Islands, primarily Guadalcanal, as a first step towards capturing Rabaul, the main Japanese base in Southeast Asia.\nBoth plans started in July, but by mid-September, the Battle for Guadalcanal took priority for the Japanese, and troops in New Guinea were ordered to withdraw from the Port Moresby area to the northern part of the island, where they faced Australian and United States troops in the Battle of Buna–Gona. Guadalcanal soon became a focal point for both sides with heavy commitments of troops and ships in the battle for Guadalcanal. By the start of 1943, the Japanese were defeated on the island and withdrew their troops. In Burma, Commonwealth forces mounted two operations. The first was a disastrous offensive into the Arakan region in late 1942 that forced a retreat back to India by May 1943. The second was the insertion of irregular forces behind Japanese frontlines in February which, by the end of April, had achieved mixed results.\n\n\n=== Eastern Front (1942–1943) ===\n\nDespite considerable losses, in early 1942 Germany and its allies stopped a major Soviet offensive in central and southern Russia, keeping most territorial gains they had achieved during the previous year. In May, the Germans defeated Soviet offensives in the Kerch Peninsula and at Kharkov, and then in June 1942 launched their main summer offensive against southern Russia, to seize the oil fields of the Caucasus and occupy the Kuban steppe, while maintaining positions on the northern and central areas of the front. The Germans split Army Group South into two groups: Army Group A advanced to the lower Don River and struck south-east to the Caucasus, while Army Group B headed towards the Volga River. The Soviets decided to make their stand at Stalingrad on the Volga.\nBy mid-November, the Germans had nearly taken Stalingrad in bitter street fighting. The Soviets began their second winter counter-offensive, starting with an encirclement of German forces at Stalingrad, and an assault on the Rzhev salient near Moscow, though the latter failed disastrously. By early February 1943, the German Army had taken tremendous losses; German troops at Stalingrad had been defeated, and the front-line had been pushed back beyond its position before the summer offensive. In mid-February, after the Soviet push had tapered off, the Germans launched another attack on Kharkov, creating a salient in their front line around the Soviet city of Kursk.\n\n\n=== Western Europe/Atlantic and Mediterranean (1942–1943) ===\n\nExploiting poor American naval command decisions, the German navy ravaged Allied shipping off the American Atlantic coast. By November 1941, Commonwealth forces had launched a counter-offensive in North Africa, Operation Crusader, and reclaimed all the gains the Germans and Italians had made. The Germans also launched a North African offensive in January, pushing the British back to positions at the Gazala line by early February, followed by a temporary lull in combat which Germany used to prepare for their upcoming offensives. Concerns that the Japanese might use bases in Vichy-held Madagascar caused the British to invade the island in early May 1942. An Axis offensive in Libya forced an Allied retreat deep inside Egypt until Axis forces were stopped at El Alamein. On the Continent, raids of Allied commandos on strategic targets, culminating in the failed Dieppe Raid, demonstrated the Western Allies' inability to launch an invasion of continental Europe without much better preparation, equipment, and operational security.\nIn August 1942, the Allies succeeded in repelling a second attack against El Alamein and, at a high cost, managed to deliver desperately needed supplies to the besieged Malta. A few months later, the Allies commenced an attack of their own in Egypt, dislodging the Axis forces and beginning a drive west across Libya. This attack was followed up shortly after by Anglo-American landings in French North Africa, which resulted in the region joining the Allies. Hitler responded to the French colony's defection by ordering the occupation of Vichy France; although Vichy forces did not resist this violation of the armistice, they managed to scuttle their fleet to prevent its capture by German forces. Axis forces in Africa withdrew into Tunisia, which was conquered by the Allies in May 1943.\nIn June 1943, the British and Americans began a strategic bombing campaign against Germany with a goal to disrupt the war economy, reduce morale, and \"de-house\" the civilian population. The firebombing of Hamburg was among the first attacks in this campaign, inflicting significant casualties and considerable losses on infrastructure of this important industrial centre.\n\n\n=== Allies gain momentum (1943–1944) ===\n\nAfter the Guadalcanal campaign, the Allies initiated several operations against Japan in the Pacific. In May 1943, Canadian and U.S. forces were sent to eliminate Japanese forces from the Aleutians. Soon after, the United States, with support from Australia, New Zealand and Pacific Islander forces, began major ground, sea and air operations to isolate Rabaul by capturing surrounding islands, and breach the Japanese Central Pacific perimeter at the Gilbert and Marshall Islands. By the end of March 1944, the Allies had completed both of these objectives and had also neutralised the major Japanese base at Truk in the Caroline Islands. In April, the Allies launched an operation to retake Western New Guinea.\nIn the Soviet Union, both the Germans and the Soviets spent the spring and early summer of 1943 preparing for large offensives in central Russia. On 5 July 1943, Germany attacked Soviet forces around the Kursk Bulge. Within a week, German forces had exhausted themselves against the Soviets' well-constructed defences, and for the first time in the war, Hitler cancelled an operation before it had achieved tactical or operational success. This decision was partially affected by the Western Allies' invasion of Sicily launched on 9 July, which, combined with previous Italian failures, resulted in the ousting and arrest of Mussolini later that month.\nOn 12 July 1943, the Soviets launched their own counter-offensives, thereby dispelling any chance of German victory or even stalemate in the east. The Soviet victory at Kursk marked the end of German superiority, giving the Soviet Union the initiative on the Eastern Front. The Germans tried to stabilise their eastern front along the hastily fortified Panther–Wotan line, but the Soviets broke through it at Smolensk and the Lower Dnieper Offensive.\nOn 3 September 1943, the Western Allies invaded the Italian mainland, following Italy's armistice with the Allies and the ensuing German occupation of Italy. Germany, with the help of fascists, responded to the armistice by disarming Italian forces that were in many places without superior orders, seizing military control of Italian areas, and creating a series of defensive lines. German special forces then rescued Mussolini, who then soon established a new client state in German-occupied Italy named the Italian Social Republic, causing an Italian civil war. The Western Allies fought through several lines until reaching the main German defensive line in mid-November.\n\nGerman operations in the Atlantic also suffered. By May 1943, as Allied counter-measures became increasingly effective, the resulting sizeable German submarine losses forced a temporary halt of the German Atlantic naval campaign. In November 1943, Franklin D. Roosevelt and Winston Churchill met with Chiang Kai-shek in Cairo and then with Joseph Stalin in Tehran. The former conference determined the post-war return of Japanese territory and the military planning for the Burma campaign, while the latter included agreement that the Western Allies would invade Europe in 1944 and that the Soviet Union would declare war on Japan within three months of Germany's defeat.\nFrom November 1943, during the seven-week Battle of Changde, the Chinese awaited allied relief as they forced Japan to fight a costly war of attrition. In January 1944, the Allies launched a series of attacks in Italy against the line at Monte Cassino and tried to outflank it with landings at Anzio.\nOn 27 January 1944, Soviet troops launched a major offensive that expelled German forces from the Leningrad region, thereby ending the most lethal siege in history. The following Soviet offensive was halted on the pre-war Estonian border by the German Army Group North aided by Estonians hoping to re-establish national independence. This delay slowed subsequent Soviet operations in the Baltic Sea region. By late May 1944, the Soviets had liberated Crimea, largely expelled Axis forces from Ukraine, and made incursions into Romania, which were repulsed by the Axis troops. The Allied offensives in Italy had succeeded and, at the expense of allowing several German divisions to retreat, Rome was captured on 4 June.\nThe Allies had mixed success in mainland Asia. In March 1944, the Japanese launched the first of two invasions, an operation against Allied positions in Assam, India, and soon besieged Commonwealth positions at Imphal and Kohima. In May 1944, British and Indian forces mounted a counter-offensive that drove Japanese troops back to Burma by July, and Chinese forces that had invaded northern Burma in late 1943 besieged Japanese troops in Myitkyina. The second Japanese invasion of China aimed to destroy China's main fighting forces, secure railways between Japanese-held territory and capture Allied airfields. By June, the Japanese had conquered the province of Henan and begun a new attack on Changsha.\n\n\n=== Allies close in (1944) ===\n\nOn 6 June 1944 (commonly known as D-Day), after three years of Soviet pressure, the Western Allies invaded northern France. After reassigning several Allied divisions from Italy, they also attacked southern France. These landings were successful and led to the defeat of the German Army units in France. Paris was liberated on 25 August by the local resistance assisted by the Free French Forces, both led by General Charles de Gaulle, and the Western Allies continued to push back German forces in western Europe during the latter part of the year. An attempt to advance into northern Germany spearheaded by a major airborne operation in the Netherlands failed. After that, the Western Allies slowly pushed into Germany, but failed to cross the Ruhr river. In Italy, the Allied advance slowed due to the last major German defensive line.\nOn 22 June, the Soviets launched a strategic offensive in Belarus (\"Operation Bagration\") that nearly destroyed the German Army Group Centre. Soon after that, another Soviet strategic offensive forced German troops from Western Ukraine and Eastern Poland. The Soviets formed the Polish Committee of National Liberation to control territory in Poland and combat the Polish Armia Krajowa; the Soviet Red Army remained in the Praga district on the other side of the Vistula and watched passively as the Germans quelled the Warsaw Uprising initiated by the Armia Krajowa. The national uprising in Slovakia was also quelled by the Germans. The Soviet Red Army's strategic offensive in eastern Romania cut off and destroyed the considerable German troops there and triggered a successful coup d'état in Romania and in Bulgaria, followed by those countries' shift to the Allied side.\n\nIn September 1944, Soviet troops advanced into Yugoslavia and forced the rapid withdrawal of German Army Groups E and F in Greece, Albania and Yugoslavia to rescue them from being cut off. By this point, the communist-led Partisans under Marshal Josip Broz Tito, who had led an increasingly successful guerrilla campaign against the occupation since 1941, controlled much of the territory of Yugoslavia and engaged in delaying efforts against German forces further south. In northern Serbia, the Soviet Red Army, with limited support from Bulgarian forces, assisted the Partisans in a joint liberation of the capital city of Belgrade on 20 October. A few days later, the Soviets launched a massive assault against German-occupied Hungary that lasted until the fall of Budapest in February 1945. Unlike impressive Soviet victories in the Balkans, bitter Finnish resistance to the Soviet offensive in the Karelian Isthmus denied the Soviets occupation of Finland and led to a Soviet-Finnish armistice on relatively mild conditions, although Finland was forced to fight their former German allies.\nBy the start of July 1944, Commonwealth forces in Southeast Asia had repelled the Japanese sieges in Assam, pushing the Japanese back to the Chindwin River while the Chinese captured Myitkyina. In September 1944, Chinese forces captured Mount Song and reopened the Burma Road. In China, the Japanese had more successes, having finally captured Changsha in mid-June and the city of Hengyang by early August. Soon after, they invaded the province of Guangxi, winning major engagements against Chinese forces at Guilin and Liuzhou by the end of November and successfully linking up their forces in China and Indochina by mid-December.\nIn the Pacific, U.S. forces continued to push back the Japanese perimeter. In mid-June 1944, they began their offensive against the Mariana and Palau islands and decisively defeated Japanese forces in the Battle of the Philippine Sea. These defeats led to the resignation of the Japanese Prime Minister, Hideki Tojo, and provided the United States with air bases to launch intensive heavy bomber attacks on the Japanese home islands. In late October, American forces invaded the Filipino island of Leyte; soon after, Allied naval forces scored another large victory in the Battle of Leyte Gulf, one of the largest naval battles in history.\n\n\n=== Axis collapse and Allied victory (1944–1945) ===\n\nOn 16 December 1944, Germany made a last attempt to split the Allies on the Western Front by using most of its remaining reserves to launch a massive counter-offensive in the Ardennes and along the French-German border, hoping to encircle large portions of Western Allied troops and prompt a political settlement after capturing their primary supply port at Antwerp. By 16 January 1945, this offensive had been repulsed with no strategic objectives fulfilled. In Italy, the Western Allies remained stalemated at the German defensive line. In mid-January 1945, the Red Army attacked in Poland, pushing from the Vistula to the Oder river in Germany, and overran East Prussia. On 4 February Soviet, British, and U.S. leaders met for the Yalta Conference. They agreed on the occupation of post-war Germany, and on when the Soviet Union would join the war against Japan.\nIn February, the Soviets entered Silesia and Pomerania, while the Western Allies entered western Germany and closed to the Rhine river. By March, the Western Allies crossed the Rhine north and south of the Ruhr, encircling the German Army Group B. In early March, in an attempt to protect its last oil reserves in Hungary and retake Budapest, Germany launched its last major offensive against Soviet troops near Lake Balaton. Within two weeks, the offensive had been repulsed, the Soviets advanced to Vienna, and captured the city. In early April, Soviet troops captured Königsberg, while the Western Allies finally pushed forward in Italy and swept across western Germany capturing Hamburg and Nuremberg. American and Soviet forces met at the Elbe river on 25 April, leaving unoccupied pockets in southern Germany and around Berlin.\nSoviet troops stormed and captured Berlin in late April. In Italy, German forces surrendered on 29 April, while the Italian Social Republic capitulated two days later. On 30 April, the Reichstag was captured, signalling the military defeat of Nazi Germany.\nMajor changes in leadership occurred on both sides during this period. On 12 April, President Roosevelt died and was succeeded by his vice president, Harry S. Truman. Benito Mussolini was killed by Italian partisans on 28 April. On 30 April, Hitler committed suicide in his headquarters, and was succeeded by Grand Admiral Karl Dönitz (as President of the Reich) and Joseph Goebbels (as Chancellor of the Reich); Goebbels also committed suicide on the following day and was replaced by Lutz Graf Schwerin von Krosigk, in what would later be known as the Flensburg Government. Total and unconditional surrender in Europe was signed on 7 and 8 May, to be effective by the end of 8 May. German Army Group Centre resisted in Prague until 11 May. On 23 May all remaining members of the German government were arrested by the Allied Forces in Flensburg, while on 5 June all German political and military institutions were transferred under the control of the Allies through the Berlin Declaration.\nIn the Pacific theatre, American forces accompanied by the forces of the Philippine Commonwealth advanced in the Philippines, clearing Leyte by the end of April 1945. They landed on Luzon in January 1945 and recaptured Manila in March. Fighting continued on Luzon, Mindanao, and other islands of the Philippines until the end of the war. Meanwhile, the United States Army Air Forces launched a massive firebombing campaign of strategic cities in Japan in an effort to destroy Japanese war industry and civilian morale. A devastating bombing raid on Tokyo of 9–10 March was the deadliest conventional bombing raid in history.\n\nIn May 1945, Australian troops landed in Borneo, overrunning the oilfields there. British, American, and Chinese forces defeated the Japanese in northern Burma in March, and the British pushed on to reach Rangoon by 3 May. Chinese forces started a counterattack in the Battle of West Hunan that occurred between 6 April and 7 June 1945. American naval and amphibious forces also moved towards Japan, taking Iwo Jima by March, and Okinawa by the end of June. At the same time, a naval blockade by submarines was strangling Japan's economy and drastically reducing its ability to supply overseas forces.\nOn 11 July, Allied leaders met in Potsdam, Germany. They confirmed earlier agreements about Germany, and the American, British and Chinese governments reiterated the demand for unconditional surrender of Japan, specifically stating that \"the alternative for Japan is prompt and utter destruction\". During this conference, the United Kingdom held its general election, and Clement Attlee replaced Churchill as Prime Minister.\nThe call for unconditional surrender was rejected by the Japanese government, which believed it would be capable of negotiating for more favourable surrender terms. In early August, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki. Between the two bombings, the Soviets, pursuant to the Yalta agreement, declared war on Japan, invaded Japanese-held Manchuria and quickly defeated the Kwantung Army, which was the largest Japanese fighting force. These two events persuaded previously adamant Imperial Army leaders to accept surrender terms. The Red Army also captured the southern part of Sakhalin Island and the Kuril Islands. On the night of 9–10 August 1945, Emperor Hirohito announced his decision to accept the terms demanded by the Allies in the Potsdam Declaration. On 15 August, the Emperor communicated this decision to the Japanese people through a speech broadcast on the radio (Gyokuon-hōsō, literally \"broadcast in the Emperor's voice\"). On 15 August 1945, Japan surrendered, with the surrender documents finally signed at Tokyo Bay on the deck of the American battleship USS Missouri on 2 September 1945, ending the war.\n\n\n== Aftermath ==\n\nThe Allies established occupation administrations in Austria and Germany, both initially divided between western and eastern occupation zones controlled by the Western Allies and the Soviet Union, respectively. However, their paths soon diverged. In Germany, the western and eastern occupation zones controlled by the Western Allies and the Soviet Union officially ended in 1949, with the respective zones becoming separate countries, West Germany and East Germany. In Austria, however, occupation continued until 1955, when a joint settlement between the Western Allies and the Soviet Union permitted the reunification of Austria as a democratic state officially non-aligned with any political bloc (although in practice having better relations with the Western Allies). A denazification program in Germany led to the prosecution of Nazi war criminals in the Nuremberg trials and the removal of ex-Nazis from power, although this policy moved towards amnesty and re-integration of ex-Nazis into West German society.\nGermany lost a quarter of its pre-war (1937) territory. Among the eastern territories, Silesia, Neumark and most of Pomerania were taken over by Poland, and East Prussia was divided between Poland and the Soviet Union, followed by the expulsion to Germany of the nine million Germans from these provinces, as well as three million Germans from the Sudetenland in Czechoslovakia. By the 1950s, one-fifth of West Germans were refugees from the east. The Soviet Union also took over the Polish provinces east of the Curzon Line, from which two million Poles were expelled. North-east Romania, parts of eastern Finland, and the Baltic states were annexed into the Soviet Union. Italy lost its monarchy, colonial empire and some European territories.\nIn an effort to maintain world peace, the Allies formed the United Nations, which officially came into existence on 24 October 1945, and adopted the Universal Declaration of Human Rights in 1948 as a common standard for all member nations. The great powers that were the victors of the war—France, China, the United Kingdom, the Soviet Union and the United States—became the permanent members of the UN's Security Council. The five permanent members remain so to the present, although there have been two seat changes, between the Republic of China and the People's Republic of China in 1971, and between the Soviet Union and its successor state, the Russian Federation, following the dissolution of the Soviet Union in 1991. The alliance between the Western Allies and the Soviet Union had begun to deteriorate even before the war was over.\n\nBesides Germany, the rest of Europe was also divided into Western and Soviet spheres of influence. Most eastern and central European countries fell into the Soviet sphere, which led to establishment of Communist-led regimes, with full or partial support of the Soviet occupation authorities. As a result, East Germany, Poland, Hungary, Romania, Bulgaria, Czechoslovakia, and Albania became Soviet satellite states. Communist Yugoslavia conducted a fully independent policy, causing tension with the Soviet Union. A Communist uprising in Greece was put down with Anglo-American support and the country remained aligned with the West.\nPost-war division of the world was formalised by two international military alliances, the United States-led NATO and the Soviet-led Warsaw Pact. The long period of political tensions and military competition between them—the Cold War—would be accompanied by an unprecedented arms race and number of proxy wars throughout the world.\nIn Asia, the United States led the occupation of Japan and administered Japan's former islands in the Western Pacific, while the Soviets annexed South Sakhalin and the Kuril Islands. Korea, formerly under Japanese colonial rule, was divided and occupied by the Soviet Union in the North and the United States in the South between 1945 and 1948. Separate republics emerged on both sides of the 38th parallel in 1948, each claiming to be the legitimate government for all of Korea, which led ultimately to the Korean War.\nIn China, nationalist and communist forces resumed the civil war in June 1946. Communist forces were victorious and established the People's Republic of China on the mainland, while nationalist forces retreated to Taiwan in 1949. In the Middle East, the Arab rejection of the United Nations Partition Plan for Palestine and the creation of Israel marked the escalation of the Arab–Israeli conflict. While European powers attempted to retain some or all of their colonial empires, their losses of prestige and resources during the war rendered this unsuccessful, leading to decolonisation.\nThe global economy suffered heavily from the war, although participating nations were affected differently. The United States emerged much richer than any other nation, leading to a baby boom, and by 1950 its gross domestic product per person was much higher than that of any of the other powers, and it dominated the world economy. The Allied occupational authorities pursued a policy of industrial disarmament in Western Germany from 1945 to 1948. Due to international trade interdependencies, this policy led to an economic stagnation in Europe and delayed European recovery from the war for several years.\nAt the Bretton Woods Conference in July 1944, the Allied nations drew up an economic framework for the post-war world. The agreement created the International Monetary Fund (IMF) and the International Bank for Reconstruction and Development (IBRD), which later became part of the World Bank Group. The Bretton Woods system lasted until 1973. Recovery began with the mid-1948 currency reform in Western Germany, and was sped up by the liberalisation of European economic policy that the U.S. Marshall Plan economic aid (1948–1951) both directly and indirectly caused. The post-1948 West German recovery has been called the German economic miracle. Italy also experienced an economic boom and the French economy rebounded. By contrast, the United Kingdom was in a state of economic ruin, and although receiving a quarter of the total Marshall Plan assistance, more than any other European country, it continued in relative economic decline for decades. The Soviet Union, despite enormous human and material losses, also experienced rapid increase in production in the immediate post-war era, having seized and transferred most of Germany's industrial plants and exacted war reparations from its satellite states. Japan recovered much later. China returned to its pre-war industrial production by 1952.\n\n\n== Impact ==\n\n\n=== Casualties and war crimes ===\n\nEstimates for the total number of casualties in the war vary, because many deaths went unrecorded. Most suggest that some 60 million people died in the war, including about 20 million military personnel and 40 million civilians.\nThe Soviet Union alone lost around 27 million people during the war, including 8.7 million military and 19 million civilian deaths. A quarter of the total people in the Soviet Union were wounded or killed. Germany sustained 5.3 million military losses, mostly on the Eastern Front and during the final battles in Germany.\nAn estimated 11 to 17 million civilians died as a direct or as an indirect result of Hitler's racist policies, including mass killing of around 6 million Jews, along with Roma, homosexuals, at least 1.9 million ethnic Poles and millions of other Slavs (including Russians, Ukrainians and Belarusians), and other ethnic and minority groups. Between 1941 and 1945, more than 200,000 ethnic Serbs, along with Roma and Jews, were persecuted and murdered by the Axis-aligned Croatian Ustaše in Yugoslavia. Concurrently, Muslims and Croats were persecuted and killed by Serb nationalist Chetniks, with an estimated 50,000–68,000 victims (of which 41,000 were civilians). Also, more than 100,000 Poles were massacred by the Ukrainian Insurgent Army in the Volhynia massacres, between 1943 and 1945. At the same time, about 10,000–15,000 Ukrainians were killed by the Polish Home Army and other Polish units, in reprisal attacks.\n\nIn Asia and the Pacific, the number of people killed by Japanese troops remains contested. According to R.J. Rummel, the Japanese killed between 3 million and more than 10 million people, with the most probable case of almost 6,000,000 people. According to the British historian M. R. D. Foot, civilian deaths are between 10 million and 20 million, whereas Chinese military casualties (killed and wounded) are estimated to be over five million. Other estimates say that up to 30 million people, most of them civilians, were killed. The most infamous Japanese atrocity was the Nanjing Massacre, in which fifty to three hundred thousand Chinese civilians were raped and murdered. Mitsuyoshi Himeta reported that 2.7 million casualties occurred during the Three Alls policy. General Yasuji Okamura implemented the policy in Hebei and Shandong.\nAxis forces employed biological and chemical weapons. The Imperial Japanese Army used a variety of such weapons during its invasion and occupation of China (see Unit 731) and in early conflicts against the Soviets. Both the Germans and the Japanese tested such weapons against civilians, and sometimes on prisoners of war.\nThe Soviet Union was responsible for the Katyn massacre of 22,000 Polish officers, and the imprisonment or execution of hundreds of thousands of political prisoners by the NKVD secret police, along with mass civilian deportations to Siberia, in the Baltic states and eastern Poland annexed by the Red Army. Soviet soldiers committed mass rapes in occupied territories, especially in Germany. The exact number of German women and girls raped by Soviet troops during the war and occupation is uncertain, but historians estimate their numbers are likely in the hundreds of thousands, and possibly as many as two million, while figures for women raped by German soldiers in the Soviet Union go as far as ten million.\nThe mass bombing of cities in Europe and Asia has often been called a war crime, although no positive or specific customary international humanitarian law with respect to aerial warfare existed before or during World War II. The USAAF bombed a total of 67 Japanese cities, killing 393,000 civilians, including the atomic bombings of Hiroshima and Nagasaki, and destroying 65% of built-up areas.\n\n\n=== Genocide, concentration camps, and slave labour ===\n\nNazi Germany, under the dictatorship of Adolf Hitler, was responsible for murdering about 6 million Jews in what is now known as the Holocaust. They also murdered an additional 4 million others who were deemed \"unworthy of life\" (including the disabled and mentally ill, Soviet prisoners of war, Romani, homosexuals, Freemasons, and Jehovah's Witnesses) as part of a program of deliberate extermination, in effect becoming a \"genocidal state\". Soviet POWs were kept in especially unbearable conditions, and 3.6 million Soviet POWs out of 5.7 million died in Nazi camps during the war. In addition to concentration camps, death camps were created in Nazi Germany to exterminate people on an industrial scale. Nazi Germany extensively used forced labourers; about 12 million Europeans from German-occupied countries were abducted and used as a slave work force in German industry, agriculture and war economy.\n\nThe Soviet Gulag became a de facto system of deadly camps during 1942–1943, when wartime privation and hunger caused numerous deaths of inmates, including foreign citizens of Poland and other countries occupied in 1939–1940 by the Soviet Union, as well as Axis POWs. By the end of the war, most Soviet POWs liberated from Nazi camps and many repatriated civilians were detained in special filtration camps where they were subjected to NKVD evaluation, and 226,127 were sent to the Gulag as real or perceived Nazi collaborators.\nJapanese prisoner-of-war camps, many of which were used as labour camps, also had high death rates. The International Military Tribunal for the Far East found the death rate of Western prisoners was 27 percent (for American POWs, 37 percent), seven times that of POWs under the Germans and Italians. While 37,583 prisoners from the UK, 28,500 from the Netherlands, and 14,473 from the United States were released after the surrender of Japan, the number of Chinese released was only 56.\nAt least five million Chinese civilians from northern China and Manchukuo were enslaved between 1935 and 1941 by the East Asia Development Board, or Kōain, for work in mines and war industries. After 1942, the number reached 10 million. In Java, between 4 and 10 million rōmusha (Japanese: \"manual labourers\"), were forced to work by the Japanese military. About 270,000 of these Javanese labourers were sent to other Japanese-held areas in Southeast Asia, and only 52,000 were repatriated to Java.\n\n\n=== Occupation ===\n\nIn Europe, occupation came under two forms. In Western, Northern, and Central Europe (France, Norway, Denmark, the Low Countries, and the annexed portions of Czechoslovakia) Germany established economic policies through which it collected roughly 69.5 billion reichsmarks (27.8 billion U.S. dollars) by the end of the war; this figure does not include the plunder of industrial products, military equipment, raw materials and other goods. Thus, the income from occupied nations was over 40 percent of the income Germany collected from taxation, a figure which increased to nearly 40 percent of total German income as the war went on.\n\nIn the East, the intended gains of Lebensraum were never attained as fluctuating front-lines and Soviet scorched earth policies denied resources to the German invaders. Unlike in the West, the Nazi racial policy encouraged extreme brutality against what it considered to be the \"inferior people\" of Slavic descent; most German advances were thus followed by mass atrocities and war crimes. The Nazis killed an estimated 2.77 million ethnic Poles during the war in addition to Polish-Jewish victims of the Holocaust. Although resistance groups formed in most occupied territories, they did not significantly hamper German operations in either the East or the West until late 1943.\nIn Asia, Japan termed nations under its occupation as being part of the Greater East Asia Co-Prosperity Sphere, essentially a Japanese hegemony which it claimed was for purposes of liberating colonised peoples. Although Japanese forces were sometimes welcomed as liberators from European domination, Japanese war crimes frequently turned local public opinion against them. During Japan's initial conquest, it captured 4,000,000 barrels (640,000 m3) of oil (~550,000 tonnes) left behind by retreating Allied forces; and by 1943, was able to get production in the Dutch East Indies up to 50 million barrels (7,900,000 m3) of oil (~6.8 million tonnes), 76 percent of its 1940 output rate.\n\n\n=== Home fronts and production ===\n\nIn the 1930s Britain and the United States together controlled almost 75% of world mineral output—essential for projecting military power.\nIn Europe, before the outbreak of the war, the Allies had significant advantages in both population and economics. In 1938, the Western Allies (United Kingdom, France, Poland and the British Dominions) had a 30 percent larger population and a 30 percent higher gross domestic product than the European Axis powers (Germany and Italy); including colonies, the Allies had more than a 5:1 advantage in population and a nearly 2:1 advantage in GDP. In Asia at the same time, China had roughly six times the population of Japan but only an 89 percent higher GDP; this reduces to three times the population and only a 38 percent higher GDP if Japanese colonies are included.\nThe United States produced about two-thirds of all munitions used by the Allies in World War II, including warships, transports, warplanes, artillery, tanks, trucks, and ammunition. Although the Allies' economic and population advantages were largely mitigated during the initial rapid blitzkrieg attacks of Germany and Japan, they became the decisive factor by 1942, after the United States and Soviet Union joined the Allies and the war evolved into one of attrition. While the Allies' ability to out-produce the Axis was partly due to more access to natural resources, other factors, such as Germany and Japan's reluctance to employ women in the labour force, Allied strategic bombing, and Germany's late shift to a war economy contributed significantly. Additionally, neither Germany nor Japan planned to fight a protracted war, and had not equipped themselves to do so. To improve their production, Germany and Japan used millions of slave labourers; Germany enslaved about 12 million people, mostly from Eastern Europe, while Japan used more than 18 million people in Far East Asia.\n\n\n=== Advances in technology and its application ===\n\nAircraft were used for reconnaissance, as fighters, bombers, and ground-support, and each role developed considerably. Innovations included airlift (the capability to quickly move limited high-priority supplies, equipment, and personnel); and strategic bombing (the bombing of enemy industrial and population centres to destroy the enemy's ability to wage war). Anti-aircraft weaponry also advanced, including defences such as radar and surface-to-air artillery, in particular the introduction of the proximity fuze. The use of the jet aircraft was pioneered and led to jets becoming standard in air forces worldwide.\nAdvances were made in nearly every aspect of naval warfare, most notably with aircraft carriers and submarines. Although aeronautical warfare had relatively little success at the start of the war, actions at Taranto, Pearl Harbor, and the Coral Sea established the carrier as the dominant capital ship (in place of the battleship). In the Atlantic, escort carriers became a vital part of Allied convoys, increasing the effective protection radius and helping to close the Mid-Atlantic gap. Carriers were also more economical than battleships due to the relatively low cost of aircraft and because they are not required to be as heavily armoured. Submarines, which had proved to be an effective weapon during the First World War, were expected by all combatants to be important in the second. The British focused development on anti-submarine weaponry and tactics, such as sonar and convoys, while Germany focused on improving its offensive capability, with designs such as the Type VII submarine and wolfpack tactics. Gradually, improving Allied technologies such as the Leigh Light, Hedgehog, Squid, and homing torpedoes proved effective against German submarines.\n\nLand warfare changed from the static frontlines of trench warfare of World War I, which had relied on improved artillery that outmatched the speed of both infantry and cavalry, to increased mobility and combined arms. The tank, which had been used predominantly for infantry support in the First World War, had evolved into the primary weapon. In the late 1930s, tank design was considerably more advanced than it had been during World War I, and advances continued throughout the war with increases in speed, armour and firepower. At the start of the war, most commanders thought enemy tanks should be met by tanks with superior specifications. This idea was challenged by the poor performance of the relatively light early tank guns against armour, and German doctrine of avoiding tank-versus-tank combat. This, along with Germany's use of combined arms, were among the key elements of their highly successful blitzkrieg tactics across Poland and France. Many means of destroying tanks, including indirect artillery, anti-tank guns (both towed and self-propelled), mines, short-ranged infantry antitank weapons, and other tanks were used. Even with large-scale mechanisation, infantry remained the backbone of all forces, and throughout the war, most infantry were equipped similarly to World War I. The portable machine gun spread, a notable example being the German MG 34, and various submachine guns which were suited to close combat in urban and jungle settings. The assault rifle, a late war development incorporating many features of the rifle and submachine gun, became the standard post-war infantry weapon for most armed forces.\nMost major belligerents attempted to solve the problems of complexity and security involved in using large codebooks for cryptography by designing ciphering machines, the most well-known being the German Enigma machine. Development of SIGINT (signals intelligence) and cryptanalysis enabled the countering process of decryption. Notable examples were the Allied decryption of Japanese naval codes and British Ultra, a pioneering method for decoding Enigma that benefited from information given to the United Kingdom by the Polish Cipher Bureau, which had been decoding early versions of Enigma before the war. Another component of military intelligence was deception, which the Allies used to great effect in operations such as Mincemeat and Bodyguard.\nOther technological and engineering feats achieved during, or as a result of, the war include the world's first programmable computers (Z3, Colossus, and ENIAC), guided missiles and modern rockets, the Manhattan Project's development of nuclear weapons, operations research, the development of artificial harbours, and oil pipelines under the English Channel. Penicillin was first developed, mass-produced, and used during the war.\n\n\n== See also ==\nOpposition to World War II\nWorld War III\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== Further reading ==\nBuchanan, Andrew (7 February 2023). \"Globalizing the Second World War\". Past & Present (258): 246–281. doi:10.1093/pastj/gtab042. ISSN 0031-2746. also see online review Archived 4 May 2024 at the Wayback Machine\nGerlach, Christian (2024). Conditions of Violence. Walter de Gruyter GmbH & Co KG. ISBN 978-3-1115-6873-7.\n\n\n== External links ==\n\nWest Point Maps of the European War. Archived 23 March 2019 at the Wayback Machine.\nWest Point Maps of the Asian-Pacific War. Archived 23 March 2019 at the Wayback Machine.\nAtlas of the World Battle Fronts (July 1943 – August 1945)"
    },
    {
        "id": "ancient_greece",
        "name": "Ancient Greece",
        "text": "Ancient Greece (Ancient Greek: Ἑλλάς, romanized: Hellás) was a northeastern Mediterranean civilization, existing from the Greek Dark Ages of the 12th–9th centuries BC to the end of classical antiquity (c. 600 AD), that comprised a loose collection of culturally and linguistically related city-states and communities. Prior to the Roman period, most of these regions were officially unified only once under the Kingdom of Macedon from 338 to 323 BC. In Western history, the era of classical antiquity was immediately followed by the Early Middle Ages and the Byzantine period.\nThree centuries after the decline of Mycenaean Greece during the Bronze Age collapse, Greek urban poleis began to form in the 8th century BC, ushering in the Archaic period and the colonization of the Mediterranean Basin. This was followed by the age of Classical Greece, from the Greco-Persian Wars to the death of Alexander the Great in 323 BC, and which included the Golden Age of Athens and the Peloponnesian War. The unification of Greece by Macedon under Philip II and subsequent conquest of the Achaemenid Empire by Alexander the Great spread Hellenistic civilization across the Middle East. The Hellenistic period is considered to have ended in 30 BC, when the last Hellenistic kingdom, Ptolemaic Egypt, was annexed by the Roman Republic.\nClassical Greek culture, especially philosophy, had a powerful influence on ancient Rome, which carried a version of it throughout the Mediterranean and much of Europe. For this reason, Classical Greece is generally considered the cradle of Western civilization, the seminal culture from which the modern West derives many of its founding archetypes and ideas in politics, philosophy, science, and art.\n\n\n== Chronology ==\n\nClassical antiquity in the Mediterranean region is commonly considered to have begun in the 8th century BC (around the time of the earliest recorded poetry of Homer) and ended in the 6th century AD.\nClassical antiquity in Greece was preceded by the Greek Dark Ages (c. 1200 – c. 800 BC), archaeologically characterised by the protogeometric and geometric styles of designs on pottery. Following the Dark Ages was the Archaic period, beginning around the 8th century BC, which saw early developments in Greek culture and society leading to the Classical period from the Persian invasion of Greece in 480 BC until the death of Alexander the Great in 323 BC. The Classical period is characterized by a \"classical\" style, i.e. one which was considered exemplary by later observers, most famously in the Parthenon of Athens. Politically, the Classical period was dominated by Athens and the Delian League during the 5th century, but displaced by Spartan hegemony during the early 4th century BC, before power shifted to Thebes and the Boeotian League and finally to the League of Corinth led by Macedon. This period was shaped by the Greco-Persian Wars, the Peloponnesian War, and the Rise of Macedon.\nFollowing the Classical period was the Hellenistic period (323–146 BC), during which Greek culture and power expanded into the Near East from the death of Alexander until the Roman conquest. Roman Greece is usually counted from the Roman victory over the Corinthians at the Battle of Corinth in 146 BC to the establishment of Byzantium by Constantine as the capital of the Roman Empire in 330 AD. Finally, Late Antiquity refers to the period of Christianization during the later 4th to early 6th centuries AD, consummated by the closure of the Academy of Athens by Justinian I in 529.\n\n\n== Historiography ==\n\nThe historical period of ancient Greece is unique in world history as the first period attested directly in comprehensive, narrative historiography, while earlier ancient history or protohistory is known from much more fragmentary documents such as annals, king lists, and pragmatic epigraphy.\nHerodotus is widely known as the \"father of history\": his Histories are eponymous of the entire field. Written between the 450s and 420s BC, Herodotus' work reaches about a century into the past, discussing 6th-century BC historical figures such as Darius I of Persia, Cambyses II and Psamtik III, and alluding to some 8th-century BC persons such as Candaules. The accuracy of Herodotus' works is debated.\nHerodotus was succeeded by authors such as Thucydides, Xenophon, Demosthenes, Plato and Aristotle. Most were either Athenian or pro-Athenian, which is why far more is known about the history and politics of Athens than of many other cities.\nTheir scope is further limited by a focus on political, military and diplomatic history, ignoring economic and social history.\n\n\n== History ==\n\n\n=== Archaic period ===\n\nThe archaic period, lasting approximately from 800 to 500 BC, saw the culmination of political and social developments which had begun in the Greek dark age, with the polis (city-state) becoming the most important unit of political organisation in Greece. The absence of powerful states in Greece after the collapse of Mycenaean power, and the geography of Greece, where many settlements were separated from their neighbours by mountainous terrain, encouraged the development of small independent city-states. Several Greek states saw tyrants rise to power in this period, most famously at Corinth from 657 BC. The period also saw the founding of Greek colonies around the Mediterranean, with Euboean settlements at Al-Mina in the east as early as 800 BC, and Ischia in the west by 775. Increasing contact with non-Greek peoples in this period, especially in the Near East, inspired developments in art and architecture, the adoption of coinage, and the development of the Greek alphabet.\nAthens developed its democratic system over the course of the archaic period. Already in the 7th century, the right of all citizen men to attend the assembly appears to have been established. After a failed coup led by Cylon of Athens around 636 BC, Draco was appointed to establish a code of laws in 621. This failed to reduce the political tension between the poor and the elites, and in 594 Solon was given the authority to enact another set of reforms, which attempted to balance the power of the rich and the poor. In the middle of the 6th century, Pisistratus established himself as a tyrant, and after his death in 527 his son Hippias inherited his position; by the end of the 6th century he had been overthrown, and Cleisthenes carried out further democratising reforms.\nIn Sparta, a political system with two kings, a council of elders, and five ephors developed over the course of the 8th and 7th centuries. According to Spartan tradition, this constitution was established by the legendary lawgiver Lycurgus. Over the course of the first and second Messenian wars, Sparta subjugated the neighbouring region of Messenia, enserfing the population.\nIn the 6th century, Greek city-states began to develop formal relationships with one another, where previously individual rulers had relied on personal relationships with the elites of other cities. Towards the end of the Archaic period, Sparta began to build a series of alliances, the Peloponnesian League, with cities including Corinth, Elis, and Megara, isolating Messenia and reinforcing Sparta's position against Argos, the other major power in the Peloponnese. Other alliances in the 6th century included those between Elis and Heraea in the Peloponnese; and between the Greek colony Sybaris in southern Italy, its allies, and the Serdaioi.\n\n\n=== Classical Greece ===\n\nIn 499 BC, the Ionian city states under Persian rule rebelled against their Persian-supported tyrant rulers. Supported by troops sent from Athens and Eretria, they advanced as far as Sardis and burnt the city before being driven back by a Persian counterattack. The revolt continued until 494, when the rebelling Ionians were defeated. Darius did not forget that Athens had assisted the Ionian revolt, and in 490 he assembled an armada to retaliate. Though heavily outnumbered, the Athenians—supported by their Plataean allies—defeated the Persian hordes at the Battle of Marathon, and the Persian fleet turned tail.\n\nTen years later, a second invasion was launched by Darius' son Xerxes. The city-states of northern and central Greece submitted to the Persian forces without resistance, but a coalition of 31 Greek city states, including Athens and Sparta, determined to resist the Persian invaders. At the same time, Greek Sicily was invaded by a Carthaginian force. In 480 BC, the first major battle of the invasion was fought at Thermopylae, where a small rearguard of Greeks, led by three hundred Spartans, held a crucial pass guarding the heart of Greece for several days; at the same time Gelon, tyrant of Syracuse, defeated the Carthaginian invasion at the Battle of Himera.\nThe Persians were decisively defeated at sea by a primarily Athenian naval force at the Battle of Salamis, and on land in 479 BC at the Battle of Plataea. The alliance against Persia continued, initially led by the Spartan Pausanias but from 477 by Athens, and by 460 Persia had been driven out of the Aegean. During this long campaign, the Delian League gradually transformed from a defensive alliance of Greek states into an Athenian empire, as Athens' growing naval power intimidated the other league states. Athens ended its campaigns against Persia in 450, after a disastrous defeat in Egypt in 454, and the death of Cimon in action against the Persians on Cyprus in 450.\nAs the Athenian fight against the Persian empire waned, conflict grew between Athens and Sparta. Suspicious of the increasing Athenian power funded by the Delian League, Sparta offered aid to reluctant members of the League to rebel against Athenian domination. These tensions were exacerbated in 462 BC when Athens sent a force to aid Sparta in overcoming a helot revolt, but this aid was rejected by the Spartans. In the 450s, Athens took control of Boeotia, and won victories over Aegina and Corinth. However, Athens failed to win a decisive victory, and in 447 lost Boeotia again. Athens and Sparta signed the Thirty Years' Peace in the winter of 446/445, ending the conflict.\nDespite the treaty, Athenian relations with Sparta declined again in the 430s, and in 431 BC the Peloponnesian War began. The first phase of the war saw a series of fruitless annual invasions of Attica by Sparta, while Athens successfully fought the Corinthian empire in northwest Greece and defended its own empire, despite a plague which killed the leading Athenian statesman Pericles. The war turned after Athenian victories led by Cleon at Pylos and Sphakteria, and Sparta sued for peace, but the Athenians rejected the proposal. The Athenian failure to regain control of Boeotia at Delium and Brasidas' successes in northern Greece in 424 improved Sparta's position after Sphakteria. After the deaths of Cleon and Brasidas, the strongest proponents of war on each side, a peace treaty was negotiated in 421 by the Athenian general Nicias.\nThe peace did not last, however. In 418 BC allied forces of Athens and Argos were defeated by Sparta at Mantinea. In 415 Athens launched an ambitious naval expedition to dominate Sicily; the expedition ended in disaster at the harbor of Syracuse, with almost the entire army killed, and the ships destroyed. Soon after the Athenian defeat in Syracuse, Athens' Ionian allies began to rebel against the Delian league, while Persia began to once again involve itself in Greek affairs on the Spartan side. Initially the Athenian position continued relatively strong, with important victories at Cyzicus in 410 and Arginusae in 406. However, in 405 the Spartan Lysander defeated Athens in the Battle of Aegospotami, and began to blockade Athens' harbour; driven by hunger, Athens sued for peace, agreeing to surrender their fleet and join the Spartan-led Peloponnesian League. Following the Athenian surrender, Sparta installed an oligarchic regime, the Thirty Tyrants, in Athens, one of a number of Spartan-backed oligarchies which rose to power after the Peloponnesian war. Spartan predominance did not last: after only a year, the Thirty had been overthrown.\nThe first half of the 4th century saw the major Greek states attempt to dominate the mainland; none were successful, and their resulting weakness led to a power vacuum which was eventually filled by Macedon under Philip II and then Alexander the Great. In the immediate aftermath of the Peloponnesian war, Sparta attempted to extend their own power, leading Argos, Athens, Corinth, and Thebes to join against them. Aiming to prevent any single Greek state gaining the dominance that would allow it to challenge Persia, the Persian king initially joined the alliance against Sparta, before imposing the Peace of Antalcidas (\"King's Peace\") which restored Persia's control over the Anatolian Greeks.\n\nBy 371 BC, Thebes was in the ascendancy, defeating Sparta at the Battle of Leuctra, killing the Spartan king Cleombrotus I, and invading Laconia. Further Theban successes against Sparta in 369 led to Messenia gaining independence; Sparta never recovered from the loss of Messenia's fertile land and the helot workforce it provided. The rising power of Thebes led Sparta and Athens to join forces; in 362 they were defeated by Thebes at the Battle of Mantinea. In the aftermath of Mantinea, none of the major Greek states were able to dominate. Though Thebes had won the battle, their general Epaminondas was killed, and they spent the following decades embroiled in wars with their neighbours; Athens, meanwhile, saw its second naval alliance, formed in 377, collapse in the mid-350s.\nThe power vacuum in Greece after the Battle of Mantinea was filled by Macedon, under Philip II. In 338 BC, he defeated a Greek alliance at the Battle of Chaeronea, and subsequently formed the League of Corinth. Philip planned to lead the League to invade Persia, but was murdered in 336 BC. His son Alexander the Great was left to fulfil his father's ambitions. After campaigns against Macedon's western and northern enemies, and those Greek states that had broken from the League of Corinth following the death of Philip, Alexander began his campaign against Persia in 334 BC. He conquered Persia, defeating Darius III at the Battle of Issus in 333 BC, and after the Battle of Gaugamela in 331 BC proclaimed himself king of Asia. From 329 BC he led expeditions to Bactria and then India; further plans to invade Arabia and North Africa were halted by his death in 323 BC.\n\n\n=== Hellenistic Greece ===\n\nThe period from the death of Alexander the Great in 323 BC until the death of Cleopatra, the last Macedonian ruler of Egypt, is known as the Hellenistic period. In the early part of this period, a new form of kingship developed based on Macedonian and Near Eastern traditions. The first Hellenistic kings were previously Alexander's generals, and took power in the period following his death, though they were not part of existing royal lineages and lacked historic claims to the territories they controlled. The most important of these rulers in the decades after Alexander's death were Antigonus I and his son Demetrius in Macedonia and the rest of Greece, Ptolemy in Egypt, and Seleucus I in Syria and the former Persian empire; smaller Hellenistic kingdoms included the Attalids in Anatolia and the Greco-Bactrian kingdom.\n\nIn the early part of the Hellenistic period, the exact borders of the Hellenistic kingdoms were not settled. Antigonus attempted to expand his territory by attacking the other successor kingdoms until they joined against him, and he was killed at the Battle of Ipsus in 301 BC. His son Demetrius spent many years in Seleucid captivity, and his son, Antigonus II, only reclaimed the Macedonian throne around 276. Meanwhile, the Seleucid kingdom gave up territory in the east to the Indian king Chandragupta Maurya in exchange for war elephants, and later lost large parts of Persia to the Parthian Empire. By the mid-3rd century, the kingdoms of Alexander's successors was mostly stable, though there continued to be disputes over border areas.\nThe great capitals of Hellenistic culture were Alexandria in the Ptolemaic Kingdom and Antioch in the Seleucid Empire.\nThe conquests of Alexander had numerous consequences for the Greek city-states. It greatly widened the horizons of the Greeks and led to a steady emigration of the young and ambitious to the new Greek empires in the east. Many Greeks migrated to Alexandria, Antioch and the many other new Hellenistic cities founded in Alexander's wake, as far away as present-day Afghanistan and Pakistan, where the Greco-Bactrian Kingdom and the Indo-Greek Kingdom survived until the end of the 1st century BC.\nThe city-states within Greece formed themselves into two leagues; the Achaean League (including Corinth and Argos) and the Aetolian League (including Sparta and Athens). For much of the period until the Roman conquest, these leagues were at war, often participating in the conflicts between the Diadochi (the successor states to Alexander's empire).\nThe Antigonid Kingdom became involved in a war with the Roman Republic in the late 3rd century. Although the First Macedonian War was inconclusive, the Romans, in typical fashion, continued to fight Macedon until it was completely absorbed into the Roman Republic (by 149 BC). In the east, the unwieldy Seleucid Empire gradually disintegrated, although a rump survived until 64 BC, whilst the Ptolemaic Kingdom continued in Egypt until 30 BC when it too was conquered by the Romans. The Aetolian league grew wary of Roman involvement in Greece, and sided with the Seleucids in the Roman–Seleucid War; when the Romans were victorious, the league was effectively absorbed into the Republic. Although the Achaean league outlasted both the Aetolian league and Macedon, it was also soon defeated and absorbed by the Romans in 146 BC, bringing Greek independence to an end.\n\n\n=== Roman Greece ===\n\nThe Greek peninsula came under Roman rule during the 146 BC conquest of Greece after the Battle of Corinth. Macedonia became a Roman province while southern Greece came under the surveillance of Macedonia's prefect; however, some Greek poleis managed to maintain a partial independence and avoid taxation. The Aegean Islands were added to this territory in 133 BC. Athens and other Greek cities revolted in 88 BC, and the peninsula was crushed by the Roman general Sulla. The Roman civil wars devastated the land even further, until Augustus organized the peninsula as the province of Achaea in 27 BC.\nGreece was a key eastern province of the Roman Empire, as the Roman culture had long been in fact Greco-Roman. The Greek language served as a lingua franca in the East and in Italy, and many Greek intellectuals such as Galen would perform most of their work in Rome.\n\n\n== Geography ==\n\n\n=== Regions ===\n\nThe territory of Greece is mountainous, and as a result, ancient Greece consisted of many smaller regions, each with its own dialect, cultural peculiarities, and identity. Regionalism and regional conflicts were prominent features of ancient Greece. Cities tended to be located in valleys between mountains, or on coastal plains, and dominated a certain area around them.\nIn the south lay the Peloponnese, consisting of the regions of Laconia (southeast), Messenia (southwest), Elis (west), Achaia (north), Korinthia (northeast), Argolis (east), and Arcadia (center). These names survive to the present day as regional units of modern Greece, though with somewhat different boundaries. Mainland Greece to the north, nowadays known as Central Greece, consisted of Aetolia and Acarnania in the west, Locris, Doris, and Phocis in the center, while in the east lay Boeotia, Attica, and Megaris. Northeast lay Thessaly, while Epirus lay to the northwest. Epirus stretched from the Ambracian Gulf in the south to the Ceraunian Mountains and the Aoos river in the north, and consisted of Chaonia (north), Molossia (center), and Thesprotia (south). In the northeast corner was Macedonia, originally consisting Lower Macedonia and its regions, such as Elimeia, Pieria, and Orestis. Around the time of Alexander I of Macedon, the Argead kings of Macedon started to expand into Upper Macedonia, lands inhabited by independent Macedonian tribes like the Lyncestae, Orestae and the Elimiotae and to the west, beyond the Axius river, into Eordaia, Bottiaea, Mygdonia, and Almopia, regions settled by Thracian tribes. To the north of Macedonia lay various non-Greek peoples such as the Paeonians due north, the Thracians to the northeast, and the Illyrians, with whom the Macedonians were frequently in conflict, to the northwest. Chalcidice was settled early on by southern Greek colonists and was considered part of the Greek world, while from the late 2nd millennium BC substantial Greek settlement also occurred on the eastern shores of the Aegean, in Anatolia.\n\n\n=== Colonies ===\n\nDuring the Archaic period, the Greek population grew beyond the capacity of the limited arable land of Greece proper, resulting in the large-scale establishment of colonies elsewhere: according to one estimate, the population of the widening area of Greek settlement increased roughly ten-fold from 800 to 400 BC, from 800,000 to as many as 7+1⁄2–10 million. This was not simply for trade, but also to found settlements. These Greek colonies were not, as Roman colonies were, dependent on their mother-city, but were independent city-states in their own right.\nGreeks settled outside of Greece in two distinct ways. The first was in permanent settlements founded by Greeks, which formed as independent poleis. The second form was in what historians refer to as emporia; trading posts which were occupied by both Greeks and non-Greeks and which were primarily concerned with the manufacture and sale of goods. Examples of this latter type of settlement are found at Al Mina in the east and Pithekoussai in the west. From around 750 to 500 BC, Greeks settled colonies in all directions. To the east, the Aegean coast of Asia Minor was colonized first, followed by Cyprus and the coasts of Thrace, the Sea of Marmara and south coast of the Black Sea.\nEventually, Greek colonization reached as far northeast as present-day Ukraine and Russia (Taganrog). To the west the coasts of Illyria, Southern Italy (called \"Magna Graecia\") were settled, followed by Southern France, Corsica, and even eastern Spain. Greek colonies were also founded in Egypt and Libya. Modern Syracuse, Naples, Marseille and Istanbul had their beginnings as the Greek colonies Syracusae (Συράκουσαι), Neapolis (Νεάπολις), Massalia (Μασσαλία) and Byzantion (Βυζάντιον). These colonies played an important role in the spread of Greek influence throughout Europe and also aided in the establishment of long-distance trading networks between the Greek city-states, boosting the economy of ancient Greece.\n\n\n== Politics and society ==\n\n\n=== Political structure ===\n\nAncient Greece consisted of several hundred relatively independent city-states (poleis). This was a situation unlike that in most other contemporary societies, which were either tribal or kingdoms ruling over relatively large territories. Undoubtedly, the geography of Greece—divided and sub-divided by hills, mountains, and rivers—contributed to the fragmentary nature of ancient Greece. On the one hand, the ancient Greeks had no doubt that they were \"one people\"; they had the same religion, same basic culture, and same language. Furthermore, the Greeks were very aware of their tribal origins; Herodotus was able to extensively categorise the city-states by tribe. Yet, although these higher-level relationships existed, they seem to have rarely had a major role in Greek politics. The independence of the poleis was fiercely defended; unification was something rarely contemplated by the ancient Greeks. Even when, during the second Persian invasion of Greece, a group of city-states allied themselves to defend Greece, the vast majority of poleis remained neutral, and after the Persian defeat, the allies quickly returned to infighting.\nThus, the major peculiarities of the ancient Greek political system were its fragmented nature (and that this does not particularly seem to have tribal origin), and the particular focus on urban centers within otherwise tiny states. The peculiarities of the Greek system are further evidenced by the colonies that they set up throughout the Mediterranean, which, though they might count a certain Greek polis as their 'mother' (and remain sympathetic to her), were completely independent of the founding city.\nInevitably smaller poleis might be dominated by larger neighbors, but conquest or direct rule by another city-state appears to have been quite rare. Instead the poleis grouped themselves into leagues, membership of which was in a constant state of flux. Later in the Classical period, the leagues would become fewer and larger, be dominated by one city (particularly Athens, Sparta and Thebes); and often poleis would be compelled to join under threat of war (or as part of a peace treaty). Even after Philip II of Macedon conquered the heartlands of ancient Greece, he did not attempt to annex the territory or unify it into a new province, but compelled most of the poleis to join his own Corinthian League.\n\n\n=== Government and law ===\n\nInitially many Greek city-states seem to have been petty kingdoms; there was often a city official carrying some residual, ceremonial functions of the king (basileus), e.g., the archon basileus in Athens. However, by the Archaic period and the first historical consciousness, most had already become aristocratic oligarchies. It is unclear exactly how this change occurred. For instance, in Athens, the kingship had been reduced to a hereditary, lifelong chief magistracy (archon) by c. 1050 BC; by 753 BC this had become a decennial, elected archonship, and finally an annually elected archonship by 683 BC. Through each stage, more power would have been transferred to the aristocracy as a whole, and away from a single individual.\nInevitably, the domination of politics and concomitant aggregation of wealth by small groups of families was apt to cause social unrest in many poleis. In many cities a tyrant (not in the modern sense of repressive autocracies), would at some point seize control and govern according to their own will; often a populist agenda would help sustain them in power. In a system wracked with class conflict, government by a 'strongman' was often the best solution.\nAthens fell under a tyranny in the second half of the 6th century BC. When this tyranny was ended, the Athenians founded the world's first democracy as a radical solution to prevent the aristocracy regaining power. A citizens' assembly (the Ecclesia), for the discussion of city policy, had existed since the reforms of Draco in 621 BC; all citizens were permitted to attend after the reforms of Solon (early 6th century), but the poorest citizens could not address the assembly or run for office. With the establishment of the democracy, the assembly became the de jure mechanism of government; all citizens had equal privileges in the assembly. However, non-citizens, such as metics (foreigners living in Athens) or slaves, had no political rights at all.\nAfter the rise of democracy in Athens, other city-states founded democracies. However, many retained more traditional forms of government. As so often in other matters, Sparta was a notable exception to the rest of Greece, ruled through the whole period by not one, but two hereditary monarchs. This was a form of diarchy. The Kings of Sparta belonged to the Agiads and the Eurypontids, descendants respectively of Eurysthenes and Procles. Both dynasties' founders were believed to be twin sons of Aristodemus, a Heraclid ruler. However, the powers of these kings were held in check by both a council of elders (the Gerousia) and magistrates specifically appointed to watch over the kings (the Ephors).\n\n\n=== Social structure ===\nOnly free, land-owning, native-born men could be citizens entitled to the full protection of the law in a city-state. In most city-states, unlike the situation in Rome, social prominence did not allow special rights. Sometimes families controlled public religious functions, but this ordinarily did not give any extra power in the government. In Athens, the population was divided into four social classes based on wealth. People could change classes if they made more money. In Sparta, all male citizens were called homoioi, meaning \"peers\". However, Spartan kings, who served as the city-state's dual military and religious leaders, came from two families.\nWomen in Ancient Greece appear to have primarily performed domestic tasks, managed households, and borne and reared children.\n\n\n==== Slavery ====\n\nSlaves had no power or status. Slaves had the right to have a family and own property, subject to their master's goodwill and permission, but they had no political rights. By 600 BC, chattel slavery had spread in Greece. By the 5th century BC, slaves made up one-third of the total population in some city-states. Between 40 and 80% of the population of Classical Athens were slaves. Slaves outside of Sparta almost never revolted because they were made up of too many nationalities and were too scattered to organize. However, unlike later Western culture, the ancient Greeks did not think in terms of race.\nMost families owned slaves as household servants and laborers, and even poor families might have owned a few slaves. Owners were not allowed to beat or kill their slaves. Owners often promised to free slaves in the future to encourage slaves to work hard. Unlike in Rome, freedmen did not become citizens. Instead, they were mixed into the population of metics, which included people from foreign countries or other city-states who were officially allowed to live in the state.\nCity-states legally owned slaves. These public slaves had a larger measure of independence than slaves owned by families, living on their own and performing specialized tasks. In Athens, public slaves were trained to look out for counterfeit coinage, while temple slaves acted as servants of the temple's deity and Scythian slaves were employed in Athens as a police force corralling citizens to political functions.\nSparta had a special type of slaves called helots. Helots were Messenians enslaved en masse during the Messenian Wars by the state and assigned to families where they were forced to stay. Helots raised food and did household chores so that women could concentrate on raising strong children while men could devote their time to training as hoplites. Their masters treated them harshly, and helots revolted against their masters several times. In 370/369 BC, as a result of Epaminondas' liberation of Messenia from Spartan rule, the helot system there came to an end and the helots won their freedom. However, it persisted in Laconia until the 2nd century BC.\n\n\n=== Education ===\n\nFor most of Greek history, education was private, except in Sparta. During the Hellenistic period, some city-states established public schools. Only wealthy families could afford a teacher. Boys learned how to read, write and quote literature. They also learned to sing and play one musical instrument and were trained as athletes for military service. They studied not for a job but to become an effective citizen. Girls also learned to read, write and do simple arithmetic so they could manage the household. They almost never received education after childhood.\nBoys went to school at the age of seven, or went to the barracks, if they lived in Sparta. The three types of teachings were: grammatistes for arithmetic, kitharistes for music and dancing, and Paedotribae for sports.\nBoys from wealthy families attending the private school lessons were taken care of by a paidagogos, a household slave selected for this task who accompanied the boy during the day. Classes were held in teachers' private houses and included reading, writing, mathematics, singing, and playing the lyre and flute. When the boy became 12 years old the schooling started to include sports such as wrestling, running, and throwing discus and javelin. In Athens, some older youths attended academy for the finer disciplines such as culture, sciences, music, and the arts. The schooling ended at age 18, followed by military training in the army usually for one or two years.\nSome of Athens' greatest such schools included the Lyceum (the so-called Peripatetic school founded by Aristotle of Stageira) and the Platonic Academy (founded by Plato of Athens). The education system of the wealthy ancient Greeks is also called Paideia.\n\n\n=== Economy ===\n\nAt its economic height in the 5th and 4th centuries BC, the free citizenry of Classical Greece represented perhaps the most prosperous society in the ancient world, some economic historians considering Greece one of the most advanced pre-industrial economies. In terms of wheat, wages reached an estimated 7–12 kg (15–26 lb) daily for an unskilled worker in urban Athens, 2–3 times the 3.75 kg (8.3 lb) of an unskilled rural labourer in Roman Egypt, though Greek farm incomes too were on average lower than those available to urban workers.\nWhile slave conditions varied widely, the institution served to sustain the incomes of the free citizenry: an estimate of economic development drawn from the latter (or derived from urban incomes alone) is therefore likely to overstate the true overall level despite widespread evidence for high living standards.\n\n\n=== Warfare ===\n\nAt least in the Archaic period, the fragmentary nature of ancient Greece, with many competing city-states, increased the frequency of conflict but conversely limited the scale of warfare. Unable to maintain professional armies, the city-states relied on their own citizens to fight. This inevitably reduced the potential duration of campaigns, as citizens would need to return to their own professions (especially in the case of, for example, farmers). Campaigns would therefore often be restricted to summer. When battles occurred, they were usually set piece and intended to be decisive. Casualties were slight compared to later battles, rarely amounting to more than five percent of the losing side, but the slain often included the most prominent citizens and generals who led from the front.\nThe scale and scope of warfare in ancient Greece changed dramatically as a result of the Greco-Persian Wars. To fight the enormous armies of the Achaemenid Empire was effectively beyond the capabilities of a single city-state. The eventual triumph of the Greeks was achieved by alliances of city-states (the exact composition changing over time), allowing the pooling of resources and division of labor. Although alliances between city-states occurred before this time, nothing on this scale had been seen before. The rise of Athens and Sparta as pre-eminent powers during this conflict led directly to the Peloponnesian War, which saw further development of the nature of warfare, strategy and tactics. Fought between leagues of cities dominated by Athens and Sparta, the increased manpower and financial resources increased the scale and allowed the diversification of warfare. Set-piece battles during the Peloponnesian war proved indecisive and instead there was increased reliance on attritionary strategies, naval battles and blockades and sieges. These changes greatly increased the number of casualties and the disruption of Greek society.\nAthens owned one of the largest war fleets in ancient Greece. It had over 200 triremes each powered by 170 oarsmen who were seated in 3 rows on each side of the ship. The city could afford such a large fleet—it had over 34,000 oarsmen—because it owned a lot of silver mines that were worked by slaves.\nAccording to Josiah Ober, Greek city-states faced approximately a one-in-three chance of destruction during the archaic and classical period.\n\n\n== Culture ==\n\n\n=== Philosophy ===\n\nAncient Greek philosophy focused on the role of reason and inquiry. In many ways, it had an important influence on modern philosophy, as well as modern science. Clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and Islamic scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day.\nNeither reason nor inquiry began with the ancient Greeks. Defining the difference between the Greek quest for knowledge and the quests of the elder civilizations, such as the ancient Egyptians and Babylonians, has long been a topic of study by theorists of civilization.\nThe first known philosophers of Greece were the pre-Socratics, who attempted to provide naturalistic, non-mythical descriptions of the world. They were followed by Socrates, one of the first philosophers based in Athens during its golden age whose ideas, despite being known by second-hand accounts instead of writings of his own, laid the basis of Western philosophy. Socrates' disciple Plato, who wrote The Republic and established a radical difference between ideas and the concrete world, and Plato's disciple Aristotle, who wrote extensively about nature and ethics, are also immensely influential in Western philosophy to this day. The later Hellenistic philosophy, also originating in Greece, is defined by names such as Antisthenes (cynicism), Zeno of Citium (stoicism) and Plotinus (Neoplatonism).\n\n\n=== Literature and theatre ===\n\nThe earliest Greek literature was poetry and was composed for performance rather than private consumption. The earliest Greek poet known is Homer, although he was certainly part of an existing tradition of oral poetry. Homer's poetry, though it was developed around the same time that the Greeks developed writing, would have been composed orally; the first poet to certainly compose their work in writing was Archilochus, a lyric poet from the mid-7th century BC. Tragedy developed around the end of the archaic period, taking elements from across the pre-existing genres of late archaic poetry. Towards the beginning of the classical period, comedy began to develop—the earliest date associated with the genre is 486 BC, when a competition for comedy became an official event at the City Dionysia in Athens, though the first preserved ancient comedy is Aristophanes' Acharnians, produced in 425.\n\nLike poetry, Greek prose had its origins in the archaic period, and the earliest writers of Greek philosophy, history, and medical literature all date to the 6th century BC. Prose first emerged as the writing style adopted by the presocratic philosophers Anaximander and Anaximenes—though Thales of Miletus, considered the first Greek philosopher, apparently wrote nothing. Prose as a genre reached maturity in the classical era, and the major Greek prose genres—philosophy, history, rhetoric, and dialogue—developed in this period.\nThe Hellenistic period saw the literary centre of the Greek world move from Athens, where it had been in the classical period, to Alexandria. At the same time, other Hellenistic kings such as the Antigonids and the Attalids were patrons of scholarship and literature, turning Pella and Pergamon respectively into cultural centres. It was thanks to this cultural patronage by Hellenistic kings, and especially the Museum at Alexandria, that so much ancient Greek literature has survived. The Library of Alexandria, part of the Museum, had the previously unenvisaged aim of collecting together copies of all known authors in Greek. Almost all of the surviving non-technical Hellenistic literature is poetry, and Hellenistic poetry tended to be highly intellectual, blending different genres and traditions, and avoiding linear narratives. The Hellenistic period also saw a shift in the ways literature was consumed—while in the archaic and classical periods literature had typically been experienced in public performance, in the Hellenistic period it was more commonly read privately. At the same time, Hellenistic poets began to write for private, rather than public, consumption.\nWith Octavian's victory at Actium in 31 BC, Rome began to become a major centre of Greek literature, as important Greek authors such as Strabo and Dionysius of Halicarnassus came to Rome. The period of greatest innovation in Greek literature under Rome was the \"long second century\" from approximately 80 AD to around 230 AD. This innovation was especially marked in prose, with the development of the novel and a revival of prominence for display oratory both dating to this period.\n\n\n=== Music and dance ===\n\nIn Ancient Greek society, music was ever-present and considered a fundamental component of civilisation. It was an important part of public religious worship, private ceremonies such as weddings and funerals, and household entertainment. Men sang and played music at the symposium; both men and women sang at work; and children's games involved song and dance.\nAncient Greek music was primarily vocal, sung either by a solo singer or a chorus, and usually accompanied by an instrument; purely instrumental music was less common. The Greeks used stringed instruments, including lyres, harps, and lutes; and wind instruments, of which the most important was the aulos, a reed instrument. Percussion instruments played a relatively unimportant role supporting stringed and wind instruments, and were used in certain religious cults.\n\n\n=== Science and technology ===\n\nAncient Greek mathematics contributed many important developments to mathematics, including the basic rules of geometry, the idea of formal mathematical proof, and discoveries in number theory, mathematical analysis, applied mathematics, and approached close to establishing integral calculus. The discoveries of several Greek mathematicians, including Pythagoras, Euclid, and Archimedes, are still used in mathematical teaching today.\nThe Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. In the 3rd century BC, Aristarchus of Samos was the first to suggest a heliocentric system. Archimedes in his treatise The Sand Reckoner revives Aristarchus' hypothesis that \"the fixed stars and the Sun remain unmoved, while the Earth revolves about the Sun on the circumference of a circle\". Otherwise, only fragmentary descriptions of Aristarchus' idea survive. Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy. In the 2nd century BC Hipparchus of Nicea made a number of contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed the modern system of apparent magnitudes.\nThe Antikythera mechanism, a device for calculating the movements of planets, dates from about 80 BC and was the first ancestor of the astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.\nThe ancient Greeks also made important discoveries in the medical field. Hippocrates was a physician of the Classical period, and is considered one of the most outstanding figures in the history of medicine. He is referred to as the \"father of medicine\" in recognition of his lasting contributions to the field as the founder of the Hippocratic school of medicine. This intellectual school revolutionized medicine in ancient Greece, establishing it as a discipline distinct from other fields that it had traditionally been associated with (notably theurgy and philosophy), thus making medicine a profession.\n\n\n=== Art and architecture ===\n\nThe art of ancient Greece has exercised an enormous influence on the culture of many countries from ancient times to the present day, particularly in the areas of sculpture and architecture. In the West, the art of the Roman Empire was largely derived from Greek models. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, with ramifications as far as Japan. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece dominated the art of the Western world.\n\n\n=== Religion ===\n\nReligion was a central part of ancient Greek life. Though the Greeks of different cities and tribes worshipped similar gods, religious practices were not uniform and the gods were thought of differently in different places. The Greeks were polytheistic, worshipping many gods, but as early as the 6th century BC a pantheon of twelve Olympians began to develop. Greek religion was influenced by the practices of the Greeks' near eastern neighbours at least as early as the archaic period, and by the Hellenistic period this influence was seen in both directions.\nThe most important religious act in ancient Greece was animal sacrifice, most commonly of sheep and goats. Sacrifice was accompanied by public prayer, and prayer and hymns were themselves a major part of ancient Greek religious life.\n\n\n== Legacy ==\n\nThe civilization of ancient Greece has been immensely influential on language, politics, educational systems, philosophy, science, and the arts. It became the Leitkultur of the Roman Empire to the point of marginalizing native Italic traditions. As Horace put it,\n\nGraecia capta ferum victorem cepit et artis / intulit agresti Latio (Epistulae 2.1.156f.)Captive Greece took captive her uncivilised conqueror and instilled her arts in rustic Latium.\nVia the Roman Empire, Greek culture came to be foundational to Western culture in general.\nThe Byzantine Empire inherited Classical Greek-Hellenistic culture directly, without Latin intermediation, and the preservation of Classical Greek learning in medieval Byzantine tradition further exerted a strong influence on the Slavs and later on the Islamic Golden Age and the Western European Renaissance. A modern revival of Classical Greek learning took place in the Neoclassicism movement in 18th- and 19th-century Europe and the Americas.\n\n\n== See also ==\n\nList of ancient Greek writers\nClassical demography\nRegions in Greco-Roman antiquity\nScience in classical antiquity\nModern influence of Ancient Greece\nList of archaeologically attested women from the ancient Mediterranean region\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Notes ===\n\n\n=== Bibliography ===\n\n\n== Further reading ==\n\n\n== External links ==\n\nThe Canadian Museum of Civilization—Greece Secrets of the Past\nAncient Greece website from the British Museum\nEconomic history of ancient Greece (archived 2 May 2006)\nThe Greek currency history\nLimenoscope, an ancient Greek ports database (archived 11 May 2011)\nThe Ancient Theatre Archive, Greek and Roman theatre architecture\nIllustrated Greek History, Janice Siegel, Department of Classics, Hampden–Sydney College, Virginia"
    },
    {
        "id": "impressionism",
        "name": "Impressionism",
        "text": "Impressionism was a 19th-century art movement characterized by visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, unusual visual angles, and inclusion of movement as a crucial element of human perception and experience. Impressionism originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.\nThe Impressionists faced harsh opposition from the conventional art community in France. The name of the style derives from the title of a Claude Monet work, Impression, soleil levant (Impression, Sunrise), which provoked the critic Louis Leroy to coin the term in a satirical 1874 review of the First Impressionist Exhibition published in the Parisian newspaper Le Charivari. The development of Impressionism in the visual arts was soon followed by analogous styles in other media that became known as Impressionist music and Impressionist literature.\n\n\n== Overview ==\n\nRadicals in their time, the early Impressionists violated the rules of academic painting. They constructed their pictures from freely brushed colours that took precedence over lines and contours, following the example of painters such as Eugène Delacroix and J. M. W. Turner. They also painted realistic scenes of everyday life in natural settings, often outdoors, attempting to capture a moment as experienced. \nPreviously, paintings were accomplished in studio, whether landscape art, still life or portrait, with an emphasis on verisimilitude. The Impressionists found that they could capture the momentary and transient effects of sunlight by painting outdoors or en plein air. They portrayed overall visual effects instead of details, and used short \"broken\" brush strokes of mixed and pure unmixed colour—not blended smoothly or shaded, as was customary—to achieve an effect of intense colour vibration.\n\nImpressionism emerged in France at the same time that a number of other painters, including the Italian artists known as the Macchiaioli, and Winslow Homer in the United States, were also exploring plein-air painting. The Impressionists, however, developed new techniques specific to the style. Encompassing what its adherents argued was a different way of seeing, it is an art of immediacy and movement, of candid poses and compositions, of the play of light expressed in a bright and varied use of colour. In 1876, the poet and critic Stéphane Mallarmé said of the new style: \"The represented subject, being composed of a harmony of reflected and ever-changing lights, cannot be supposed always to look the same but palpitates with movement, light, and life\".\nThe public, at first hostile, gradually came to believe that the Impressionists had captured a fresh and original vision, even if the art critics and art establishment disapproved of the new style. By recreating the sensation in the eye that views the subject, rather than delineating the details of the subject, and by creating a welter of techniques and forms, Impressionism is a precursor of various painting styles, including Post-Impressionism, Fauvism, and Cubism.\n\n\n== Beginnings ==\n\nIn the middle of the 19th century—a time of rapid industrialization and unsettling social change in France, as Emperor Napoleon III rebuilt Paris and waged war—the Académie des Beaux-Arts dominated French art. The Académie was the preserver of traditional French painting standards of content and style. Historical subjects, religious themes, and portraits were valued; landscape and still life were not. The Académie preferred carefully finished images that looked realistic when examined closely. Paintings in this style were made up of precise brush strokes carefully blended to hide the artist's hand in the work. Colour was restrained and often toned down further by the application of a thick golden varnish.\nThe Académie had an annual, juried art show, the Salon de Paris, and artists whose work was displayed in the show won prizes, garnered commissions, and enhanced their prestige. The standards of the juries represented the values of the Académie, represented by the works of such artists as Jean-Léon Gérôme and Alexandre Cabanel. Using an eclectic mix of techniques and formulas established in Western painting since the Renaissance—such as linear perspective and figure types derived from Classical Greek art—these artists produced escapist visions of a reassuringly ordered world. By the 1850s, some artists, notably the Realist painter Gustave Courbet, had gained public attention and critical censure by depicting contemporary realities without the idealization demanded by the Académie. \nIn the early 1860s, four young painters—Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, and Frédéric Bazille—met while studying under the academic artist Charles Gleyre. They discovered that they shared an interest in painting landscape and contemporary life rather than historical or mythological scenes. Following a practice—pioneered by artists such as the Englishman John Constable— that had become increasingly popular by mid-century, they often ventured into the countryside together to paint in the open air. Their purpose was not to make sketches to be developed into carefully finished works in the studio, as was the usual custom, but to complete their paintings out-of-doors. \nBy painting in sunlight directly from nature, and making bold use of the vivid synthetic pigments that had become available since the beginning of the century, they began to develop a lighter and brighter manner of painting that extended further the Realism of Courbet and the Barbizon school. A favourite meeting place for the artists was the Café Guerbois on Avenue de Clichy in Paris, where the discussions were often led by Édouard Manet, whom the younger artists greatly admired. They were soon joined by Camille Pissarro, Paul Cézanne, and Armand Guillaumin.\n\nDuring the 1860s, the Salon jury routinely rejected about half of the works submitted by Monet and his friends in favour of works by artists faithful to the approved style. In 1863, the Salon jury rejected Manet's The Luncheon on the Grass (Le déjeuner sur l'herbe) primarily because it depicted a nude woman with two clothed men at a picnic. While the Salon jury routinely accepted nudes in historical and allegorical paintings, they condemned Manet for placing a realistic nude in a contemporary setting. The jury's severely worded rejection of Manet's painting appalled his admirers, and the unusually large number of rejected works that year perturbed many French artists.\nAfter Emperor Napoleon III saw the rejected works of 1863, he decreed that the public be allowed to judge the work themselves, and the Salon des Refusés (Salon of the Refused) was organized. While many viewers came only to laugh, the Salon des Refusés drew attention to the existence of a new tendency in art and attracted more visitors than the regular Salon.\n\nArtists' petitions requesting a new Salon des Refusés in 1867, and again in 1872, were denied. In December 1873, Monet, Renoir, Pissarro, Sisley, Cézanne, Berthe Morisot, Edgar Degas and several other artists founded the Société anonyme des artistes peintres, sculpteurs, graveurs, etc. to exhibit their artworks independently. Members of the association were expected to forswear participation in the Salon. The organizers invited a number of other progressive artists to join them in their inaugural exhibition, including the older Eugène Boudin, whose example had first persuaded Monet to adopt plein air painting years before. Another painter who greatly influenced Monet and his friends, Johan Jongkind, declined to participate, as did Édouard Manet. In total, thirty artists participated in their first exhibition, held in April 1874 at the studio of the photographer Nadar.\n\nThe critical response was mixed. Monet and Cézanne received the harshest attacks. Critic and humorist Louis Leroy wrote a scathing review in the newspaper Le Charivari in which, making wordplay with the title of Claude Monet's Impression, Sunrise (Impression, soleil levant), he gave the artists the name by which they became known. Derisively titling his article \"The Exhibition of the Impressionists\", Leroy declared that Monet's painting was at most, a sketch, and could hardly be termed a finished work.\nHe wrote, in the form of a dialogue between viewers,\n\n\"Impression—I was certain of it. I was just telling myself that, since I was impressed, there had to be some impression in it ... and what freedom, what ease of workmanship! Wallpaper in its embryonic state is more finished than that seascape.\"\n\nThe term Impressionist quickly gained favour with the public. It was also accepted by the artists themselves, even though they were a diverse group in style and temperament, unified primarily by their spirit of independence and rebellion. They exhibited together—albeit with shifting membership—eight times between 1874 and 1886. The Impressionists' style, with its loose, spontaneous brushstrokes, would soon become synonymous with modern life.\nMonet, Sisley, Morisot, and Pissarro may be considered the \"purest\" Impressionists, in their consistent pursuit of an art of spontaneity, sunlight, and colour. Degas rejected much of this, as he believed in the primacy of drawing over colour and belittled the practice of painting outdoors. Renoir turned away from Impressionism for a time during the 1880s, and never entirely regained his commitment to its ideas. Édouard Manet, although regarded by the Impressionists as their leader, never abandoned his liberal use of black as a colour (while Impressionists avoided its use and preferred to obtain darker colours by mixing), and never participated in the Impressionist exhibitions. He continued to submit his works to the Salon, where his painting Spanish Singer had won a 2nd class medal in 1861, and he urged the others to do likewise, arguing that \"the Salon is the real field of battle\" where a reputation could be made.\n\nAmong the artists of the core group (minus Bazille, who had died in the Franco-Prussian War in 1870), defections occurred as Cézanne, followed later by Renoir, Sisley, and Monet, abstained from the group exhibitions so they could submit their works to the Salon. Disagreements arose from issues such as Guillaumin's membership in the group, championed by Pissarro and Cézanne against opposition from Monet and Degas, who thought him unworthy. Degas invited Mary Cassatt to display her work in the 1879 exhibition, but also insisted on the inclusion of Jean-François Raffaëlli, Ludovic Lepic, and other realists who did not represent Impressionist practices, causing Monet in 1880 to accuse the Impressionists of \"opening doors to first-come daubers\". \nIn this regard, the seventh Paris Impressionist exhibition in 1882 was the most selective of all including the works of only nine \"true\" impressionists, namely Gustave Caillebotte, Paul Gauguin, Armand Guillaumin, Claude Monet, Berthe Morisot, Camille Pissarro, Pierre-Auguste Renoir, Alfred Sisley, and Victor Vignon. The group then divided again over the invitations to Paul Signac and Georges Seurat to exhibit with them at the 8th Impressionist exhibition in 1886. Pissarro was the only artist to show at all eight Paris Impressionist exhibitions.\nThe individual artists achieved few financial rewards from the Impressionist exhibitions, but their art gradually won a degree of public acceptance and support. Their dealer, Durand-Ruel, played a major role in this as he kept their work before the public and arranged shows for them in London and New York. Although Sisley died in poverty in 1899, Renoir had a great Salon success in 1879. Monet became secure financially during the early 1880s and so did Pissarro by the early 1890s. By this time the methods of Impressionist painting, in a diluted form, had become commonplace in Salon art.\n\n\n== Impressionist techniques ==\n\nFrench painters who prepared the way for Impressionism include the Romantic colourist Eugène Delacroix; the leader of the realists, Gustave Courbet; and painters of the Barbizon school such as Théodore Rousseau. The Impressionists learned much from the work of Johan Barthold Jongkind, Jean-Baptiste-Camille Corot and Eugène Boudin, who painted from nature in a direct and spontaneous style that prefigured Impressionism, and who befriended and advised the younger artists.\nA number of identifiable techniques and working habits contributed to the innovative style of the Impressionists. Although these methods had been used by previous artists—and are often conspicuous in the work of artists such as Frans Hals, Diego Velázquez, Peter Paul Rubens, John Constable, and J. M. W. Turner—the Impressionists were the first to use them all together, and with such consistency. These techniques include:\n\nShort, thick strokes of paint quickly capture the essence of the subject, rather than its details. The paint is often applied impasto.\nColours are applied side by side with as little mixing as possible, a technique that exploits the principle of simultaneous contrast to make the colour appear more vivid to the viewer.\nGreys and dark tones are produced by mixing complementary colours. Pure impressionism avoids the use of black paint.\nWet paint is placed into wet paint without waiting for successive applications to dry, producing softer edges and intermingling of colour.\nImpressionist paintings do not exploit the transparency of thin paint films (glazes), which earlier artists manipulated carefully to produce effects. The impressionist painting surface is typically opaque.\nThe paint is applied to a white or light-coloured ground. Previously, painters often used dark grey or strongly coloured grounds.\nThe play of natural light is emphasized. Close attention is paid to the reflection of colours from object to object. Painters often worked in the evening to produce effets de soir—the shadowy effects of evening or twilight.\nIn paintings made en plein air (outdoors), shadows are boldly painted with the blue of the sky as it is reflected onto surfaces, giving a sense of freshness previously not represented in painting. Blue shadows on snow inspired the technique.\nNew technology played a role in the development of the style. Impressionists took advantage of the mid-century introduction of premixed paints in tin tubes (resembling modern toothpaste tubes), which allowed artists to work more spontaneously, both outdoors and indoors. Previously, painters made their own paints individually, by grinding and mixing dry pigment powders with linseed oil, which were then stored in animal bladders.\nMany vivid synthetic pigments became commercially available to artists for the first time during the 19th century. These included cobalt blue, viridian, cadmium yellow, and synthetic ultramarine blue, all of which were in use by the 1840s, before Impressionism. The Impressionists' manner of painting made bold use of these pigments, and of even newer colours such as cerulean blue, which became commercially available to artists in the 1860s.\nThe Impressionists' progress toward a brighter style of painting was gradual. During the 1860s, Monet and Renoir sometimes painted on canvases prepared with the traditional red-brown or grey ground. By the 1870s, Monet, Renoir, and Pissarro usually chose to paint on grounds of a lighter grey or beige colour, which functioned as a middle tone in the finished painting. By the 1880s, some of the Impressionists had come to prefer white or slightly off-white grounds, and no longer allowed the ground colour a significant role in the finished painting.\n\n\n== Content and composition ==\n\nThe Impressionists reacted to modernity by exploring \"a wide range of non-academic subjects in art\" such as middle-class leisure activities and \"urban themes, including train stations, cafés, brothels, the theater, and dance.\" They found inspiration in the newly widened avenues of Paris, bounded by new tall buildings that offered opportunities to depict bustling crowds, popular entertainments, and nocturnal lighting in artificially closed-off spaces. \nA painting such as Caillebotte's Paris Street; Rainy Day (1877) strikes a modern note by emphasizing the isolation of individuals amid the outsized buildings and spaces of the urban environment. When painting landscapes, the Impressionists did not hesitate to include the factories that were proliferating in the countryside. Earlier painters of landscapes had conventionally avoided smokestacks and other signs of industrialization, regarding them as blights on nature's order and unworthy of art.\nPrior to the Impressionists, other painters, notably such 17th-century Dutch painters as Jan Steen, had emphasized common subjects, but their methods of composition were traditional. They arranged their compositions so that the main subject commanded the viewer's attention. J. M. W. Turner, while an artist of the Romantic era, anticipated the style of impressionism with his artwork. The Impressionists relaxed the boundary between subject and background so that the effect of an Impressionist painting often resembles a snapshot, a part of a larger reality captured as if by chance. Photography was gaining popularity, and as cameras became more portable, photographs became more candid. Photography inspired Impressionists to represent momentary action, not only in the fleeting lights of a landscape, but in the day-to-day lives of people.\nThe development of Impressionism can be considered partly as a reaction by artists to the challenge presented by photography, which seemed to devalue the artist's skill in reproducing reality. Both portrait and landscape paintings were deemed somewhat deficient and lacking in truth as photography \"produced lifelike images much more efficiently and reliably\".\nIn spite of this, photography actually inspired artists to pursue other means of creative expression, and rather than compete with photography to emulate reality, artists focused \"on the one thing they could inevitably do better than the photograph—by further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated\". The Impressionists sought to express their perceptions of nature, rather than create exact representations. This allowed artists to depict subjectively what they saw with their \"tacit imperatives of taste and conscience\". Photography encouraged painters to exploit aspects of the painting medium, like colour, which photography then lacked: \"The Impressionists were the first to consciously offer a subjective alternative to the photograph\".\n\nAnother major influence was Japanese ukiyo-e art prints (Japonism). The art of these prints contributed significantly to the \"snapshot\" angles and unconventional compositions that became characteristic of Impressionism. An example is Monet's Jardin à Sainte-Adresse, 1867, with its bold blocks of colour and composition on a strong diagonal slant showing the influence of Japanese prints.\nEdgar Degas was both an avid photographer and a collector of Japanese prints. His The Dance Class (La classe de danse) of 1874 shows both influences in its asymmetrical composition. The dancers are seemingly caught off guard in various awkward poses, leaving an expanse of empty floor space in the lower right quadrant. He also captured his dancers in sculpture, such as the Little Dancer of Fourteen Years.\n\n\n== Female Impressionists ==\n\nImpressionists, in varying degrees, were looking for ways to depict visual experience and contemporary subjects. Female Impressionists were interested in these same ideals but had many social and career limitations compared to male Impressionists. They were particularly excluded from the imagery of the bourgeois social sphere of the boulevard, cafe, and dance hall. \nAs well as imagery, women were excluded from the formative discussions that resulted in meetings in those places. That was where male Impressionists were able to form and share ideas about Impressionism. In the academic realm, women were believed to be incapable of handling complex subjects, which led teachers to restrict what they taught female students. It was also considered unladylike to excel in art, since women's true talents were then believed to center on homemaking and mothering.\nYet several women were able to find success during their lifetime, even though their careers were affected by personal circumstances – Bracquemond, for example, had a husband who was resentful of her work which caused her to give up painting. The four most well known, namely, Mary Cassatt, Eva Gonzalès, Marie Bracquemond, and Berthe Morisot, are, and were, often referred to as the 'Women Impressionists'. Their participation in the series of eight Impressionist exhibitions that took place in Paris from 1874 to 1886 varied: Morisot participated in seven, Cassatt in four, Bracquemond in three, and Gonzalès did not participate.\n\nThe critics of the time lumped these four together without regard to their personal styles, techniques, or subject matter. Critics viewing their works at the exhibitions often attempted to acknowledge the women artists' talents but circumscribed them within a limited notion of femininity. Arguing for the suitability of Impressionist technique to women's manner of perception, Parisian critic S.C. de Soissons wrote:One can understand that women have no originality of thought, and that literature and music have no feminine character; but surely women know how to observe, and what they see is quite different from that which men see, and the art which they put in their gestures, in their toilet, in the decoration of their environment is sufficient to give is the idea of an instinctive, of a peculiar genius which resides in each one of them.\nWhile Impressionism legitimized the domestic social life as subject matter, of which women had intimate knowledge, it also tended to limit them to that subject matter. Portrayals of often-identifiable sitters in domestic settings, which could offer commissions, were dominant in the exhibitions. The subjects of the paintings were often women interacting with their environment by either their gaze or movement. Cassatt, in particular, was aware of her placement of subjects: she kept her predominantly female figures from objectification and cliche; when they are not reading, they converse, sew, drink tea, and when they are inactive, they seem lost in thought.\nThe women Impressionists, like their male counterparts, were striving for \"truth\", for new ways of seeing and new painting techniques; each artist had an individual painting style. Women Impressionists, particularly Morisot and Cassatt, were conscious of the balance of power between women and objects in their paintings – the bourgeois women depicted are not defined by decorative objects, but instead, interact with and dominate the things with which they live. There are many similarities in their depictions of women who seem both at ease and subtly confined. Gonzalès' Box at the Italian Opera depicts a woman staring into the distance, at ease in a social sphere but confined by the box and the man standing next to her. Cassatt's painting Young Girl at a Window is brighter in color but remains constrained by the canvas edge as she looks out the window.\n\nDespite their success in their ability to have a career and Impressionism's demise attributed to its allegedly feminine characteristics—its sensuality, dependence on sensation, physicality, and fluidity—the four women artists, and other, lesser-known women Impressionists, were largely omitted from art historical textbooks covering Impressionist artists until Tamar Garb's Women Impressionists published in 1986. For example, Impressionism by Jean Leymarie, published in 1955 included no information on any women Impressionists.\nPainter Androniqi Zengo Antoniu is co-credited with the introduction of impressionism to Albania.\n\n\n== Prominent Impressionists ==\nThe central figures in the development of Impressionism in France, listed alphabetically, were:\n\nFrédéric Bazille (1841–1870), who only posthumously participated in the Impressionist exhibitions\nGustave Caillebotte (1848–1894), who, younger than the others, joined forces with them in the mid-1870s\nMary Cassatt (1844–1926), American-born, she lived in Paris and participated in four Impressionist exhibitions\nPaul Cézanne (1839–1906), although he later broke away from the Impressionists\nEdgar Degas (1834–1917), who despised the term Impressionist\nArmand Guillaumin (1841–1927)\nÉdouard Manet (1832–1883), who did not participate in any of the Impressionist exhibitions\nClaude Monet (1840–1926), the most prolific of the Impressionists and the one who embodies their aesthetic most obviously\nBerthe Morisot (1841–1895) who participated in all Impressionist exhibitions except in 1879\nCamille Pissarro (1830–1903), who was the only artist to participate in all eight Impressionist exhibitions.\nPierre-Auguste Renoir (1841–1919), who participated in Impressionist exhibitions in 1874, 1876, 1877 and 1882\nAlfred Sisley (1839–1899)\n\n\n== Timeline: lives of the Impressionists ==\n\n\n== Gallery ==\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== Associates and influenced artists ==\n\nAmong the close associates of the Impressionists, Victor Vignon is the only artist outside the group of prominent names who participated to the most exclusive Seventh Paris Impressionist Exhibition in 1882, which was indeed a rejection to the previous less restricted exhibitions chiefly organized by Degas. Originally from the school of Corot, Vignon was a friend of Camille Pissarro, whose influence is evident in his impressionist style after the late 1870s, and a friend of post-impressionist Vincent van Gogh.\nThere were several other close associates of the Impressionists who adopted their methods to some degree. These include Jean-Louis Forain, who participated in Impressionist exhibitions in 1879, 1880, 1881 and 1886, and Giuseppe De Nittis, an Italian artist living in Paris who participated in the first Impressionist exhibit at the invitation of Degas, although the other Impressionists disparaged his work. Federico Zandomeneghi was another Italian friend of Degas who showed with the Impressionists. Eva Gonzalès was a follower of Manet who did not exhibit with the group. \nJames Abbott McNeill Whistler was an American-born painter who played a part in Impressionism although he did not join the group and preferred grayed colours. Walter Sickert, an English artist, was initially a follower of Whistler, and later an important disciple of Degas. He did not exhibit with the Impressionists. In 1904, the artist and writer Wynford Dewhurst wrote the first important study of the French painters published in English, Impressionist Painting: its genesis and development, which did much to popularize Impressionism in Great Britain.\nBy the early 1880s, Impressionist methods were affecting, at least superficially, the art of the Salon. Fashionable painters such as Jean Béraud and Henri Gervex found critical and financial success by brightening their palettes while retaining the smooth finish expected of Salon art. Works by these artists are sometimes casually referred to as Impressionism, despite their remoteness from Impressionist practice.\nThe influence of the French Impressionists lasted long after most of them had died. Artists like J.D. Kirszenbaum were borrowing Impressionist techniques throughout the twentieth century.\n\n\n== Beyond France ==\n\nAs the influence of Impressionism spread beyond France, artists, too numerous to list, became identified as practitioners of the new style. Some of the more important examples are:\n\nThe American Impressionists, including Mary Cassatt, William Merritt Chase, Frederick Carl Frieseke, Childe Hassam, Willard Metcalf, Lilla Cabot Perry, Theodore Robinson, Edmund Charles Tarbell, John Henry Twachtman, Catherine Wiley and J. Alden Weir.\nThe Australian Impressionists, including Tom Roberts, Arthur Streeton, Walter Withers, Charles Conder, Frederick McCubbin and E. Phillips Fox (who were prominent members of the Heidelberg School), and John Russell, a friend of Van Gogh, Rodin, Monet and Matisse.\nThe Amsterdam Impressionists in the Netherlands, including George Hendrik Breitner, Isaac Israëls, Willem Bastiaan Tholen, Willem de Zwart, Willem Witsen, Marie Henry Mackenzie and Jan Toorop.\nThe California Impressionists, including William Wendt, Guy Rose, Alson Clark, Donna N. Schuster, and Sam Hyde Harris.\nAnna Boch, Vincent van Gogh's friend Eugène Boch, Georges Lemmen and Théo van Rysselberghe, Impressionist painters from Belgium.\nThe Slovenian Impressionists, Ivan Grohar, Rihard Jakopič, Matija Jama, and Matej Sternen. Their beginning was in the school of Anton Ažbe in Munich and they were influenced by Jurij Šubic and Ivana Kobilca, Slovenian painters working in Paris.\nWynford Dewhurst, Walter Richard Sickert, and Philip Wilson Steer were well known Impressionist painters from the United Kingdom. Pierre Adolphe Valette, who was born in France but who worked in Manchester, was the tutor of L. S. Lowry.\nThe German Impressionists, including Max Liebermann, Lovis Corinth, Ernst Oppler, Max Slevogt and August von Brandis.\nLászló Mednyánszky and Pál Szinyei-Merse in Hungary\nTheodor von Ehrmanns and Hugo Charlemont who were rare Impressionists among the more dominant Vienna Secessionist painters in Austria.\nWilliam John Leech, Roderic O'Conor, and Walter Osborne in Ireland\nKonstantin Korovin and Valentin Serov in Russia\nFrancisco Oller y Cestero, a native of Puerto Rico and a friend of Pissarro and Cézanne\nJames Nairn in New Zealand\nWilliam McTaggart in Scotland\nMaurice Cullen, Laura Muntz Lyall and Helen McNicoll, Canadian artists\nWładysław Podkowiński, a Polish Impressionist and symbolist\nNicolae Grigorescu in Romania\nNazmi Ziya Güran, who brought Impressionism to Turkey\nChafik Charobim in Egypt\nEliseu Visconti in Brazil\nJoaquín Sorolla and Fermín Arango in Spain\nFaustino Brughetti, Fernando Fader, Candido Lopez, Martín Malharro, Walter de Navazio, Ramón Silva in Argentina\nSkagen Painters a group of Scandinavian artists who painted in a small Danish fishing village\nNadežda Petrović, Milo Milunović, Kosta Miličević, Milan Milovanovi and Mališa Glišić in Serbia\nÁsgrímur Jónsson in Iceland\nFujishima Takeji in Japan\nFrits Thaulow in Norway and later France\n\n\n== Impressionism in other media ==\n\n\n=== Sculpture ===\n\nWhile Edgar Degas was primarily known as a painter in his lifetime, he began to pursue the medium of sculpture later in his artistic career in the 1880s. He created as many as 150 sculptures during his lifetime. Degas preferred the medium of wax for his sculptures because it allowed him to make changes, start over, and further explore the modelling process. Only one of Degas's sculptures, Little Dancer of Fourteen Years, was exhibited in his lifetime, which was exhibited at the Sixth Impressionist Exhibition in 1881. Little Dancer proved to be controversial with critics. Some considered Degas to have overthrown sculptural traditions in the same way that Impressionism had overthrown the traditions of painting. Others found it to be ugly. Following the Degas's death in 1917, his heirs authorized bronze castings from 73 of the artist's sculptures.\nThe sculptor Auguste Rodin is sometimes called an Impressionist for the way he used roughly modeled surfaces to suggest transient light effects. The sculptor Medardo Rosso has also been called an Impressionist.\nSome Russian artists created Impressionistic sculptures of animals in order to break away from old world concepts. Their works have been described as endowing birds and beasts with new spiritual characteristics.\n\n\n=== Photography and film ===\nWhile his photographs are less known than his paintings or his sculptures, Edgar Degas also pursued photography later in his life. His photographs were never exhibited during his lifetime, and not much attention was given to them following his death. It was not until the late 20th century that scholars started to take interest in Degas's photographs.\nPictorialist photographers, whose work is characterized by soft focus and atmospheric effects, have also been called Impressionists. These Impressionist photographers used various techniques such as photographing subjects out of focus, using soft focus lenses or pinhole lenses, and manipulating the gum bichromate process to create images that resembled Impressionist paintings.\nFrench Impressionist Cinema is a term applied to a loosely defined group of films and filmmakers in France from 1919 to 1929, although these years are debatable. French Impressionist filmmakers include Abel Gance, Jean Epstein, Germaine Dulac, Marcel L'Herbier, Louis Delluc, and Dmitry Kirsanoff.\n\n\n=== Music ===\n\nMusical Impressionism is the name given to a movement in European classical music that arose in the late 19th century and continued into the middle of the 20th century. Originating in France, musical Impressionism is characterized by suggestion and atmosphere, and eschews the emotional excesses of the Romantic era. Impressionist composers favoured short forms such as the nocturne, arabesque, and prelude, and often explored uncommon scales such as the whole tone scale. Perhaps the most notable innovations of Impressionist composers were the introduction of major 7th chords and the extension of chord structures in 3rds to five- and six-part harmonies.\nThe influence of visual Impressionism on its musical counterpart is debatable. Claude Debussy and Maurice Ravel are generally considered the greatest Impressionist composers, but Debussy disavowed the term, calling it the invention of critics. Erik Satie was also considered in this category, though his approach was regarded as less serious, more musical novelty in nature. \nPaul Dukas is another French composer sometimes considered an Impressionist, but his style is perhaps more closely aligned to the late Romanticists. Musical Impressionism beyond France includes the work of such composers as Ottorino Respighi (Italy), Ralph Vaughan Williams, Cyril Scott, and John Ireland (England), Manuel De Falla and Isaac Albeniz (Spain), and Charles Griffes (America).\nAmerican Impressionist music differs from European Impressionist music, and these differences are mainly reflected in Charles Griffith's poetry of flute and orchestral music. He is also the most prolific Impressionist composer in the United States.\n\n\n=== Literature ===\n\nThe term Impressionism has also been used to describe works of literature in which a few select details suffice to convey the sensory impressions of an incident or scene. Impressionist literature is closely related to Symbolism, with its major exemplars being Baudelaire, Mallarmé, Rimbaud, and Verlaine. Authors such as Virginia Woolf, D.H. Lawrence, Henry James, and Joseph Conrad have written works that are Impressionistic in the way that they describe, rather than interpret, the impressions, sensations and emotions that constitute a character's mental life. Some literary scholars, such as John G. Peters, believe literary Impressionism is better defined by its philosophical stance than by any supposed relationship with Impressionist painting.\n\n\n== Post-Impressionism ==\n\nDuring the 1880s several artists began to develop different precepts for the use of colour, pattern, form, and line, derived from the Impressionist example: Vincent van Gogh, Paul Gauguin, Georges Seurat, and Henri de Toulouse-Lautrec. These artists were slightly younger than the Impressionists, and their work is known as post-Impressionism. Post-Impressionist artists reacted against the Impressionists' concern with realistically reproducing the optical sensations of light and colour; they turned instead toward symbolic content and the expression of emotion. \nPost-Impressionism prefigured the characteristics of Futurism and Cubism, reflecting the change of attitude towards art in European society. Some of the original Impressionist artists also ventured into this new territory; Camille Pissarro briefly painted in a pointillist manner, and even Monet abandoned strict plein air painting. Paul Cézanne, who participated in the first and third Impressionist exhibitions, developed a highly individual vision emphasising pictorial structure, and he is more often called a post-Impressionist. Although these cases illustrate the difficulty of assigning labels, the work of the original Impressionist painters may, by definition, be categorised as Impressionism.\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n== See also ==\nExpressionism, a reaction to Impressionism\nLuminism (Impressionism)\nCantonese school of painting, influenced by Impressionism\nLes XX\nGeneral:\n\nArt periods\nHistory of Painting\nWestern Painting\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Citations ===\n\n\n=== Works cited ===\n\n\n== External links ==\n\nContemporary Impressionism in Photography\nHecht Museum\n\n The French Impressionists (1860–1900) at Project Gutenberg\nMuseumsportal Schleswig-Holstein\nImpressionism : A Centenary Exhibition, the Metropolitan Museum of Art, December 12, 1974 – February 10, 1975, fully digitized text from The Metropolitan Museum of Art libraries\nSuburban Pastoral The Guardian, 24 February 2007\nImpressionism: Paintings collected by European Museums (1999) was an art exhibition co-organized by the High Museum of Art, Atlanta, the Seattle Art Museum, and the Denver Art Museum, touring from May through December 1999. Online guided tour\nMonet's Years at Giverny: Beyond Impressionism, 1978 exhibition catalogue fully online as PDF from The Metropolitan Museum of Art, which discusses Monet's role in this movement\nDegas: The Artist's Mind, 1976 exhibition catalogue fully online as PDF from The Metropolitan Museum of Art, which discusses Degas's role in this movement\nDefinition of impressionism on the Tate Art Glossary\nParis 1874 Inventing impressionism Exhibition at the Musée d’Orsay, from March 26th to July 14th, 2024.\nParis 1874: The Impressionist Moment Exhibition at the National Gallery of Art from September 8, 2024 to January 19, 2025."
    },
    {
        "id": "shakespeare",
        "name": "William Shakespeare",
        "text": "William Shakespeare (c. 23 April 1564 – 23 April 1616) was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the \"Bard of Avon\" (or simply \"the Bard\"). His extant works, including collaborations, consist of some 39 plays, 154 sonnets, three long narrative poems and a few other verses, some of uncertain authorship. His plays have been translated into every major living language and are performed more often than those of any other playwright. Shakespeare remains arguably the most influential writer in the English language, and his works continue to be studied and reinterpreted.\nShakespeare was born and raised in Stratford-upon-Avon, Warwickshire. At the age of 18, he married Anne Hathaway, with whom he had three children: Susanna, and twins Hamnet and Judith. Sometime between 1585 and 1592, he began a successful career in London as an actor, writer, and part-owner (\"sharer\") of a playing company called the Lord Chamberlain's Men, later known as the King's Men after the ascension of King James VI of Scotland to the English throne. At age 49 (around 1613), he appears to have retired to Stratford, where he died three years later. Few records of Shakespeare's private life survive; this has stimulated considerable speculation about such matters as his physical appearance, his sexuality, his religious beliefs and even certain fringe theories as to whether the works attributed to him were written by others.\nShakespeare produced most of his known works between 1589 and 1613. His early plays were primarily comedies and histories and are regarded as some of the best works produced in these genres. He then wrote mainly tragedies until 1608, among them Hamlet, Othello, King Lear and Macbeth, all considered to be among the finest works in English. In the last phase of his life, he wrote tragicomedies (also known as romances) such as The Winter's Tale and The Tempest, and collaborated with other playwrights.\nMany of Shakespeare's plays were published in editions of varying quality and accuracy during his lifetime. However, in 1623, John Heminges and Henry Condell, two fellow actors and friends of Shakespeare's, published a more definitive text known as the First Folio, a posthumous collected edition of Shakespeare's dramatic works that includes 36 of his plays. Its Preface was a prescient poem by Ben Jonson, a former rival of Shakespeare, who hailed Shakespeare with the now famous epithet: \"not of an age, but for all time\".\n\n\n== Life ==\n\n\n=== Early life ===\n\nShakespeare was the son of John Shakespeare, an alderman and a successful glover (glove-maker) originally from Snitterfield in Warwickshire, and Mary Arden, the daughter of an affluent landowning family. He was born in Stratford-upon-Avon, where he was baptised on 26 April 1564. His date of birth is unknown but is traditionally observed on 23 April, Saint George's Day. This date, which can be traced to William Oldys and George Steevens, has proved appealing to biographers because Shakespeare died on the same date in 1616. He was the third of eight children, and the eldest surviving son.\nAlthough no attendance records for the period survive, most biographers agree that Shakespeare was probably educated at the King's New School in Stratford, a free school chartered in 1553, about a quarter-mile (400 m) from his home. Grammar schools varied in quality during the Elizabethan era, but grammar school curricula were largely similar: the basic Latin text was standardised by royal decree, and the school would have provided an intensive education in grammar based upon Latin classical authors.\nAt the age of 18, Shakespeare married 26-year-old Anne Hathaway. The consistory court of the Diocese of Worcester issued a marriage licence on 27 November 1582. The next day, two of Hathaway's neighbours posted bonds guaranteeing that no lawful claims impeded the marriage. The ceremony may have been arranged in some haste since the Worcester chancellor allowed the marriage banns to be read once instead of the usual three times, and six months after the marriage Anne gave birth to a daughter, Susanna, baptised 26 May 1583. Twins, son Hamnet and daughter Judith, followed almost two years later and were baptised 2 February 1585. Hamnet died of unknown causes at the age of 11 and was buried 11 August 1596.\n\nAfter the birth of the twins, Shakespeare left few historical traces until he is mentioned as part of the London theatre scene in 1592. The exception is the appearance of his name in the \"complaints bill\" of a law case before the Queen's Bench court at Westminster dated Michaelmas Term 1588 and 9 October 1589. Scholars refer to the years between 1585 and 1592 as Shakespeare's \"lost years\". Biographers attempting to account for this period have reported many apocryphal stories. Nicholas Rowe, Shakespeare's first biographer, recounted a Stratford legend that Shakespeare fled the town for London to escape prosecution for deer poaching in the estate of local squire Thomas Lucy. Shakespeare is also supposed to have taken his revenge on Lucy by writing a scurrilous ballad about him. Another 18th-century story has Shakespeare starting his theatrical career minding the horses of theatre patrons in London. John Aubrey reported that Shakespeare had been a country schoolmaster. Some 20th-century scholars suggested that Shakespeare may have been employed as a schoolmaster by Alexander Hoghton of Lancashire, a Catholic landowner who named a certain \"William Shakeshafte\" in his will. Little evidence substantiates such stories other than hearsay collected after his death, and Shakeshafte was a common name in the Lancashire area.\n\n\n=== London and theatrical career ===\nIt is not known definitively when Shakespeare began writing, but contemporary allusions and records of performances show that several of his plays were on the London stage by 1592. By then, he was sufficiently known in London to be attacked in print by the playwright Robert Greene in his Groats-Worth of Wit from that year:\n\n... there is an upstart Crow, beautified with our feathers, that with his Tiger's heart wrapped in a Player's hide, supposes he is as well able to bombast out a blank verse as the best of you: and being an absolute Johannes factotum, is in his own conceit the only Shake-scene in a country.\nScholars differ on the exact meaning of Greene's words, but most agree that Greene was accusing Shakespeare of reaching above his rank in trying to match such university-educated writers as Christopher Marlowe, Thomas Nashe, and Greene himself (the so-called \"University Wits\"). The italicised phrase parodying the line \"Oh, tiger's heart wrapped in a woman's hide\" from Shakespeare's Henry VI, Part 3, along with the pun \"Shake-scene\", clearly identify Shakespeare as Greene's target. As used here, Johannes Factotum (\"Jack of all trades\") refers to a second-rate tinkerer with the work of others, rather than the more common \"universal genius\".\nGreene's attack is the earliest surviving mention of Shakespeare's work in the theatre. Biographers suggest that his career may have begun any time from the mid-1580s to just before Greene's remarks. After 1594, Shakespeare's plays were performed at The Theatre, in Shoreditch, only by the Lord Chamberlain's Men, a company owned by a group of players, including Shakespeare, that soon became the leading playing company in London. After the death of Queen Elizabeth in 1603, the company was awarded a royal patent by the new King James I, and changed its name to the King's Men.\n\nIn 1599, a partnership of members of the company built their own theatre on the south bank of the River Thames, which they named the Globe. In 1608, the partnership also took over the Blackfriars indoor theatre. Extant records of Shakespeare's property purchases and investments indicate that his association with the company made him a wealthy man, and in 1597, he bought the second-largest house in Stratford, New Place, and in 1605, invested in a share of the parish tithes in Stratford.\nSome of Shakespeare's plays were published in quarto editions, beginning in 1594, and by 1598, his name had become a selling point and began to appear on the title pages. Shakespeare continued to act in his own and other plays after his success as a playwright. The 1616 edition of Ben Jonson's Works names him on the cast lists for Every Man in His Humour (1598) and Sejanus His Fall (1603). The absence of his name from the 1605 cast list for Jonson's Volpone is taken by some scholars as a sign that his acting career was nearing its end. The First Folio of 1623, however, lists Shakespeare as one of \"the Principal Actors in all these Plays\", some of which were first staged after Volpone, although one cannot know for certain which roles he played. In 1610, John Davies of Hereford wrote that \"good Will\" played \"kingly\" roles. In 1709, Rowe passed down a tradition that Shakespeare played the ghost of Hamlet's father. Later traditions maintain that he also played Adam in As You Like It, and the Chorus in Henry V, though scholars doubt the sources of that information.\nThroughout his career, Shakespeare divided his time between London and Stratford. In 1596, the year before he bought New Place as his family home in Stratford, Shakespeare was living in the parish of St Helen's, Bishopsgate, north of the River Thames. He moved across the river to Southwark by 1599, the same year his company constructed the Globe Theatre there. By 1604, he had moved north of the river again, to an area north of St Paul's Cathedral with many fine houses. There, he rented rooms from a French Huguenot named Christopher Mountjoy, a maker of women's wigs and other headgear.\n\n\n=== Later years and death ===\n\nNicholas Rowe was the first biographer to record the tradition, repeated by Samuel Johnson, that Shakespeare retired to Stratford \"some years before his death\". He was still working as an actor in London in 1608; in an answer to the sharers' petition in 1635, Cuthbert Burbage stated that after purchasing the lease of the Blackfriars Theatre in 1608 from Henry Evans, the King's Men \"placed men players\" there, \"which were Heminges, Condell, Shakespeare, etc.\". However, it is perhaps relevant that the bubonic plague raged in London throughout 1609. The London public playhouses were repeatedly closed during extended outbreaks of the plague (a total of over 60 months closure between May 1603 and February 1610), which meant there was often no acting work. Retirement from all work was uncommon at that time. Shakespeare continued to visit London during the years 1611–1614. In 1612, he was called as a witness in Bellott v Mountjoy, a court case concerning the marriage settlement of Mountjoy's daughter, Mary. In March 1613, he bought a gatehouse in the former Blackfriars priory; and from November 1614, he was in London for several weeks with his son-in-law, John Hall. After 1610, Shakespeare wrote fewer plays, and none are attributed to him after 1613. His last three plays were collaborations, probably with John Fletcher, who succeeded him as the house playwright of the King's Men. He retired in 1613, before the Globe Theatre burned down during the performance of Henry VIII on 29 June.\nShakespeare died on 23 April 1616, at the age of 52. He died within a month of signing his will, a document which he begins by describing himself as being in \"perfect health\". No extant contemporary source explains how or why he died. Half a century later, John Ward, the vicar of Stratford, wrote in his notebook: \"Shakespeare, Drayton, and Ben Jonson had a merry meeting and, it seems, drank too hard, for Shakespeare died of a fever there contracted\", not an impossible scenario since Shakespeare knew Jonson and Drayton. Of the tributes from fellow authors, one refers to his relatively sudden death: \"We wondered, Shakespeare, that thou went'st so soon / From the world's stage to the grave's tiring room.\"\n\nHe was survived by his wife and two daughters. Susanna had married a physician, John Hall, in 1607, and Judith had married Thomas Quiney, a vintner, two months before Shakespeare's death. Shakespeare signed his last will and testament on 25 March 1616; the following day, Thomas Quiney, his new son-in-law, was found guilty of fathering an illegitimate son by Margaret Wheeler, both of whom had died during childbirth. Thomas was ordered by the church court to do public penance, which would have caused much shame and embarrassment for the Shakespeare family.\nShakespeare bequeathed the bulk of his large estate to his elder daughter Susanna under stipulations that she pass it down intact to \"the first son of her body\". The Quineys had three children, all of whom died without marrying. The Halls had one child, Elizabeth, who married twice but died without children in 1670, ending Shakespeare's direct line. Shakespeare's will scarcely mentions his wife, Anne, who was probably entitled to one-third of his estate automatically. He did make a point, however, of leaving her \"my second best bed\", a bequest that has led to much speculation. Some scholars see the bequest as an insult to Anne, whereas others believe that the second-best bed would have been the matrimonial bed and therefore rich in significance.\n\nShakespeare was buried in the chancel of the Holy Trinity Church two days after his death. The epitaph carved into the stone slab covering his grave includes a curse against moving his bones, which was carefully avoided during restoration of the church in 2008:\n\nSome time before 1623, a funerary monument was erected in his memory on the north wall, with a half-effigy of him in the act of writing. Its plaque compares him to Nestor, Socrates, and Virgil. In 1623, in conjunction with the publication of the First Folio, the Droeshout engraving was published. Shakespeare has been commemorated in many statues and memorials around the world, including funeral monuments in Southwark Cathedral and Poets' Corner in Westminster Abbey.\n\n\n== Plays ==\n\nMost playwrights of the period typically collaborated with others at some point, as critics agree Shakespeare did, mostly early and late in his career.\nThe first recorded works of Shakespeare are Richard III and the three parts of Henry VI, written in the early 1590s during a vogue for historical drama. Shakespeare's plays are difficult to date precisely, however, and studies of the texts suggest that Titus Andronicus, The Comedy of Errors, The Taming of the Shrew, and The Two Gentlemen of Verona may also belong to Shakespeare's earliest period. His first histories, which draw heavily on the 1587 edition of Raphael Holinshed's Chronicles of England, Scotland, and Ireland, dramatise the destructive results of weak or corrupt rule and have been interpreted as a justification for the origins of the Tudor dynasty. The early plays were influenced by the works of other Elizabethan dramatists, especially Thomas Kyd and Christopher Marlowe, by the traditions of medieval drama, and by the plays of Seneca. The Comedy of Errors was also based on classical models, but no source for The Taming of the Shrew has been found, though it has an identical plot but different wording as another play with a similar name. Like The Two Gentlemen of Verona, in which two friends appear to approve of rape, the Shrew's story of the taming of a woman's independent spirit by a man sometimes troubles modern critics, directors, and audiences.\n\nShakespeare's early classical and Italianate comedies, containing tight double plots and precise comic sequences, give way in the mid-1590s to the romantic atmosphere of his most acclaimed comedies. A Midsummer Night's Dream is a witty mixture of romance, fairy magic, and comic lowlife scenes. Shakespeare's next comedy, the equally romantic The Merchant of Venice, contains a portrayal of the vengeful Jewish moneylender Shylock, which reflects dominant Elizabethan views but may appear derogatory to modern audiences. The wit and wordplay of Much Ado About Nothing, the charming rural setting of As You Like It, and the lively merrymaking of Twelfth Night complete Shakespeare's sequence of great comedies. After the lyrical Richard II, written almost entirely in verse, Shakespeare introduced prose comedy into the histories of the late 1590s, Henry IV, Part 1 and 2, and Henry V. Henry IV features Falstaff, rogue, wit and friend of Prince Hal. His characters become more complex and tender as he switches deftly between comic and serious scenes, prose and poetry, and achieves the narrative variety of his mature work. This period begins and ends with two tragedies: Romeo and Juliet, the famous romantic tragedy of sexually charged adolescence, love, and death; and Julius Caesar—based on Sir Thomas North's 1579 translation of Plutarch's Parallel Lives—which introduced a new kind of drama. According to Shakespearean scholar James Shapiro, in Julius Caesar, \"the various strands of politics, character, inwardness, contemporary events, even Shakespeare's own reflections on the act of writing, began to infuse each other\".\n\nIn the early 17th century, Shakespeare wrote the so-called \"problem plays\" Measure for Measure, Troilus and Cressida, and All's Well That Ends Well and a number of his best known tragedies. Many critics believe that Shakespeare's tragedies represent the peak of his art. Hamlet has probably been analysed more than any other Shakespearean character, especially for his famous soliloquy which begins \"To be or not to be; that is the question\". Unlike the introverted Hamlet, whose fatal flaw is hesitation, Othello and Lear are undone by hasty errors of judgement. The plots of Shakespeare's tragedies often hinge on such fatal errors or flaws, which overturn order and destroy the hero and those he loves. In Othello, Iago stokes Othello's sexual jealousy to the point where he murders the innocent wife who loves him. In King Lear, the old king commits the tragic error of giving up his powers, initiating the events which lead to the torture and blinding of the Earl of Gloucester and the murder of Lear's youngest daughter, Cordelia. According to the critic Frank Kermode, \"the play...offers neither its good characters nor its audience any relief from its cruelty\". In Macbeth, the shortest and most compressed of Shakespeare's tragedies, uncontrollable ambition incites Macbeth and his wife, Lady Macbeth, to murder the rightful king and usurp the throne until their own guilt destroys them in turn. In this play, Shakespeare adds a supernatural element to the tragic structure. His last major tragedies, Antony and Cleopatra and Coriolanus, contain some of Shakespeare's finest poetry and were considered his most successful tragedies by the poet and critic T. S. Eliot. Eliot wrote, \"Shakespeare acquired more essential history from Plutarch than most men could from the whole British Museum.\"\nIn his final period, Shakespeare turned to romance or tragicomedy and completed three more major plays: Cymbeline, The Winter's Tale, and The Tempest, as well as the collaboration, Pericles, Prince of Tyre. Less bleak than the tragedies, these four plays are graver in tone than the comedies of the 1590s, but they end with reconciliation and the forgiveness of potentially tragic errors. Some commentators have seen this change in mood as evidence of a more serene view of life on Shakespeare's part, but it may merely reflect the theatrical fashion of the day. Shakespeare collaborated on two further surviving plays, Henry VIII and The Two Noble Kinsmen, probably with John Fletcher.\n\n\n=== Classification ===\n\nShakespeare's works include the 36 plays printed in the First Folio of 1623, listed according to their folio classification as comedies, histories, and tragedies. Two plays not included in the First Folio, The Two Noble Kinsmen and Pericles, Prince of Tyre, are now accepted as part of the canon, with today's scholars agreeing that Shakespeare made major contributions to the writing of both. No Shakespearean poems were included in the First Folio, partly because the collection was compiled by men of theatre.\nIn the late 19th century, Edward Dowden classified four of the late comedies as romances, and though many scholars prefer to call them tragicomedies, Dowden's term is often used. In 1896, Frederick S. Boas coined the term \"problem plays\" to describe four plays: All's Well That Ends Well, Measure for Measure, Troilus and Cressida, and Hamlet. \"Dramas as singular in theme and temper cannot be strictly called comedies or tragedies\", he wrote. \"We may, therefore, borrow a convenient phrase from the theatre of today and class them together as Shakespeare's problem plays.\" The term, much debated and sometimes applied to other plays, remains in use, though Hamlet is definitively classed as a tragedy.\n\n\n=== Performances ===\n\nIt is not clear for which companies Shakespeare wrote his early plays. The title page of the 1594 edition of Titus Andronicus reveals that the play had been acted by three different troupes. After the plagues of 1592–93, Shakespeare's plays were performed by his own company at The Theatre and the Curtain in Shoreditch, north of the Thames. Londoners flocked there to see the first part of Henry IV, Leonard Digges recording, \"Let but Falstaff come, Hal, Poins, the rest ... and you scarce shall have a room\". When the company found themselves in dispute with their landlord, they pulled The Theatre down and used the timbers to construct the Globe Theatre, the first playhouse built by actors for actors, on the south bank of the Thames at Southwark. The Globe opened in autumn 1599, with Julius Caesar one of the first plays staged. Most of Shakespeare's greatest post-1599 plays were written for the Globe, including Hamlet, Othello, and King Lear.\n\nAfter the Lord Chamberlain's Men were renamed the King's Men in 1603, they entered a special relationship with the new King James. Although the performance records are patchy, the King's Men performed seven of Shakespeare's plays at court between 1 November 1604, and 31 October 1605, including two performances of The Merchant of Venice. After 1608, they performed at the indoor Blackfriars Theatre during the winter and the Globe during the summer. The indoor setting, combined with the Jacobean fashion for lavishly staged masques, allowed Shakespeare to introduce more elaborate stage devices. In Cymbeline, for example, Jupiter descends \"in thunder and lightning, sitting upon an eagle: he throws a thunderbolt. The ghosts fall on their knees.\"\nThe actors in Shakespeare's company included the famous Richard Burbage, William Kempe, Henry Condell and John Heminges. Burbage played the leading role in the first performances of many of Shakespeare's plays, including Richard III, Hamlet, Othello, and King Lear. The popular comic actor Will Kempe played the servant Peter in Romeo and Juliet and Dogberry in Much Ado About Nothing, among other characters. He was replaced around 1600 by Robert Armin, who played roles such as Touchstone in As You Like It and the fool in King Lear. In 1613, Sir Henry Wotton recorded that Henry VIII \"was set forth with many extraordinary circumstances of pomp and ceremony\". On 29 June, however, a cannon set fire to the thatch of the Globe and burned the theatre to the ground, an event which pinpoints the date of a Shakespeare play with rare precision.\n\n\n=== Textual sources ===\n\nIn 1623, John Heminges and Henry Condell, two of Shakespeare's friends from the King's Men, published the First Folio, a collected edition of Shakespeare's plays. It contained 36 texts, including 18 printed for the first time. Most of the others had already appeared in quarto versions—flimsy books made from sheets of paper folded twice to make four leaves. No evidence suggests that Shakespeare approved these editions, which the First Folio describes as \"stol'n and surreptitious copies\".\nAlfred Pollard termed some of the pre-1623 versions as \"bad quartos\" because of their adapted, paraphrased or garbled texts, which may in places have been reconstructed from memory. Where several versions of a play survive, each differs from the others. The differences may stem from copying or printing errors, from notes by actors or audience members, or from Shakespeare's own papers. In some cases, for example, Hamlet, Troilus and Cressida, and Othello, Shakespeare could have revised the texts between the quarto and folio editions. In the case of King Lear, however, while most modern editions do conflate them, the 1623 folio version is so different from the 1608 quarto that the Oxford Shakespeare prints them both, arguing that they cannot be conflated without confusion.\n\n\n== Poems ==\nIn 1593 and 1594, when the theatres were closed because of plague, Shakespeare published two narrative poems on sexual themes, Venus and Adonis and The Rape of Lucrece. He dedicated them to Henry Wriothesley, Earl of Southampton. In Venus and Adonis, an innocent Adonis rejects the sexual advances of Venus; while in The Rape of Lucrece, the virtuous wife Lucrece is raped by the lustful Tarquin. Influenced by Ovid's Metamorphoses, the poems show the guilt and moral confusion that result from uncontrolled lust. Both proved popular and were often reprinted during Shakespeare's lifetime. A third narrative poem, A Lover's Complaint, in which a young woman laments her seduction by a persuasive suitor, was printed in the first edition of the Sonnets in 1609. Most scholars now accept that Shakespeare wrote A Lover's Complaint. Critics consider that its fine qualities are marred by leaden effects. The Phoenix and the Turtle, printed in Robert Chester's 1601 Love's Martyr, mourns the deaths of the legendary phoenix and his lover, the faithful turtle dove. In 1599, two early drafts of sonnets 138 and 144 appeared in The Passionate Pilgrim, published under Shakespeare's name but without his permission.\n\n\n=== Sonnets ===\n\nPublished in 1609, the Sonnets were the last of Shakespeare's non-dramatic works to be printed. Scholars are not certain when each of the 154 sonnets was composed, but evidence suggests that Shakespeare wrote sonnets throughout his career for a private readership. Even before the two unauthorised sonnets appeared in The Passionate Pilgrim in 1599, Francis Meres had referred in 1598 to Shakespeare's \"sugred Sonnets among his private friends\". Few analysts believe that the published collection follows Shakespeare's intended sequence. He seems to have planned two contrasting series: one about uncontrollable lust for a married woman of dark complexion (the \"dark lady\"), and one about conflicted love for a fair young man (the \"fair youth\"). It remains unclear if these figures represent real individuals, or if the authorial \"I\" who addresses them represents Shakespeare himself, though Wordsworth believed that with the sonnets \"Shakespeare unlocked his heart\".\n\nThe 1609 edition was dedicated to a \"Mr. W.H.\", credited as \"the only begetter\" of the poems. It is not known whether this was written by Shakespeare himself or by the publisher, Thomas Thorpe, whose initials appear at the foot of the dedication page; nor is it known who Mr. W.H. was, despite numerous theories, or whether Shakespeare even authorised the publication. Critics praise the Sonnets as a profound meditation on the nature of love, sexual passion, procreation, death, and time.\n\n\n== Style ==\n\nShakespeare's first plays were written in the conventional style of the day. He wrote them in a stylised language that does not always spring naturally from the needs of the characters or the drama. The poetry depends on extended, sometimes elaborate metaphors and conceits, and the language is often rhetorical—written for actors to declaim rather than speak. The grand speeches in Titus Andronicus, in the view of some critics, often hold up the action, for example; and the verse in The Two Gentlemen of Verona has been described as stilted.\n\nHowever, Shakespeare soon began to adapt the traditional styles to his own purposes. The opening soliloquy of Richard III has its roots in the self-declaration of Vice in medieval drama. At the same time, Richard's vivid self-awareness looks forward to the soliloquies of Shakespeare's mature plays. No single play marks a change from the traditional to the freer style. Shakespeare combined the two throughout his career, with Romeo and Juliet perhaps the best example of the mixing of the styles. By the time of Romeo and Juliet, Richard II, and A Midsummer Night's Dream in the mid-1590s, Shakespeare had begun to write a more natural poetry. He increasingly tuned his metaphors and images to the needs of the drama itself.\nShakespeare's standard poetic form was blank verse, composed in iambic pentameter. In practice, this meant that his verse was usually unrhymed and consisted of ten syllables to a line, spoken with a stress on every second syllable. The blank verse of his early plays is quite different from that of his later ones. It is often beautiful, but its sentences tend to start, pause, and finish at the end of lines, with the risk of monotony. Once Shakespeare mastered traditional blank verse, he began to interrupt and vary its flow. This technique releases the new power and flexibility of the poetry in plays such as Julius Caesar and Hamlet. Shakespeare uses it, for example, to convey the turmoil in Hamlet's mind:\n\nAfter Hamlet, Shakespeare varied his poetic style further, particularly in the more emotional passages of the late tragedies. The literary critic A. C. Bradley described this style as \"more concentrated, rapid, varied, and, in construction, less regular, not seldom twisted or elliptical\". In the last phase of his career, Shakespeare adopted many techniques to achieve these effects. These included run-on lines, irregular pauses and stops, and extreme variations in sentence structure and length. In Macbeth, for example, the language darts from one unrelated metaphor or simile to another: \"was the hope drunk/ Wherein you dressed yourself?\" (1.7.35–38); \"... pity, like a naked new-born babe/ Striding the blast, or heaven's cherubim, hors'd/ Upon the sightless couriers of the air ...\" (1.7.21–25). The listener is challenged to complete the sense. The late romances, with their shifts in time and surprising turns of plot, inspired a last poetic style in which long and short sentences are set against one another, clauses are piled up, subject and object are reversed, and words are omitted, creating an effect of spontaneity.\nShakespeare combined poetic genius with a practical sense of the theatre. Like all playwrights of the time, he dramatised stories from sources such as Plutarch and Holinshed. He reshaped each plot to create several centres of interest and to show as many sides of a narrative to the audience as possible. This strength of design ensures that a Shakespeare play can survive translation, cutting, and wide interpretation without loss to its core drama. As Shakespeare's mastery grew, he gave his characters clearer and more varied motivations and distinctive patterns of speech. He preserved aspects of his earlier style in the later plays, however. In Shakespeare's late romances, he deliberately returned to a more artificial style, which emphasised the illusion of theatre.\n\n\n== Legacy ==\n\n\n=== Influence ===\n\nShakespeare's work has made a significant and lasting impression on later theatre and literature. In particular, he expanded the dramatic potential of characterisation, plot, language, and genre. Until Romeo and Juliet, for example, romance had not been viewed as a worthy topic for tragedy. Soliloquies had been used mainly to convey information about characters or events, but Shakespeare used them to explore characters' minds. His work heavily influenced later poetry. The Romantic poets attempted to revive Shakespearean verse drama, though with little success. Critic George Steiner described all English verse dramas from Coleridge to Tennyson as \"feeble variations on Shakespearean themes\". John Milton, considered by many to be the most important English poet after Shakespeare, wrote in tribute: \"Thou in our wonder and astonishment/ Hast built thyself a live-long monument.\"\nShakespeare influenced novelists such as Thomas Hardy, William Faulkner, and Charles Dickens. The American novelist Herman Melville's soliloquies owe much to Shakespeare; his Captain Ahab in Moby-Dick is a classic tragic hero, inspired by King Lear. Scholars have identified 20,000 pieces of music linked to Shakespeare's works, including Felix Mendelssohn's overture and incidental music for A Midsummer Night's Dream and Sergei Prokofiev's ballet Romeo and Juliet. His work has inspired several operas, among them Giuseppe Verdi's Macbeth, Otello and Falstaff, whose critical standing compares with that of the source plays. Shakespeare has also inspired many painters, including the Romantics and the Pre-Raphaelites, while William Hogarth's 1745 painting of actor David Garrick playing Richard III was decisive in establishing the genre of theatrical portraiture in Britain. The Swiss Romantic artist Henry Fuseli, a friend of William Blake, even translated Macbeth into German. The psychoanalyst Sigmund Freud drew on Shakespearean psychology, in particular, that of Hamlet, for his theories of human nature. Shakespeare has been a rich source for filmmakers; Akira Kurosawa adapted Macbeth and King Lear as Throne of Blood and Ran, respectively. Other examples of Shakespeare on film include Max Reinhardt's A Midsummer Night's Dream, Laurence Olivier's Hamlet and Al Pacino's documentary Looking For Richard. Orson Welles, a lifelong lover of Shakespeare, directed and starred in Macbeth, Othello and Chimes at Midnight, in which he plays John Falstaff, which Welles himself called his best work.\nIn Shakespeare's day, English grammar, spelling, and pronunciation were less standardised than they are now, and his use of language helped shape modern English. Samuel Johnson quoted him more often than any other author in his A Dictionary of the English Language, the first serious work of its type. Expressions such as \"with bated breath\" (Merchant of Venice) and \"a foregone conclusion\" (Othello) have found their way into everyday English speech.\nShakespeare's influence extends far beyond his native England and the English language. His reception in Germany was particularly significant; as early as the 18th century Shakespeare was widely translated and popularised in Germany, and gradually became a \"classic of the German Weimar era;\" Christoph Martin Wieland was the first to produce complete translations of Shakespeare's plays in any language. Actor and theatre director Simon Callow writes, \"this master, this titan, this genius, so profoundly British and so effortlessly universal, each different culture – German, Italian, Russian – was obliged to respond to the Shakespearean example; for the most part, they embraced it, and him, with joyous abandon, as the possibilities of language and character in action that he celebrated liberated writers across the continent. Some of the most deeply affecting productions of Shakespeare have been non-English, and non-European. He is that unique writer: he has something for everyone.\"\nAccording to Guinness World Records, Shakespeare remains the world's best-selling playwright, with sales of his plays and poetry believed to have achieved in excess of four billion copies in the almost 400 years since his death. He is also the third most translated author in history.\n\n\n=== Critical reputation ===\n\nShakespeare was not revered in his lifetime, but he received a large amount of praise. In 1598, the cleric and author Francis Meres singled him out from a group of English playwrights as \"the most excellent\" in both comedy and tragedy. The authors of the Parnassus plays at St John's College, Cambridge, numbered him with Chaucer, Gower, and Spenser. In the First Folio, Ben Jonson called Shakespeare the \"Soul of the age, the applause, delight, the wonder of our stage\", although he had remarked elsewhere that \"Shakespeare wanted art\" (lacked skill).\nBetween the Restoration of the monarchy in 1660 and the end of the 17th century, classical ideas were in vogue. As a result, critics of the time mostly rated Shakespeare below John Fletcher and Ben Jonson. Thomas Rymer, for example, condemned Shakespeare for mixing the comic with the tragic. Nevertheless, poet and critic John Dryden rated Shakespeare highly, saying of Jonson, \"I admire him, but I love Shakespeare\". He also famously remarked that Shakespeare \"was naturally learned; he needed not the spectacles of books to read nature; he looked inwards, and found her there.\" For several decades, Rymer's view held sway. But during the 18th century, critics began to respond to Shakespeare on his own terms and, like Dryden, to acclaim what they termed his natural genius. A series of scholarly editions of his work, notably those of Samuel Johnson in 1765 and Edmond Malone in 1790, added to his growing reputation. By 1800, he was firmly enshrined as the national poet, and described as the \"Bard of Avon\" (or simply \"the Bard\"). In the 18th and 19th centuries, his reputation also spread abroad. Among those who championed him were the writers Voltaire, Goethe, Stendhal, and Victor Hugo.\n\nDuring the Romantic era, Shakespeare was praised by the poet and literary philosopher Samuel Taylor Coleridge, and the critic August Wilhelm Schlegel translated his plays in the spirit of German Romanticism. In the 19th century, critical admiration for Shakespeare's genius often bordered on adulation. \"This King Shakespeare,\" the essayist Thomas Carlyle wrote in 1840, \"does not he shine, in crowned sovereignty, over us all, as the noblest, gentlest, yet strongest of rallying signs; indestructible\". The Victorians produced his plays as lavish spectacles on a grand scale. The playwright and critic George Bernard Shaw mocked the cult of Shakespeare worship as \"bardolatry\", claiming that the new naturalism of Ibsen's plays had made Shakespeare obsolete.\nThe modernist revolution in the arts during the early 20th century, far from discarding Shakespeare, eagerly enlisted his work in the service of the avant-garde. The Expressionists in Germany and the Futurists in Moscow mounted productions of his plays. Marxist playwright and director Bertolt Brecht devised an epic theatre under the influence of Shakespeare. The poet and critic T. S. Eliot argued against Shaw that Shakespeare's \"primitiveness\" in fact made him truly modern. Eliot, along with G. Wilson Knight and the school of New Criticism, led a movement towards a closer reading of Shakespeare's imagery. In the 1950s, a wave of new critical approaches replaced modernism and paved the way for post-modern studies of Shakespeare. Comparing Shakespeare's accomplishments to those of leading figures in philosophy and theology, Harold Bloom wrote, \"Shakespeare was larger than Plato and than St. Augustine. He encloses us because we see with his fundamental perceptions.\"\n\n\n== Speculation ==\n\n\n=== Authorship ===\n\nAround 230 years after Shakespeare's death, doubts began to be expressed about the authorship of the works attributed to him. Proposed alternative candidates include Francis Bacon, Christopher Marlowe, and Edward de Vere, 17th Earl of Oxford. Several \"group theories\" have also been proposed. All but a few Shakespeare scholars and literary historians consider it a fringe theory, with only a small minority of academics who believe that there is reason to question the traditional attribution, but interest in the subject, particularly the Oxfordian theory of Shakespeare authorship, continues into the 21st century.\n\n\n=== Religion ===\n\nShakespeare conformed to the official state religion, but his private views on religion have been the subject of debate. Shakespeare's will uses a Protestant formula, and he was a confirmed member of the Church of England, where he was married, his children were baptised, and where he is buried. \nSome scholars are of the view that members of Shakespeare's family were Catholics, at a time when practising Catholicism in England was against the law. Shakespeare's mother, Mary Arden, certainly came from a pious Catholic family. The strongest evidence might be a Catholic statement of faith signed by his father, John Shakespeare, found in 1757 in the rafters of his former house in Henley Street. However, the document is now lost and scholars differ as to its authenticity. In 1591, the authorities reported that John Shakespeare had missed church \"for fear of process for debt\", a common Catholic excuse. In 1606, the name of William's daughter Susanna appears on a list of those who failed to attend Easter communion in Stratford. \nOther authors argue that there is a lack of evidence about Shakespeare's religious beliefs. Scholars find evidence both for and against Shakespeare's Catholicism, Protestantism, or lack of belief in his plays, but the truth may be impossible to prove.\n\n\n=== Sexuality ===\n\nFew details of Shakespeare's sexuality are known. At 18, he married 26-year-old Anne Hathaway, who was pregnant. Susanna, the first of their three children, was born six months later on 26 May 1583. Over the centuries, some readers have posited that Shakespeare's sonnets are autobiographical, and point to them as evidence of his love for a young man. Others read the same passages as the expression of intense friendship rather than romantic love. The 26 so-called \"Dark Lady\" sonnets, addressed to a married woman, are taken as evidence of heterosexual liaisons.\n\n\n=== Portraiture ===\n\nNo written contemporary description of Shakespeare's physical appearance survives, and no evidence suggests that he ever commissioned a portrait. From the 18th century, the desire for authentic Shakespeare portraits fuelled claims that various surviving pictures depicted Shakespeare. That demand also led to the production of several fake portraits, as well as misattributions, re-paintings, and relabelling of portraits of other people. \nSome scholars suggest that the Droeshout portrait, which Ben Jonson approved of as a good likeness, and his Stratford monument provide perhaps the best evidence of his appearance. Of the claimed paintings, art historian Tarnya Cooper concluded that the Chandos portrait had \"the strongest claim of any of the known contenders to be a true portrait of Shakespeare\". After a three-year study supported by the National Portrait Gallery, London, the portrait's owners, Cooper contended that its composition date, contemporary with Shakespeare, its subsequent provenance, and the sitter's attire, all supported the attribution.\n\n\n== See also ==\nOutline of William Shakespeare\nEnglish Renaissance theatre\nSpelling of Shakespeare's name\nWorld Shakespeare Bibliography\nShakespeare's Politics\n\n\n== References ==\n\n\n=== Notes ===\n\n\n=== Citations ===\n\n\n=== Sources ===\nBooks\n\nArticles and online\n\n\n== External links ==\n\nDigital editions\n\nWilliam Shakespeare's plays on Bookwise\nInternet Shakespeare Editions\nThe Folger Shakespeare\nOpen Source Shakespeare complete works, with search engine and concordance\nThe Shakespeare Quartos Archive\nWorks by William Shakespeare in eBook form at Standard Ebooks\nWorks by William Shakespeare at Project Gutenberg\nWorks by or about William Shakespeare at the Internet Archive\nWorks by William Shakespeare at LibriVox (public domain audiobooks) \nExhibitions\n\nShakespeare Documented an online exhibition documenting Shakespeare in his own time\nShakespeare's Will from The National Archives\nThe Shakespeare Birthplace Trust\nWilliam Shakespeare at the British Library. Archived 23 September 2021 at the Wayback Machine.\nMusic\n\nWorks by William Shakespeare set to music: free scores in the Choral Public Domain Library (ChoralWiki)\nWorks by William Shakespeare set to music: Scores at the International Music Score Library Project\nEducation\n\nShakespeare at Home an online resource providing free educational resources on William Shakespeare and the Renaissance world. Activities are dyslexia friendly and suitable for all ages.\nLegacy and criticism\n\nRecords on Shakespeare's Theatre Legacy from the UK Parliamentary Collections\nWinston Churchill & Shakespeare – UK Parliament Living Heritage"
    },
    {
        "id": "macroeconomics",
        "name": "Macroeconomics",
        "text": "Macroeconomics is a branch of economics that deals with the performance, structure, behavior, and decision-making of an economy as a whole. This includes regional, national, and global economies. Macroeconomists study topics such as output/GDP (gross domestic product) and national income, unemployment (including unemployment rates), price indices and inflation, consumption, saving, investment, energy, international trade, and international finance.\nMacroeconomics and microeconomics are the two most general fields in economics. The focus of macroeconomics is often on a country (or larger entities like the whole world) and how its markets interact to produce large-scale phenomena that economists refer to as aggregate variables. In microeconomics the focus of analysis is often a single market, such as whether changes in supply or demand are to blame for price increases in the oil and automotive sectors. \nFrom introductory classes in \"principles of economics\" through doctoral studies, the macro/micro divide is institutionalized in the field of economics. Most economists identify as either macro- or micro-economists.\nMacroeconomics is traditionally divided into topics along different time frames: the analysis of short-term fluctuations over the business cycle, the determination of structural levels of variables like inflation and unemployment in the medium (i.e. unaffected by short-term deviations) term, and the study of long-term economic growth. It also studies the consequences of policies targeted at mitigating fluctuations like fiscal or monetary policy, using taxation and government expenditure or interest rates, respectively, and of policies that can affect living standards in the long term, e.g. by affecting growth rates.\nMacroeconomics as a separate field of research and study is generally recognized to start in 1936, when John Maynard Keynes published his The General Theory of Employment, Interest and Money, but its intellectual predecessors are much older. Since World War II, various macroeconomic schools of thought like Keynesians, monetarists, new classical and new Keynesian economists have made contributions to the development of the macroeconomic research mainstream.\n\n\n== Basic macroeconomic concepts ==\nMacroeconomics encompasses a variety of concepts and variables, but above all the three central macroeconomic variables are output, unemployment, and inflation.: 39  Besides, the time horizon varies for different types of macroeconomic topics, and this distinction is crucial for many research and policy debates.: 54  A further important dimension is that of an economy's openness, economic theory distinguishing sharply between closed economies and open economies.: 373 \n\n\n=== Time frame ===\nIt is usual to distinguish between three time horizons in macroeconomics, each having its own focus on e.g. the determination of output:: 54 \n\nthe short run (e.g. a few years): Focus is on business cycle fluctuations and changes in aggregate demand which often drive them. Stabilization policies like monetary policy or fiscal policy are relevant in this time frame\nthe medium run (e.g. a decade): Over the medium run, the economy tends to an output level determined by supply factors like the capital stock, the technology level and the labor force, and unemployment tends to revert to its structural (or \"natural\") level. These factors move slowly, so that it is a reasonable approximation to take them as given in a medium-term time scale, though labour market policies and competition policy are instruments that may influence the economy's structures and hence also the medium-run equilibrium\nthe long run (e.g. a couple of decades or more): On this time scale, emphasis is on the determinants of long-run economic growth like accumulation of human and physical capital, technological innovations and demographic changes. Potential policies to influence these developments are education reforms, incentives to change saving rates or to increase R&D activities.\n\n\n=== Output and income ===\nNational output is the total amount of everything a country produces in a given period of time. Everything that is produced and sold generates an equal amount of income. The total net output of the economy is usually measured as gross domestic product (GDP). Adding net factor incomes from abroad to GDP produces gross national income (GNI), which measures total income of all residents in the economy. In most countries, the difference between GDP and GNI are modest so that GDP can approximately be treated as total income of all the inhabitants as well, but in some countries, e.g. countries with very large net foreign assets (or debt), the difference may be considerable.: 385 \nEconomists interested in long-run increases in output study economic growth. Advances in technology, accumulation of machinery and other capital, and better education and human capital, are all factors that lead to increased economic output over time. However, output does not always increase consistently over time. Business cycles can cause short-term drops in output called recessions. Economists look for macroeconomic policies that prevent economies from slipping into either recessions or overheating and that lead to higher productivity levels and standards of living.\n\n\n=== Unemployment ===\n\nThe amount of unemployment in an economy is measured by the unemployment rate, i.e. the percentage of persons in the labor force who do not have a job, but who are actively looking for one. People who are retired, pursuing education, or discouraged from seeking work by a lack of job prospects are not part of the labor force and consequently not counted as unemployed, either.: 156 \nUnemployment has a short-run cyclical component which depends on the business cycle, and a more permanent structural component, which can be loosely thought of as the average unemployment rate in an economy over extended periods, and which is often termed the natural or structural: 167  rate of unemployment.\nCyclical unemployment occurs when growth stagnates. Okun's law represents the empirical relationship between unemployment and short-run GDP growth. The original version of Okun's law states that a 3% increase in output would lead to a 1% decrease in unemployment.\nThe structural or natural rate of unemployment is the level of unemployment that will occur in a medium-run equilibrium, i.e. a situation with a cyclical unemployment rate of zero. There may be several reasons why there is some positive unemployment level even in a cyclically neutral situation, which all have their foundation in some kind of market failure:\n\nSearch unemployment (also called frictional unemployment) occurs when workers and firms are heterogeneous and there is imperfect information, generally causing a time-consuming search and matching process when filling a job vacancy in a firm, during which the prospective worker will often be unemployed. Sectoral shifts and other reasons for a changed demand from firms for workers with particular skills and characteristics, which occur continually in a changing economy, may also cause more search unemployment because of increased mismatch.\nEfficiency wage models are labor market models in which firms choose not to lower wages to the level where supply equals demand because the lower wages would lower employees' efficiency levels\nTrade unions, which are important actors in the labor market in some countries, may exercise market power in order to keep wages over the market-clearing level for the benefice of their members even at the cost of some unemployment\nLegal minimum wages may prevent the wage from falling to a market-clearing level, causing unemployment among low-skilled (and low-paid) workers. In the case of employers having some monopsony power, however, employment effects may have the opposite sign.\n\n\n=== Inflation and deflation ===\n\nA general price increase across the entire economy is called inflation. When prices decrease, there is deflation. Economists measure these changes in prices with price indexes. Inflation will increase when an economy becomes overheated and grows too quickly. Similarly, a declining economy can lead to decreasing inflation and even in some cases deflation.\nCentral bankers conducting monetary policy usually have as a main priority to avoid too high inflation, typically by adjusting interest rates. High inflation as well as deflation can lead to increased uncertainty and other negative consequences, in particular when the inflation (or deflation) is unexpected. Consequently, most central banks aim for a positive, but stable and not very high inflation level.\nChanges in the inflation level may be the result of several factors. Too much aggregate demand in the economy will cause an overheating, raising inflation rates via the Phillips curve because of a tight labor market leading to large wage increases which will be transmitted to increases in the price of the products of employers. Too little aggregate demand will have the opposite effect of creating more unemployment and lower wages, thereby decreasing inflation. Aggregate supply shocks will also affect inflation, e.g. the oil crises of the 1970s and the 2021–2023 global energy crisis. Changes in inflation may also impact the formation of inflation expectations, creating a self-fulfilling inflationary or deflationary spiral.\nThe monetarist quantity theory of money holds that changes in the price level are directly caused by changes in the money supply. Whereas there is empirical evidence that there is a long-run positive correlation between the growth rate of the money stock and the rate of inflation, the quantity theory has proved unreliable in the short- and medium-run time horizon relevant to monetary policy and is abandoned as a practical guideline by most central banks today.\n\n\n=== Open economy macroeconomics ===\nOpen economy macroeconomics deals with the consequences of international trade in goods, financial assets and possibly factor markets like labor migration and international relocation of firms (physical capital). It explores what determines import, export, the balance of trade and over longer horizons the accumulation of net foreign assets. An important topic is the role of exchange rates and the pros and cons of maintaining a fixed exchange rate system or even a currency union like the Economic and Monetary Union of the European Union, drawing on the research literature on optimum currency areas.\n\n\n=== GDP Equation Using Expenditure Approach ===\nOne way to calculate Gross Domestic Product, or total net output, is the expenditure method. The GDP essentially tells you how big the economy is. The larger the GDP value, the bigger the economy. The expenditure approach involves looking at four main components: Consumer Spending, Government Spending, Investment Spending, and Net Exports. Consumer Spending is made up of ordinary consumers spending money on different kinds of products and also investing their money in residential markets. Government Spending involves the government spending money on goods and services and they may assist consumers or businesses with spending as well. For instance, purchasing physical capital for businesses. While transfer payments, which includes things like welfare or social security payments), are things that a government pays, it is not included in the final calculation of the expenditure approach because it is not paying for any final goods and services. Investment spending involves businesses spending money on physical capital/equipment to help with producing goods and services. Lastly, net exports is just exports minus imports. Exports are goods and services that a country is selling to people abroad and imports are goods and services that people from a country are receiving from abroad.\nHence, the equation for the expenditure approach to calculating the Gross Domestic Product is\nGDP = Consumer Spending(CS) + Government Spending(GS) + Investment Spending(IS) + Net Exports(EXP-IMP). \n\n\n=== GDP Deflator Equation & Explanation ===\nAnother concern with measuring a country's economic growth is that even though we see the GDP growing, that does not inherently mean the economy is growing. Most of the increase in GDP may just be due to inflation. To know whether this is the case, we have to calculate the GDP Deflator which adjusts the GDP for inflation.\nGDP Deflator = (Nominal GDP/Real GDP) x 100\nNominal GDP is GDP that includes inflation and Real GDP is GDP adjusted for inflation. To adjust for inflation means that the effect of inflation on the value was removed.\nA GDP Deflator of 100 indicates that there is no inflation nor deflation. A GDP Deflator value that is greater than 100 indicates that there is inflation. A GDP Deflator value that is less than 100 indicates that there is deflation.\n\n\n== Development ==\n\nMacroeconomics as a separate field of research and study is generally recognized to start with the publication of John Maynard Keynes' The General Theory of Employment, Interest, and Money in 1936.: 526  The terms \"macrodynamics\" and \"macroanalysis\" were introduced by Ragnar Frisch in 1933, and Lawrence Klein in 1946 used the word \"macroeconomics\" itself in a journal title in 1946. but naturally several of the themes which are central to macroeconomic research had been discussed by thoughtful economists and other writers long before 1936.\n\n\n=== Before Keynes ===\nIn particular, macroeconomic questions before Keynes were the topic of the two long-standing traditions of business cycle theory and monetary theory. William Stanley Jevons was one of the pioneers of the first tradition, whereas the quantity theory of money, labelled the oldest surviving theory in economics, as an example of the second was described already in the 16th century by Martín de Azpilcueta and later discussed by personalities like John Locke and David Hume. In the first decades of the 20th century monetary theory was dominated by the eminent economists Alfred Marshall, Knut Wicksell and Irving Fisher.\n\n\n=== Keynes and Keynesian economics ===\nWhen the Great Depression struck, the reigning economists had difficulty explaining how goods could go unsold and workers could be left unemployed. In the prevailing neoclassical economics paradigm, prices and wages would drop until the market cleared, and all goods and labor were sold. Keynes in his main work, the General Theory, initiated what is known as the Keynesian Revolution. He offered a new interpretation of events and a whole intellectural framework - a novel theory of economics that explained why markets might not clear, which would evolve into a school of thought known as Keynesian economics, also called Keynesianism or Keynesian theory.: 526 \nIn Keynes' theory, aggregate demand - by Keynes called \"effective demand\" - was key to determining output. Even if Keynes conceded that output might eventually return to a medium-run equilibrium (or \"potential\") level, the process would be slow at best. Keynes coined the term liquidity preference (his preferred name for what is also known as money demand) and explained how monetary policy might affect aggregate demand, at the same time offering clear policy recommendations for an active role of fiscal policy in stabilizing aggregate demand and hence output and employment. In addition, he explained how the multiplier effect would magnify a small decrease in consumption or investment and cause declines throughout the economy, and noted the role that uncertainty and animal spirits can play in the economy.: 526 \nThe generation following Keynes combined the macroeconomics of the General Theory with neoclassical microeconomics to create the neoclassical synthesis. By the 1950s, most economists had accepted the synthesis view of the macroeconomy.: 526  Economists like Paul Samuelson, Franco Modigliani, James Tobin, and Robert Solow developed formal Keynesian models and contributed formal theories of consumption, investment, and money demand that fleshed out the Keynesian framework.: 527 \n\n\n=== Monetarism ===\nMilton Friedman updated the quantity theory of money to include a role for money demand. He argued that the role of money in the economy was sufficient to explain the Great Depression, and that aggregate demand oriented explanations were not necessary. Friedman also argued that monetary policy was more effective than fiscal policy; however, Friedman doubted the government's ability to \"fine-tune\" the economy with monetary policy. He generally favored a policy of steady growth in money supply instead of frequent intervention.: 528 \nFriedman also challenged the original simple Phillips curve relationship between inflation and unemployment. Friedman and Edmund Phelps (who was not a monetarist) proposed an \"augmented\" version of the Phillips curve that excluded the possibility of a stable, long-run tradeoff between inflation and unemployment. When the oil shocks of the 1970s created a high unemployment and high inflation, Friedman and Phelps were vindicated. Monetarism was particularly influential in the early 1980s, but fell out of favor when central banks found the results disappointing when trying to target money supply instead of interest rates as monetarists recommended, concluding that the relationships between money growth, inflation and real GDP growth are too unstable to be useful in practical monetary policy making.\n\n\n=== New classical economics ===\nNew classical macroeconomics further challenged the Keynesian school. A central development in new classical thought came when Robert Lucas introduced rational expectations to macroeconomics. Prior to Lucas, economists had generally used adaptive expectations where agents were assumed to look at the recent past to make expectations about the future. Under rational expectations, agents are assumed to be more sophisticated.: 530  Consumers will not simply assume a 2% inflation rate just because that has been the average the past few years; they will look at current monetary policy and economic conditions to make an informed forecast. In the new classical models with rational expectations, monetary policy only had a limited impact.\nLucas also made an influential critique of Keynesian empirical models. He argued that forecasting models based on empirical relationships would keep producing the same predictions even as the underlying model generating the data changed. He advocated models based on fundamental economic theory (i.e. having an explicit microeconomic foundation) that would, in principle, be structurally accurate as economies changed.: 530 \nFollowing Lucas's critique, new classical economists, led by Edward C. Prescott and Finn E. Kydland, created real business cycle (RBC) models of the macro economy. RBC models were created by combining fundamental equations from neo-classical microeconomics to make quantitative models. In order to generate macroeconomic fluctuations, RBC models explained recessions and unemployment with changes in technology instead of changes in the markets for goods or money. Critics of RBC models argue that technological changes, which typically diffuse slowly throughout the economy, could hardly generate the large short-run output fluctuations that we observe. In addition, there is strong empirical evidence that monetary policy does affect real economic activity, and the idea that technological regress can explain recent recessions seems implausible.: 533 : 195 \nDespite criticism of the realism in the RBC models, they have been very influential in economic methodology by providing the first examples of general equilibrium models based on microeconomic foundations and a specification of underlying shocks that aim to explain the main features of macroeconomic fluctuations, not only qualitatively, but also quantitatively. In this way, they were forerunners of the later DSGE models.: 194 \n\n\n=== New Keynesian response ===\nNew Keynesian economists responded to the new classical school by adopting rational expectations and focusing on developing micro-founded models that were immune to the Lucas critique. Like classical models, new classical models had assumed that prices would be able to adjust perfectly and monetary policy would only lead to price changes. New Keynesian models investigated sources of sticky prices and wages due to imperfect competition, which would not adjust, allowing monetary policy to impact quantities instead of prices. Stanley Fischer and John B. Taylor produced early work in this area by showing that monetary policy could be effective even in models with rational expectations when contracts locked in wages for workers. Other new Keynesian economists, including Olivier Blanchard, Janet Yellen, Julio Rotemberg, Greg Mankiw, David Romer, and Michael Woodford, expanded on this work and demonstrated other cases where various market imperfections caused inflexible prices and wages leading in turn to monetary and fiscal policy having real effects. Other researchers focused on imperferctions in labor markets, developing models of efficiency wages or search and matching (SAM) models, or imperfections in credit markets like Ben Bernanke.: 532–36 \nBy the late 1990s, economists had reached a rough consensus. The market imperfections and nominal rigidities of new Keynesian theory was combined with rational expectations and the RBC methodology to produce a new and popular type of models called dynamic stochastic general equilibrium (DSGE) models. The fusion of elements from different schools of thought has been dubbed the new neoclassical synthesis. These models are now used by many central banks and are a core part of contemporary macroeconomics.: 535–36 \n\n\n=== 2007–2008 financial crisis ===\nThe 2007–2008 financial crisis, which led to the Great Recession, led to major reassessment of macroeconomics, which as a field generally had neglected the potential role of financial institutions in the economy. After the crisis, macroeconomic researchers have turned their attention in several new directions:\n\nthe financial system and the nature of macrofinancial linkages and frictions, studying leverage, liquidity and complexity problems in the financial sector, the use of macroprudential tools and the dangers of an unsustainable public debt: 537 \nincreased emphasis on empirical work as part of the so-called credibility revolution in economics, using improved methods to distinguish between correlation and causality to improve future policy discussions\ninterest in understanding the importance of heterogeneity among the economic agents, leading among other examples to the construction of heterogeneous agent new Keynesian models (HANK models), which may potentially also improve understanding of the impact of macroeconomics on the income distribution\nunderstanding the implications of integrating the findings of the increasingly useful behavioral economics literature into macroeconomics and behavioral finance\n\n\n=== Growth models ===\nResearch in the economics of the determinants behind long-run economic growth has followed its own course. The Harrod-Domar model from the 1940s attempted to build a long-run growth model inspired by Keynesian demand-driven considerations. The Solow–Swan model worked out by Robert Solow and, independently, Trevor Swan in the 1950s achieved more long-lasting success, however, and is still today a common textbook model for explaining economic growth in the long-run. The model operates with a production function where national output is the product of two inputs: capital and labor. The Solow model assumes that labor and capital are used at constant rates without the fluctuations in unemployment and capital utilization commonly seen in business cycles. In this model, increases in output, i.e. economic growth, can only occur because of an increase in the capital stock, a larger population, or technological advancements that lead to higher productivity (total factor productivity). An increase in the savings rate leads to a temporary increase as the economy creates more capital, which adds to output. However, eventually the depreciation rate will limit the expansion of capital: savings will be used up replacing depreciated capital, and no savings will remain to pay for an additional expansion in capital. Solow's model suggests that economic growth in terms of output per capita depends solely on technological advances that enhance productivity. The Solow model can be interpreted as a special case of the more general Ramsey growth model, where households' savings rates are not constant as in the Solow model, but derived from an explicit intertemporal utility function.\nIn the 1980s and 1990s endogenous growth theory arose to challenge the neoclassical growth theory of Ramsey and Solow. This group of models explains economic growth through factors such as increasing returns to scale for capital and learning-by-doing that are endogenously determined instead of the exogenous technological improvement used to explain growth in Solow's model. Another type of endogenous growth models endogenized the process of technological progress by modelling research and development activities by profit-maximizing firms explicitly within the growth models themselves.: 280–308 \n\n\n==== Environmental and climate issues ====\n\nSince the 1970s, various environmental problems have been integrated into growth and other macroeconomic models to study their implications more thoroughly. During the oil crises of the 1970s when scarcity problems of natural resources were high on the public agenda, economists like Joseph Stiglitz and Robert Solow introduced non-renewable resources into neoclassical growth models to study the possibilities of maintaining growth in living standards under these conditions.: 201–39  More recently, the issue of climate change and the possibilities of a sustainable development are examined in so-called integrated assessment models, pioneered by William Nordhaus. In macroeconomic models in environmental economics, the economic system is dependant upon the environment. In this case, the circular flow of income diagram may be replaced by a more complex flow diagram reflecting the input of solar energy, which sustains natural inputs and environmental services which are then used as units of production. Once consumed, natural inputs pass out of the economy as pollution and waste. The potential of an environment to provide services and materials is referred to as an \"environment's source function\", and this function is depleted as resources are consumed or pollution contaminates the resources. The \"sink function\" describes an environment's ability to absorb and render harmless waste and pollution: when waste output exceeds the limit of the sink function, long-term damage occurs.: 8 \n\n\n== Macroeconomic policy ==\nThe division into various time frames of macroeconomic research leads to a parallel division of macroeconomic policies into short-run policies aimed at mitigating the harmful consequences of business cycles (known as stabilization policy) and medium- and long-run policies targeted at improving the structural levels of macroeconomic variables.: 18 \nStabilization policy is usually implemented through two sets of tools: fiscal and monetary policy. Both forms of policy are used to stabilize the economy, i.e. limiting the effects of the business cycle by conducting expansive policy when the economy is in a recession or contractive policy in the case of overheating.\nStructural policies may be labor market policies which aim to change the structural unemployment rate or policies which affect long-run propensities to save, invest, or engage in education or research and development.: 19 \n\n\n=== Monetary policy ===\n\nCentral banks conduct monetary policy mainly by adjusting short-term interest rates. The actual method through which the interest rate is changed differs from central bank to central bank, but typically the implementation happens either directly via administratively changing the central bank's own offered interest rates or indirectly via open market operations.\nVia the monetary transmission mechanism, interest rate changes affect investment, consumption, asset prices like stock prices and house prices, and through exchange rate reactions export and import. In this way aggregate demand, employment and ultimately inflation is affected. Expansionary monetary policy lowers interest rates, increasing economic activity, whereas contractionary monetary policy raises interest rates. In the case of a fixed exchange rate system, interest rate decisions together with direct intervention by central banks on exchange rate dynamics are major tools to control the exchange rate.\nIn developed countries, most central banks follow inflation targeting, focusing on keeping medium-term inflation close to an explicit target, say 2%, or within an explicit range. This includes the Federal Reserve and the European Central Bank, which are generally considered to follow a strategy very close to inflation targeting, even though they do not officially label themselves as inflation targeters. In practice, an official inflation targeting often leaves room for the central bank to also help stabilize output and employment, a strategy known as \"flexible inflation targeting\". Most emerging economies focus their monetary policy on maintaining a fixed exchange rate regime, aligning their currency with one or more foreign currencies, typically the US dollar or the euro.\nConventional monetary policy can be ineffective in situations such as a liquidity trap. When nominal interest rates are near zero, central banks cannot loosen monetary policy through conventional means. In that situation, they may use unconventional monetary policy such as quantitative easing to help stabilize output. Quantity easing can be implemented by buying not only government bonds, but also other assets such as corporate bonds, stocks, and other securities. This allows lower interest rates for a broader class of assets beyond government bonds. A similar strategy is to lower long-term interest rates by buying long-term bonds and selling short-term bonds to create a flat yield curve, known in the US as Operation Twist.\n\n\n=== Fiscal policy ===\n\nFiscal policy is the use of government's revenue (taxes) and expenditure as instruments to influence the economy.\nFor example, if the economy is producing less than potential output, government spending can be used to employ idle resources and boost output, or taxes could be lowered to boost private consumption which has a similar effect. Government spending or tax cuts do not have to make up for the entire output gap. There is a multiplier effect that affects the impact of government spending. For instance, when the government pays for a bridge, the project not only adds the value of the bridge to output, but also allows the bridge workers to increase their consumption and investment, which helps to close the output gap.\nThe effects of fiscal policy can be limited by partial or full crowding out. When the government takes on spending projects, it limits the amount of resources available for the private sector to use. Full crowding out occurs in the extreme case when government spending simply replaces private sector output instead of adding additional output to the economy. A crowding out effect may also occur if government spending should lead to higher interest rates, which would limit investment.\nSome fiscal policy is implemented through automatic stabilizers without any active decisions by politicians. Automatic stabilizers do not suffer from the policy lags of discretionary fiscal policy. Automatic stabilizers use conventional fiscal mechanisms, but take effect as soon as the economy takes a downturn: spending on unemployment benefits automatically increases when unemployment rises, and tax revenues decrease, which shelters private income and consumption from part of the fall in market income.: 657 \n\n\n=== Comparison of fiscal and monetary policy ===\nThere is a general consensus that both monetary and fiscal instruments may affect demand and activity in the short run (i.e. over the business cycle).: 657  Economists usually favor monetary over fiscal policy to mitigate moderate fluctuations, however, because it has two major advantages. First, monetary policy is generally implemented by independent central banks instead of the political institutions that control fiscal policy. Independent central banks are less likely to be subject to political pressures for overly expansionary policies. Second, monetary policy may suffer shorter inside lags and outside lags than fiscal policy. There are some exceptions, however: Firstly, in the case of a major shock, monetary stabilization policy may not be sufficient and should be supplemented by active fiscal stabilization.: 659  Secondly, in the case of a very low interest level, the economy may be in a liquidity trap in which monetary policy becomes ineffective, which makes fiscal policy the more potent tool to stabilize the economy. Thirdly, in regimes where monetary policy is tied to fulfilling other targets, in particular fixed exchange rate regimes, the central bank cannot simultaneously adjust its interest rates to mitigate domestic business cycle fluctuations, making fiscal policy the only usable tool for such countries.\n\n\n== Macroeconomic models ==\n\nMacroeconomic teaching, research and informed debates normally evolve around formal (diagrammatic or equational) macroeconomic models to clarify assumptions and show their consequences in a precise way. Models include simple theoretical models, often containing only a few equations, used in teaching and research to highlight key basic principles, and larger applied quantitative models used by e.g. governments, central banks, think tanks and international organisations to predict effects of changes in economic policy or other exogenous factors or as a basis for making economic forecasting.\nWell-known specific theoretical models include short-term models like the Keynesian cross, the IS–LM model and the Mundell–Fleming model, medium-term models like the AD–AS model, building upon a Phillips curve, and long-term growth models like the Solow–Swan model, the Ramsey–Cass–Koopmans model and Peter Diamond's overlapping generations model. Quantitative models include early large-scale macroeconometric model, the new classical real business cycle models, microfounded computable general equilibrium (CGE) models used for medium-term (structural) questions like international trade or tax reforms, Dynamic stochastic general equilibrium (DSGE) models used to analyze business cycles, not least in many central banks, or integrated assessment models like DICE.\n\n\n=== Specific models ===\n\n\n==== IS–LM model ====\n\nThe IS–LM model, invented by John Hicks in 1936, gives the underpinnings of aggregate demand (itself discussed below). It answers the question \"At any given price level, what is the quantity of goods demanded?\" The graphic model shows combinations of interest rates and output that ensure equilibrium in both the goods and money markets under the model's assumptions. The goods market is modeled as giving equality between investment and public and private saving (IS), and the money market is modeled as giving equilibrium between the money supply and liquidity preference (equivalent to money demand).\nThe IS curve consists of the points (combinations of income and interest rate) where investment, given the interest rate, is equal to public and private saving, given output. The IS curve is downward sloping because output and the interest rate have an inverse relationship in the goods market: as output increases, more income is saved, which means interest rates must be lower to spur enough investment to match saving.\nThe traditional LM curve is upward sloping because the interest rate and output have a positive relationship in the money market: as income (identically equal to output in a closed economy) increases, the demand for money increases, resulting in a rise in the interest rate in order to just offset the incipient rise in money demand.\nThe IS-LM model is often used in elementary textbooks to demonstrate the effects of monetary and fiscal policy, though it ignores many complexities of most modern macroeconomic models. A problem related to the LM curve is that modern central banks largely ignore the money supply in determining policy, contrary to the model's basic assumptions.: 262  In some modern textbooks, consequently, the traditional IS-LM model has been modified by replacing the traditional LM curve with an assumption that the central bank simply determines the interest rate of the economy directly.: 194 : 113 \n\n\n==== AD-AS model ====\n\nThe AD–AS model is a common textbook model for explaining the macroeconomy. The original version of the model shows the price level and level of real output given the equilibrium in aggregate demand and aggregate supply. The aggregate demand curve's downward slope means that more output is demanded at lower price levels. The downward slope can be explained as the result of three effects: the Pigou or real balance effect, which states that as real prices fall, real wealth increases, resulting in higher consumer demand of goods; the Keynes or interest rate effect, which states that as prices fall, the demand for money decreases, causing interest rates to decline and borrowing for investment and consumption to increase; and the net export effect, which states that as prices rise, domestic goods become comparatively more expensive to foreign consumers, leading to a decline in exports.\nIn many representations of the AD–AS model, the aggregate supply curve is horizontal at low levels of output and becomes inelastic near the point of potential output, which corresponds with full employment. Since the economy cannot produce beyond the potential output, any AD expansion will lead to higher price levels instead of higher output.\nIn modern textbooks, the AD–AS model is often presented slightly differently, however, in a diagram showing not the price level, but the inflation rate along the vertical axis,: 263 : 399–428 : 595  making it easier to relate the diagram to real-world policy discussions.: vii  In this framework, the AD curve is downward sloping because higher inflation will cause the central bank, which is assumed to follow an inflation target, to raise the interest rate which will dampen economic activity, hence reducing output. The AS curve is upward sloping following a standard modern Phillips curve thought, in which a higher level of economic activity lowers unemployment, leading to higher wage growth and in turn higher inflation.: 263 \n\n\n== Real-Life Applications & Data ==\n\n\n=== Trump's Proposed Tariff Policy in Feb 2025 ===\nIn early February of 2025, United States of America President Donald Trump stated that he would be imposing a 25% tariff on imported goods from Mexico and Canada and a 10% tariff on imported goods from China for US consumers. A tariff in this case is essentially a tax on imported goods and services. US consumers will be less likely to buy imports from those three countries due to the higher price they would have to pay. This was projected to reduce US imports by 15% and generate federal revenue of $100 billion.\nWhile imports from Mexico and Canada are important to the US, the US does not rely as much on Canadian and Mexican imports compared to Mexico's and Canada's economies being highly reliant on their exports to the USA. There would be higher production and grocery costs for the US but Mexico would have its economy reduced by 16% as the US takes in 80% of their car exports and 60% of their petroleum exports. In addition, Canada would have its economy reduced by similar amounts as the US takes in 70% of all of their exports.\nGoing back to our earlier expenditure approach to calculating GDP, we can see that (Exports - Imports) would reduce significantly due to reduced exports which means a negative Net Exports and a lower GDP. Hence, we see the projections showing that Canada's and Mexico's economies would be reduced greatly.\n\n\n=== GDP Deflator Data Overview ===\nWhen looking at data presented by the Bureau of Economic Analysis, with a base year of 2017, we see that the GDP deflator has been trending upwards since then. The base year serves as the standard year to which we can compare whether GDP increased or decreased. The base year's prices are used when calculating Real GDP for a specific year. For instance, calculating 2020's GDP Deflator would be = 2020's Nominal GDP/2020's Real GDP(Using 2017 Prices). The GDP Deflator has risen from 100 to 126.22 in 2024 Q4. So we see with real-life data that there has been a lot of inflation over the past decade. This trend was followed during the COVID-19 pandemic as well.  \n\n\n== See also ==\n\nMicroeconomics\nBusiness cycle accounting\nEconomic development\nGrowth accounting\n\n\n== Notes ==\n\n\n== References ==\n\nBlanchard, Olivier. (2009). \"The State of Macro.\" Annual Review of Economics 1(1): 209–228.\nBlanchard, Olivier (2021). Macroeconomics (Eighth, global ed.). Harlow, England: Pearson. ISBN 978-0-134-89789-9.\nBlaug, Mark (2002). \"Endogenous growth theory\". In Snowdon, Brian; Vane, Howard (eds.). An Encyclopedia of Macroeconomics. Northampton, Massachusetts: Edward Elgar Publishing. ISBN 978-1-84542-180-9.\nDimand, Robert W. (2008). \"Macroeconomics, origins and history of\". In Durlauf, Steven N.; Blume, Lawrence E. (eds.). The New Palgrave Dictionary of Economics. Palgrave Macmillan UK. pp. 236–44. doi:10.1057/9780230226203.1009. ISBN 978-0-333-78676-5.\nDurlauf, Steven N.; Hester, Donald D. (2008). \"IS–LM\". In Durlauf, Steven N.; Blume, Lawrence E. (eds.). The New Palgrave Dictionary of Economics (2nd ed.). Palgrave Macmillan. pp. 585–91. doi:10.1057/9780230226203.0855. ISBN 978-0-333-78676-5.\nDwivedi, D.N. (2001). Macroeconomics: theory and policy. New Delhi: Tata McGraw-Hill. ISBN 978-0-07-058841-7.\nGärtner, Manfred (2006). Macroeconomics. Pearson Education Limited. ISBN 978-0-273-70460-7.\nHealey, Nigel M. (2002). \"AD-AS model\". In Snowdon, Brian; Vane, Howard (eds.). An Encyclopedia of Macroeconomics. Northampton, Massachusetts: Edward Elgar Publishing. pp. 11–18. ISBN 978-1-84542-180-9.\nLevi, Maurice (2014). The Macroeconomic Environment of Business (Core Concepts and Curious Connections). New Jersey: World Scientific Publishing. ISBN 978-981-4304-34-4.\nMankiw, Nicholas Gregory (2022). Macroeconomics (Eleventh, international ed.). New York, NY: Worth Publishers, Macmillan Learning. ISBN 978-1-319-26390-4.\nMayer, Thomas (2002). \"Monetary policy: role of\". In Snowdon, Brian; Vane, Howard R. (eds.). An Encyclopedia of Macroeconomics. Northampton, Massachusetts: Edward Elgar Publishing. pp. 495–99. ISBN 978-1-84542-180-9.\nNakamura, Emi and Jón Steinsson. (2018). \"Identification in Macroeconomics.\" Journal of Economic Perspectives 32(3): 59–86.\nPeston, Maurice (2002). \"IS-LM model: closed economy\". In Snowdon, Brian; Vane, Howard R. (eds.). An Encyclopedia of Macroeconomics. Edward Elgar. ISBN 9781840643879.\nRomer, David (2019). Advanced macroeconomics (Fifth ed.). New York, NY: McGraw-Hill. ISBN 978-1-260-18521-8.\nSolow, Robert (2002). \"Neoclassical growth model\". In Snowdon, Brian; Vane, Howard (eds.). An Encyclopedia of Macroeconomics. Northampton, Massachusetts: Edward Elgar Publishing. ISBN 1840643870.\nSnowdon, Brian, and Howard R. Vane, ed. (2002). An Encyclopedia of Macroeconomics, Description & scroll to Contents-preview links.\nSnowdon, Brian; Vane, Howard R. (2005). Modern Macroeconomics: Its Origins, Development And Current State. Edward Elgar Publishing. ISBN 1845421809.\nSørensen, Peter Birch; Whitta-Jacobsen, Hans Jørgen (2022). Introducing advanced macroeconomics: growth and business cycles (Third ed.). Oxford, United Kingdom New York, NY: Oxford University Press. ISBN 978-0-19-885049-6.\nWarsh, David (2006). Knowledge and the Wealth of Nations. Norton. ISBN 978-0-393-05996-0.\n\n\n== Further reading ==\nMacroeconomic Modeling: The Cowles Commission Approach by Ray C. Fair"
    },
    {
        "id": "organic_chemistry",
        "name": "Organic chemistry",
        "text": "Inorganic chemistry deals with synthesis and behavior of inorganic and organometallic compounds. This field covers chemical compounds that are not carbon-based, which are the subjects of organic chemistry. The distinction between the two disciplines is far from absolute, as there is much overlap in the subdiscipline of organometallic chemistry. It has applications in every aspect of the chemical industry, including catalysis, materials science, pigments, surfactants, coatings, medications, fuels, and agriculture.\n\n\n== Occurrence ==\nMany inorganic compounds are found in nature as minerals. Soil may contain iron sulfide as pyrite or calcium sulfate as gypsum. Inorganic compounds are also found multitasking as biomolecules: as electrolytes (sodium chloride), in energy storage (ATP) or in construction (the polyphosphate backbone in DNA).\n\n\n== Bonding ==\nInorganic compounds exhibit a range of bonding properties.  Some are ionic compounds, consisting of very simple cations and anions joined by ionic bonding. Examples of salts (which are ionic compounds) are magnesium chloride MgCl2, which consists of magnesium cations Mg2+ and chloride anions Cl−; or sodium hydroxide NaOH, which consists of sodium cations Na+ and hydroxide anions OH−.  Some inorganic compounds are highly covalent, such as sulfur dioxide and iron pentacarbonyl.  Many inorganic compounds feature polar covalent bonding, which is a form of bonding intermediate between covalent and ionic bonding.  This description applies to many oxides, carbonates, and halides.  Many inorganic compounds are characterized by high melting points.  Some salts (e.g., NaCl) are very soluble in water.\nWhen one reactant contains hydrogen atoms, a reaction can take place by exchanging protons in acid-base chemistry. In a more general definition, any chemical species capable of binding to electron pairs is called a Lewis acid; conversely any molecule that tends to donate an electron pair is referred to as a Lewis base. As a refinement of acid-base interactions, the HSAB theory takes into account polarizability and size of ions.\n\n\n== Subdivisions of inorganic chemistry ==\nSubdivisions of inorganic chemistry are numerous, but include:\n\norganometallic chemistry, compounds with metal-carbon bonds. This area touches on organic synthesis, which employs many organometallic catalysts and reagents.\ncluster chemistry, compounds with several metals bound together with metal–metal bonds or bridging ligands.\nbioinorganic chemistry, biomolecules that contain metals.  This area touches on medicinal chemistry.\nmaterials chemistry and solid state chemistry, extended (i.e. polymeric) solids exhibiting properties not seen for simple molecules.  Many practical themes are associated with these areas, including ceramics.\n\n\n=== Industrial inorganic chemistry ===\nInorganic chemistry is a highly practical area of science.  Traditionally, the scale of a nation's economy could be evaluated by their productivity of sulfuric acid.\nAn important man-made inorganic compound is ammonium nitrate, used for fertilization.  The ammonia is produced through the Haber process.  Nitric acid is prepared from the ammonia by oxidation.  Another large-scale inorganic material is portland cement. Inorganic compounds are used as catalysts such as vanadium(V) oxide for the oxidation of sulfur dioxide and titanium(III) chloride for the polymerization of alkenes. Many inorganic compounds are used as reagents in organic chemistry such as lithium aluminium hydride.\n\n\n== Descriptive inorganic chemistry ==\nDescriptive inorganic chemistry focuses on the classification of compounds based on their properties.  Partly the classification focuses on the position in the periodic table of the heaviest element (the element with the highest atomic weight) in the compound, partly by grouping compounds by their structural similarities\n\n\n=== Coordination compounds ===\n\nClassical coordination compounds feature metals bound to \"lone pairs\" of electrons residing on the main group atoms of ligands such as H2O, NH3, Cl−, and CN−.  In modern coordination compounds almost all organic and inorganic compounds can be used as ligands.  The \"metal\" usually is a metal from the groups 3–13, as well as the trans-lanthanides and trans-actinides, but from a certain perspective, all chemical compounds can be described as coordination complexes.\nThe stereochemistry of coordination complexes can be quite rich, as hinted at by Werner's separation of two enantiomers of [Co((OH)2Co(NH3)4)3]6+, an early demonstration that chirality is not inherent to organic compounds.  A topical theme within this specialization is supramolecular coordination chemistry.\n\nExamples: [Co(EDTA)]−, [Co(NH3)6]3+, TiCl4(THF)2.\nCoordination compounds show a rich diversity of structures, varying from tetrahedral for titanium (e.g., TiCl4) to square planar for some nickel complexes to octahedral for coordination complexes of cobalt.  A range of transition metals can be found in biologically important compounds, such as iron in hemoglobin.\n\nExamples: iron pentacarbonyl, titanium tetrachloride, cisplatin\n\n\n=== Main group compounds ===\n\nThese species feature elements from groups I, II, III, IV, V, VI, VII, 0 (excluding hydrogen) of the periodic table.  Due to their often similar reactivity, the elements in group 3 (Sc, Y, and La) and group 12 (Zn, Cd, and Hg) are also generally included, and the lanthanides and actinides are sometimes included as well.\nMain group compounds have been known since the beginnings of chemistry, e.g., elemental sulfur and the distillable white phosphorus.  Experiments on oxygen, O2, by Lavoisier and Priestley not only identified an important diatomic gas, but opened the way for describing compounds and reactions according to stoichiometric ratios.  The discovery of a practical synthesis of ammonia using iron catalysts by Carl Bosch and Fritz Haber in the early 1900s deeply impacted mankind, demonstrating the significance of inorganic chemical synthesis.\nTypical main group compounds are SiO2, SnCl4, and N2O.  Many main group compounds can also be classed as \"organometallic\", as they contain organic groups, e.g., B(CH3)3).  Main group compounds also occur in nature, e.g., phosphate in DNA, and therefore may be classed as bioinorganic.  Conversely, organic compounds lacking (many) hydrogen ligands can be classed as \"inorganic\", such as the fullerenes, buckytubes and binary carbon oxides.\n\nExamples: tetrasulfur tetranitride S4N4, diborane B2H6, silicones, buckminsterfullerene C60.\nNoble gas compounds include several derivatives of xenon and krypton.\n\nExamples: xenon hexafluoride XeF6, xenon trioxide XeO3, and krypton difluoride KrF2\n\n\n=== Organometallic compounds ===\n\nUsually, organometallic compounds are considered to contain the M-C-H group.  The metal (M) in these species can either be a main group element or a transition metal.  Operationally, the definition of an organometallic compound is more relaxed to include also highly lipophilic complexes such as metal carbonyls and even metal alkoxides.\nOrganometallic compounds are mainly considered a special category because organic ligands are often sensitive to hydrolysis or oxidation, necessitating that organometallic chemistry employs more specialized preparative methods than was traditional in Werner-type complexes.  Synthetic methodology, especially the ability to manipulate complexes in solvents of low coordinating power, enabled the exploration of very weakly coordinating ligands such as hydrocarbons, H2, and N2.  Because the ligands are petrochemicals in some sense, the area of organometallic chemistry has greatly benefited from its relevance to industry.\n\nExamples: Cyclopentadienyliron dicarbonyl dimer (C5H5)Fe(CO)2CH3, ferrocene Fe(C5H5)2, molybdenum hexacarbonyl Mo(CO)6, triethylborane Et3B, Tris(dibenzylideneacetone)dipalladium(0) Pd2(dba)3)\n\n\n=== Cluster compounds ===\n\nClusters can be found in all classes of chemical compounds.  According to the commonly accepted definition, a cluster consists minimally of a triangular set of atoms that are directly bonded to each other.  But metal–metal bonded dimetallic complexes are highly relevant to the area.  Clusters occur in \"pure\" inorganic systems, organometallic chemistry, main group chemistry, and bioinorganic chemistry.  The distinction between very large clusters and bulk solids is increasingly blurred.  This interface is the chemical basis of nanoscience or nanotechnology and specifically arise from the study of quantum size effects in cadmium selenide clusters. Thus, large clusters can be described as an array of bound atoms intermediate in character between a molecule and a solid.\n\nExamples: Fe3(CO)12, B10H14, [Mo6Cl14]2−, 4Fe-4S\n\n\n=== Bioinorganic compounds ===\n\nBy definition, these compounds occur in nature, but the subfield includes anthropogenic species, such as pollutants (e.g., methylmercury) and drugs (e.g., Cisplatin).  The field, which incorporates many aspects of biochemistry, includes many kinds of compounds, e.g., the phosphates in DNA, and also metal complexes containing ligands that range from biological macromolecules, commonly peptides, to ill-defined species such as humic acid, and to water (e.g., coordinated to gadolinium complexes employed for MRI).  Traditionally bioinorganic chemistry focuses on electron- and energy-transfer in proteins relevant to respiration.  Medicinal inorganic chemistry includes the study of both non-essential and essential elements with applications to diagnosis and therapies.\n\nExamples: hemoglobin, methylmercury, carboxypeptidase\n\n\n=== Solid state compounds ===\n\nThis important area focuses on structure, bonding, and the physical properties of materials.  In practice, solid state inorganic chemistry uses techniques such as crystallography to gain an understanding of the properties that result from collective interactions between the subunits of the solid.  Included in solid state chemistry are metals and their alloys or intermetallic derivatives.  Related fields are condensed matter physics, mineralogy, and materials science.\n\nExamples: silicon chips, zeolites, YBa2Cu3O7\n\n\n== Spectroscopy and magnetism ==\nIn contrast to most organic compounds, many inorganic compounds are magnetic and/or colored. These properties provide information on the bonding and structure.  The magnetism of inorganic compounds can be comlex. For example, most copper(II) compounds are paramagnetic but CuII2(OAc)4(H2O)2 is almost diamagnetic below room temperature.  The explanation is due to magnetic coupling between pairs of Cu(II) sites in the acetate.\n\n\n=== Qualitative theories ===\n\nInorganic chemistry has greatly benefited from qualitative theories.  Such theories are easier to learn as they require little background in quantum theory.  Within main group compounds, VSEPR theory powerfully predicts, or at least rationalizes, the structures of main group compounds, such as an explanation for why NH3 is pyramidal whereas ClF3 is T-shaped.  For the transition metals, crystal field theory allows one to understand the magnetism of many simple complexes, such as why [FeIII(CN)6]3− has only one unpaired electron, whereas [FeIII(H2O)6]3+ has five.  A particularly powerful qualitative approach to assessing the structure and reactivity begins with classifying molecules according to electron counting, focusing on the numbers of valence electrons, usually at the central atom in a molecule.\n\n\n=== Molecular symmetry group theory ===\n\nA construct in chemistry is molecular symmetry, as embodied in Group theory. Inorganic compounds display a particularly diverse symmetries, so it is logical that Group Theory is intimately associated with inorganic chemistry. Group theory provides the language to describe the shapes of molecules according to their point group symmetry.  Group theory also enables factoring and simplification of theoretical calculations.\nSpectroscopic features are analyzed and described with respect to the symmetry properties of the, inter alia, vibrational or electronic states. Knowledge of the symmetry properties of the ground and excited states allows one to predict the numbers and intensities of absorptions in vibrational and electronic spectra.  A classic application of group theory is the prediction of the number of C–O vibrations in substituted metal carbonyl complexes.  The most common applications of symmetry to spectroscopy involve vibrational and electronic spectra.\nGroup theory highlights commonalities and differences in the bonding of otherwise disparate species.  For example, the metal-based orbitals transform identically for WF6 and W(CO)6, but the energies and populations of these orbitals differ significantly.  A similar relationship exists CO2 and molecular beryllium difluoride.\n\n\n== Thermodynamics and inorganic chemistry ==\nAn alternative quantitative approach to inorganic chemistry focuses on energies of reactions.  This approach is highly traditional and empirical, but it is also useful.  Broad concepts that are couched in thermodynamic terms include redox potential, acidity, phase changes.  A classic concept in inorganic thermodynamics is the Born–Haber cycle, which is used for assessing the energies of elementary processes such as electron affinity, some of which cannot be observed directly.\n\n\n== Mechanistic inorganic chemistry ==\nAn important aspect of inorganic chemistry focuses on reaction pathways, i.e. reaction mechanisms.\n\n\n=== Main group elements and lanthanides ===\nThe mechanisms of main group compounds of groups 13–18 are usually discussed in the context of organic chemistry (organic compounds are main group compounds, after all).  Elements heavier than C, N, O, and F often form compounds with more electrons than predicted by the octet rule, as explained in the article on hypervalent molecules.  The mechanisms of their reactions differ from organic compounds for this reason.  Elements lighter than carbon (B, Be, Li) as well as Al and Mg often form electron-deficient structures that are electronically akin to carbocations.  Such electron-deficient species tend to react via associative pathways.  The chemistry of the lanthanides mirrors many aspects of chemistry seen for aluminium.\n\n\n=== Transition metal complexes ===\nTransition metal and main group compounds often react differently. The important role of d-orbitals in bonding strongly influences the pathways and rates of ligand substitution and dissociation.  These themes are covered in articles on coordination chemistry and ligand.  Both associative and dissociative pathways are observed.\nAn overarching aspect of mechanistic transition metal chemistry is the kinetic lability of the complex illustrated by the exchange of free and bound water in the prototypical complexes [M(H2O)6]n+:\n\n[M(H2O)6]n+  + 6 H2O*  →  [M(H2O*)6]n+  +  6 H2O\nwhere H2O* denotes isotopically enriched water, e.g., H217O\nThe rates of water exchange varies by 20 orders of magnitude across the periodic table, with lanthanide complexes at one extreme and Ir(III) species being the slowest.\n\n\n==== Redox reactions ====\nRedox reactions are prevalent for the transition elements.  Two classes of redox reaction are considered: atom-transfer reactions, such as oxidative addition/reductive elimination, and electron-transfer.  A fundamental redox reaction is \"self-exchange\", which involves the degenerate reaction between an oxidant and a reductant.  For example, permanganate and its one-electron reduced relative manganate exchange one electron:\n\n[MnO4]− +  [Mn*O4]2−  →  [MnO4]2− +  [Mn*O4]−\n\n\n==== Reactions at ligands ====\nCoordinated ligands display reactivity distinct from the free ligands.  For example, the acidity of the ammonia ligands in [Co(NH3)6]3+ is elevated relative to NH3 itself.  Alkenes bound to metal cations are reactive toward nucleophiles whereas alkenes normally are not.  The large and industrially important area of catalysis hinges on the ability of metals to modify the reactivity of organic ligands.  Homogeneous catalysis occurs in solution and heterogeneous catalysis occurs when gaseous or dissolved substrates interact with surfaces of solids.  Traditionally homogeneous catalysis is considered part of organometallic chemistry and heterogeneous catalysis is discussed in the context of surface science, a subfield of solid state chemistry.  But the basic inorganic chemical principles are the same.  Transition metals, almost uniquely, react with small molecules such as CO, H2, O2, and C2H4.  The industrial significance of these feedstocks drives the active area of catalysis.  Ligands can also undergo ligand transfer reactions such as transmetalation.\n\n\n== Characterization of inorganic compounds ==\nBecause of the diverse range of elements and the correspondingly diverse properties of the resulting derivatives, inorganic chemistry is closely associated with many methods of analysis.  Older methods tended to examine bulk properties such as the electrical conductivity of solutions, melting points, solubility, and acidity.  With the advent of quantum theory and the corresponding expansion of electronic apparatus, new tools have been introduced to probe the electronic properties of inorganic molecules and solids.  Often these measurements provide insights relevant to theoretical models. Commonly encountered techniques are:\n\nX-ray crystallography: This technique allows for the 3D determination of molecular structures.\nVarious forms of spectroscopy:\nUltraviolet-visible spectroscopy: Historically, this has been an important tool, since many inorganic compounds are strongly colored\nNMR spectroscopy: Besides 1H and 13C many other NMR-active nuclei (e.g., 11B, 19F, 31P, and 195Pt) can give important information on compound properties and structure.  The NMR of paramagnetic species can provide important structural information. Proton (1H) NMR is also important because the light hydrogen nucleus is not easily detected by X-ray crystallography.\nInfrared spectroscopy: Mostly for absorptions from carbonyl ligands\nElectron nuclear double resonance (ENDOR) spectroscopy\nMössbauer spectroscopy\nElectron-spin resonance: ESR (or EPR) allows for the measurement of the environment of paramagnetic metal centres.\nElectrochemistry: Cyclic voltammetry and related techniques probe the redox characteristics of compounds.\n\n\n== Synthetic inorganic chemistry ==\nAlthough some inorganic species can be obtained in pure form from nature, most are synthesized in chemical plants and in the laboratory.\nInorganic synthetic methods can be classified roughly according to the volatility or solubility of the component reactants.  Soluble inorganic compounds are prepared using methods of organic synthesis.  For metal-containing compounds that are reactive toward air, Schlenk line and glove box techniques are followed. Volatile compounds and gases are manipulated in \"vacuum manifolds\" consisting of glass piping interconnected through valves, the entirety of which can be evacuated to 0.001 mm Hg or less. Compounds are condensed using liquid nitrogen (b.p. 78K) or other cryogens. Solids are typically prepared using tube furnaces, the reactants and products being sealed in containers, often made of fused silica (amorphous SiO2) but sometimes more specialized materials such as welded Ta tubes or Pt \"boats\". Products and reactants are transported between temperature zones to drive reactions.\n\n\n== See also ==\nImportant publications in inorganic chemistry\n\n\n== References =="
    },
    {
        "id": "french_language",
        "name": "French language",
        "text": "French (français [fʁɑ̃sɛ]  or langue française [lɑ̃ɡ fʁɑ̃sɛːz] ) is a Romance language of the Indo-European family. Like all other Romance languages, it descended from the Vulgar Latin of the Roman Empire. French evolved from Northern Old Gallo-Romance, a descendant of the Latin spoken in Northern Gaul. Its closest relatives are the other langues d'oïl—languages historically spoken in northern France and in southern Belgium, which French (Francien) largely supplanted. It was also influenced by native Celtic languages of Northern Roman Gaul and by the Germanic Frankish language of the post-Roman Frankish invaders. As a result of French and Belgian colonialism from the 16th century onward, it was introduced to new territories in the Americas, Africa, and Asia, and numerous French-based creole languages, most notably Haitian Creole, were established. A French-speaking person or nation may be referred to as Francophone in both English and French.\nFrench is an official language in 26 countries, as well as one of the most geographically widespread languages in the world, with about 50 countries and territories having it as a de jure or de facto official, administrative, or cultural language. Most of these countries are members of the Organisation internationale de la Francophonie (OIF), the community of 54 member states which share the official use or teaching of French. It is estimated to have about 310 million speakers, of which about 80 million are native speakers; it is spoken as a first language (in descending order of the number of speakers) in France, Canada (provinces of Quebec, Ontario, and New Brunswick), Belgium (Wallonia and the Brussels-Capital Region), western Switzerland (Romandy region), parts of Luxembourg, parts of the United States (Louisiana, Maine, New Hampshire, and Vermont), Monaco, the Aosta Valley region of Italy, and various communities elsewhere.\nIn Francophone Africa it is spoken mainly as a second language or lingua franca, though it has also become a native language in a small number of urban areas; in some North African countries, despite not having official status, it is also a first language among some upper classes of the population alongside the indigenous ones, but only a second one among the general population.\nIn 2015, approximately 40% of the Francophone population (including L2 and partial speakers) lived in Europe, 36% in sub-Saharan Africa and the Indian Ocean, 15% in North Africa and the Middle East, 8% in the Americas, and 1% in Asia and Oceania. French is the second most widely spoken mother tongue in the European Union. Of Europeans who speak other languages natively, approximately one-fifth are able to speak French as a second language. Many institutions of the EU use French as a working language along with English, German and Italian; in some institutions, French is the sole working language (e.g. at the Court of Justice of the European Union). French is also the 16th most natively spoken language in the world, the sixth most spoken language by total number of speakers, and is among the top five most studied languages worldwide, with about 120 million learners as of 2017.  French has a long history as an international language of literature and scientific standards and is a primary or second language of many international organisations including the United Nations, the European Union, the North Atlantic Treaty Organization, the World Trade Organization, the International Olympic Committee, the General Conference on Weights and Measures, and the International Committee of the Red Cross.\n\n\n== History ==\n\nFrench is a Romance language (meaning that it is descended primarily from Vulgar Latin) that evolved out of the Gallo-Romance dialects spoken in northern France. The language's early forms include Old French and Middle French.\n\n\n=== Vulgar Latin in Gaul ===\nDue to Roman rule, Latin was gradually adopted by the inhabitants of Gaul. As the language was learned by the common people, it developed a distinct local character, with grammatical differences from Latin as spoken elsewhere, some of which is attested in graffiti. This local variety evolved into the Gallo-Romance tongues, which include French and its closest relatives, such as Arpitan.\nThe evolution of Latin in Gaul was shaped by its coexistence for over half a millennium beside the native Celtic Gaulish language, which did not go extinct until the late sixth century, long after the fall of the Western Roman Empire. Because few Latin speakers settled in rural areas during Roman times, Latin there held little or no social value for the peasantry; as a result, 90% of the total population of Gaul remained indigenous in origin. The urban aristocracy, who used Latin for trade, education or official uses, would send their children to Roman schools and administered lands for Rome. In the fifth century, at the time of the collapse of the Western Roman Empire, the vast majority of the (predominantly rural) population remained Gaulish speakers. They shifted to Latin as their native speech only one century after the Frankish conquest of Gaul, adopting the prestige language of their urban literate elite. This eventual spread of Latin can be attributed to the social migration from the focus of urban power to village-centred economies and legal serfdom.\nThe Gaulish language likely survived into the sixth century in France despite considerable Romanization. Coexisting with Latin, Gaulish helped shape the Vulgar Latin dialects that developed into French contributing loanwords and calques (including oui, the word for \"yes\"), sound changes shaped by Gaulish influence, and influences in conjugation and word order. Recent computational studies suggest that early gender shifts may have been motivated by the gender of the corresponding word in Gaulish.\nThe estimated number of French words that can be attributed to Gaulish is placed at 154 by the Petit Robert, which is often viewed as representing standardized French, while if non-standard dialects are included, the number increases to 240. Known Gaulish loans are skewed toward certain semantic fields, such as plant life (chêne, bille, etc.), animals (mouton, cheval, etc.), nature (boue, etc.), domestic activities (ex. berceau), farming and rural units of measure (arpent, lieue, borne, boisseau), weapons, and products traded regionally rather than further afield. This semantic distribution has been attributed to peasants being the last to hold onto Gaulish.\n\n\n=== Old French ===\nThe beginning of French in Gaul was greatly influenced by Germanic invasions into the country. These invasions had the greatest impact on the northern part of the country and on the language there. A language divide began to grow across the country. The population in the north spoke langue d'oïl while the population in the south spoke langue d'oc. Langue d'oïl grew into what is known as Old French. The period of Old French spanned between the late 8th and mid-14th centuries. Old French shared many characteristics with Latin. For example, Old French made use of different possible word orders just as Latin did because it had a case system that retained the difference between nominative subjects and oblique non-subjects. The period is marked by a heavy superstrate influence from the Germanic Frankish language, which non-exhaustively included the use in upper-class speech and higher registers of V2 word order, a large percentage of the vocabulary (now at around 15% of modern French vocabulary) including the impersonal singular pronoun on (a calque of Germanic man), and the name of the language itself.\nUp until its later stages, Old French, alongside Old Occitan, maintained a relic of the old nominal case system of Latin longer than most other Romance languages (with the notable exception of Romanian which still currently maintains a case distinction), differentiating between an oblique case and a nominative case. The phonology was characterized by heavy syllabic stress, which led to the emergence of various complicated diphthongs such as -eau which would later be leveled to monophthongs.\nThe earliest evidence of what became Old French can be seen in the Oaths of Strasbourg and the Sequence of Saint Eulalia, while Old French literature began to be produced in the eleventh century, with major early works often focusing on the lives of saints (such as the Vie de Saint Alexis), or wars and royal courts, notably including the Chanson de Roland, epic cycles focused on King Arthur and his court, as well as a cycle focused on William of Orange.\nDuring the period of the Crusades French became so dominant in the Mediterranean Sea that it became a lingua franca (\"Frankish language\"), and because of increased contact with the Arabs during the Crusades, who referred to them as Franj, numerous Arabic loanwords entered French, such as amiral (admiral), alcool (alcohol), coton (cotton) and sirop (syrop), as well as scientific terms such as algébre (algebra), alchimie (alchemy) and zéro (zero).\n\n\n=== Middle French ===\n\nWithin Old French many dialects emerged but the Francien dialect is one that not only continued but also thrived during the Middle French period (14th–17th centuries). Modern French grew out of this Francien dialect. Grammatically, during the period of Middle French, noun declensions were lost and there began to be standardized rules. Robert Estienne published the first Latin-French dictionary, which included information about phonetics, etymology, and grammar. Politically, the first government authority to adopt Modern French as official was the Aosta Valley in 1536, while the Ordinance of Villers-Cotterêts (1539) named French the language of law in the Kingdom of France.\n\n\n=== Modern French ===\nDuring the 17th century, French replaced Latin as the most important language of diplomacy and international relations (lingua franca). It retained this role until approximately the middle of the 20th century, when it was replaced by English as the United States became the dominant global power following the Second World War. Stanley Meisler of the Los Angeles Times said that the fact that the Treaty of Versailles was written in English as well as French was the \"first diplomatic blow\" against the language.\nDuring the Grand Siècle (17th century), France, under the rule of powerful leaders such as Cardinal Richelieu and Louis XIV, enjoyed a period of prosperity and prominence among European nations. Richelieu established the Académie française to protect the French language. By the early 1800s, Parisian French had become the primary language of the aristocracy in France.\nNear the beginning of the 19th century, the French government began to pursue policies with the end goal of eradicating the many minorities and regional languages (patois) spoken in France. This began in 1794 with Henri Grégoire's \"Report on the necessity and means to annihilate the patois and to universalize the use of the French language\". When public education was made compulsory, only French was taught and the use of any other (patois) language was punished. The goals of the public school system were made especially clear to the French-speaking teachers sent to teach students in regions such as Occitania and Brittany. Instructions given by a French official to teachers in the department of Finistère, in western Brittany, included the following: \"And remember, Gents: you were given your position in order to kill the Breton language\". The prefect of Basses-Pyrénées in the French Basque Country wrote in 1846: \"Our schools in the Basque Country are particularly meant to replace the Basque language with French...\" Students were taught that their ancestral languages were inferior and they should be ashamed of them; this process was known in the Occitan-speaking region as Vergonha.\n\n\n== Geographic distribution ==\n\n\n=== Europe ===\n\nSpoken by 19.71% of the European Union's population, French is the third most widely spoken language in the EU, after English and German and the second-most-widely taught language after English.\nUnder the Constitution of France, French has been the official language of the Republic since 1992, although the Ordinance of Villers-Cotterêts made it mandatory for legal documents in 1539. France mandates the use of French in official government publications, public education except in specific cases, and legal contracts; advertisements must bear a translation of foreign words.\nIn Belgium, French is an official language at the federal level along with Dutch and German. At the regional level, French is the sole official language of Wallonia (excluding a part of the East Cantons, which are German-speaking) and one of the two official languages—along with Dutch—of the Brussels-Capital Region, where it is spoken by the majority of the population (approx. 80%), often as their primary language.\nFrench is one of the four official languages of Switzerland, along with German, Italian, and Romansh, and is spoken in the western part of Switzerland, called Romandy, of which Geneva is the largest city. The language divisions in Switzerland do not coincide with political subdivisions, and some cantons have bilingual status: for example, cities such as Biel/Bienne and cantons such as Valais, Fribourg and Bern. French is the native language of about 23% of the Swiss population, and is spoken by 50% of the population.\nAlong with Luxembourgish and German, French is one of the three official languages of Luxembourg, where it is generally the preferred language of business as well as of the different public administrations. It is also the official language of Monaco.\nAt a regional level, French is acknowledged as an official language in the Aosta Valley region of Italy (the first government authority to adopt Modern French as the official language in 1536, three years before France itself), in which is spoken as a first language by 1.25% of the population and as a second one by approximately 50%. French dialects remain spoken by minorities on the Channel Islands; it is also spoken in Andorra and is the main language after Catalan in El Pas de la Casa. The language is taught as the primary second language in the German state of Saarland, with French being taught from pre-school and over 43% of citizens being able to speak French.\n\n\n=== Africa ===\n\nThe majority of the world's French-speaking population lives in Africa; while it is an official language in 18 countries, it is not spoken as a first language by the majority, acting mainly as a second one or a lingua franca due to the many indigenous languages spoken in the territories. According to a 2023 estimate from the Organisation internationale de la Francophonie, an estimated 167 million African people spread across 35 countries and territories can speak French as either a first or a second language. This number does not include the people living in non-Francophone African countries who have learned French as a foreign language. Due to the rise of French in Africa, the total French-speaking population worldwide is expected to reach 700 million people in 2050. French was the fastest growing language on the continent (in terms of either official or foreign languages).\nWhile spoken mainly as a second language, French is increasingly being spoken as a native language in Francophone Africa among some communities in urban areas or the elite class, especially in the cities of Abidjan in Ivory Coast, Kinshasa and Lubumbashi in the Democratic Republic of Congo, Douala in Cameroon, Libreville in Gabon, and Antananarivo in Madagascar. However, some African countries such as Algeria intermittently attempted to remove the use of French in favor of native languages, and it was removed as an official language in Mali, Burkina Faso, and Niger in 2023, 2024, and 2025, respectively.\nThere is not a single African French, but multiple forms that diverged through contact with various indigenous African languages.\nSub-Saharan Africa is the region where the French language is most likely to expand, because of the expansion of education and rapid population growth. It is also where the language has evolved the most in recent years. Some vernacular forms of French in Africa can be difficult to understand for French speakers from other countries, but written forms of the language are very closely related to those of the rest of the French-speaking world.\n\n\n=== Americas ===\n\n\n==== Canada ====\n\nFrench is the second most commonly spoken language in Canada and one of two federal official languages alongside English. As of the 2021 Canadian census, it was the native language of 7.7 million people (21% of the population) and the second language of 2.9 million (8% of the population). French is the sole official language in the province of Quebec, where some 80% of the population speak it as a native language and 95% are capable of conducting a conversation in it. Quebec is also home to the city of Montreal, which is the world's fourth-largest French-speaking city, by number of first language speakers. New Brunswick and Manitoba are the only officially bilingual provinces, though full bilingualism is enacted only in New Brunswick, where about one third of the population is Francophone. French is also an official language of all of the territories (Northwest Territories, Nunavut, and Yukon). Out of the three, Yukon has the most French speakers, making up just under 4% of the population. Furthermore, while French is not an official language in Ontario, the French Language Services Act ensures that provincial services are available in the language. The Act applies to areas of the province where there are significant Francophone communities, namely Eastern Ontario and Northern Ontario. Elsewhere, sizable French-speaking minorities are found in southern Manitoba, Nova Scotia, Prince Edward Island and the Port au Port Peninsula in Newfoundland and Labrador, where the unique Newfoundland French dialect was historically spoken. Smaller pockets of French speakers exist in all other provinces. The Ontarian city of Ottawa, the Canadian capital, is also effectively bilingual, as it has a large population of federal government workers, who are required to offer services in both French and English, and is just across the river from the Quebecois city of Gatineau.\n\n\n==== United States ====\n\nAccording to the United States Census Bureau (2011), French is the fourth most spoken language in the United States after English, Spanish, and Chinese, when all forms of French are considered together and all dialects of Chinese are similarly combined. French is the second-most spoken language (after English) in the states of Maine and New Hampshire. In Louisiana, it is tied with Spanish for second-most spoken if Louisiana French and all creoles such as Haitian are included. French is the third most spoken language (after English and Spanish) in the states of Connecticut, Rhode Island, and New Hampshire. Louisiana is home to many distinct French dialects, collectively known as Louisiana French. New England French, essentially a variant of Canadian French, is spoken in parts of New England. Missouri French was historically spoken in Missouri and Illinois (formerly known as Upper Louisiana), but is nearly extinct today. French also survived in isolated pockets along the Gulf Coast of what was previously French Lower Louisiana, such as Mon Louis Island, Alabama and DeLisle, Mississippi (the latter only being discovered by linguists in the 1990s) but these varieties are severely endangered or presumed extinct.\n\n\n==== Caribbean ====\nFrench is one of two official languages in Haiti alongside Haitian Creole. It is the principal language of education, administration, business, and public signage and is spoken by all educated Haitians. It is also used for ceremonial events such as weddings, graduations, and church masses. The vast majority of the population speaks Haitian Creole as their first language; the rest largely speak French as a first language. As a French Creole language, Haitian Creole draws the large majority of its vocabulary from French, with influences from West African languages, as well as several European languages. It is closely related to Louisiana Creole and the creole from the Lesser Antilles.\nFrench is the sole official language of all the overseas territories of France in the Caribbean that are collectively referred to as the French West Indies, namely Guadeloupe, Saint Barthélemy, Saint Martin, and Martinique.\n\n\n==== Other territories ====\nFrench is the official language of both French Guiana on the South American continent, and of Saint Pierre and Miquelon, an archipelago off the coast of Newfoundland in North America.\n\n\n=== Asia ===\n\n\n==== Southeast Asia ====\n\nFrench was the official language of the colony of French Indochina, comprising modern-day Vietnam, Laos, and Cambodia. It continues to be an administrative language in Laos and Cambodia, although its influence has waned in recent decades. In colonial Vietnam, the elites primarily spoke French, while many servants who worked in French households spoke a French pidgin known as \"Tây Bồi\" (now extinct). After French rule ended, South Vietnam continued to use French in administration, education, and trade. However, since the Fall of Saigon and the opening of a unified Vietnam's economy, French has gradually been effectively displaced as the first foreign language of choice by English in Vietnam. Nevertheless, it continues to be taught as the other main foreign language in the Vietnamese educational system and is regarded as a cultural language.\nAll three countries are full members of La Francophonie (OIF).\n\n\n==== India ====\n\nFrench was the official language of French India, consisting of the geographically separate enclaves referred to as Puducherry. It continued to be an official language of the territory even after its cession to India in 1956 until 1965. A small number of older locals still retain knowledge of the language, although it has now given way to Tamil and English.\n\n\n==== Lebanon ====\n\nA former French mandate, Lebanon designates Arabic as the sole official language, while a special law regulates cases when French can be publicly used. Article 11 of Lebanon's Constitution states that \"Arabic is the official national language. A law determines the cases in which the French language is to be used\". The French language in Lebanon is a widespread second language among the Lebanese people, and is taught in many schools along with Arabic and English. French is used on Lebanese pound banknotes, on road signs, on Lebanese license plates, and on official buildings (alongside Arabic).\nToday, French and English are secondary languages of Lebanon, with about 40% of the population being Francophone and 40% Anglophone. The use of English is growing in the business and media environment. Out of about 900,000 students, about 500,000 are enrolled in Francophone schools, public or private, in which the teaching of mathematics and scientific subjects is provided in French. Actual usage of French varies depending on the region and social status. One-third of high school students educated in French go on to pursue higher education in English-speaking institutions. English is the language of business and communication, with French being an element of social distinction, chosen for its emotional value.\n\n\n=== Oceania ===\n\nFrench is an official language of the Pacific Island nation of Vanuatu, where 31% of the population was estimated to speak it in 2023. It is the sole official language in the French special collectivity of New Caledonia and the overseas collectivities of Wallis and Futuna and French Polynesia.\nIn New Caledonia, 97% of the population can speak, read and write French while in French Polynesia this figure is 95%, and in Wallis and Futuna, it is 84%. In French Polynesia and to a lesser extent Wallis and Futuna, where oral and written knowledge of the French language has become almost universal, French increasingly tends to displace the native Polynesian languages as the language most spoken at home. In French Polynesia, the percentage of the population who reported that French was the language they use the most at home rose from 67% at the 2007 census to 74% at the 2017 census. In Wallis and Futuna, the percentage of the population who reported that French was the language they use the most at home rose from 10% at the 2008 census to 13% at the 2018 census.\n\n\n=== Future ===\nAccording to a demographic projection led by the Université Laval and the Réseau Démographie de l'Agence universitaire de la Francophonie, the total number of French speakers will reach approximately 500 million in 2025 and 650 million by 2050, largely due to rapid population growth in sub-Saharan Africa. OIF estimates 700 million French speakers by 2050, 80% of whom will be in Africa.\nIn a study published in March 2014 by Forbes, the investment bank Natixis said that French could become the world's most spoken language by 2050.\nIn the European Union, French was the dominant language within all institutions until the 1990s. After several enlargements of the EU (1995, 2004), French significantly lost ground in favour of English, which is more widely spoken and taught in most EU countries. French currently remains one of the three working languages, or \"procedural languages\", of the EU, along with English and German. It is the second-most widely used language within EU institutions after English, but remains the preferred language of certain institutions or administrations such as the Court of Justice of the European Union, where it is the sole internal working language, or the Directorate-General for Agriculture. Since 2016, Brexit has rekindled discussions on whether or not French should again hold greater role within the institutions of the European Union.\n\n\n== Varieties ==\n\n\n== Current status and importance ==\nAccording to the OIF, approximately 321 million people worldwide are \"able to speak the language\" as of 2022, without specifying the criteria for this estimation or whom it encompasses. A leading world language, French is taught in universities around the world, and is one of the world's most influential languages because of its wide use in the worlds of journalism, jurisprudence, education, and diplomacy.\nIn diplomacy, French is one of the six official languages of the United Nations (and one of the UN Secretariat's only two working languages), one of twenty official and three procedural languages of the European Union, an official language of NATO, the International Olympic Committee, the Council of Europe, the Organisation for Economic Co-operation and Development, Organization of American States (alongside Spanish, Portuguese and English), the Eurovision Song Contest, one of eighteen official languages of the European Space Agency, World Trade Organization and the least used of the three official languages in the North American Free Trade Agreement countries. It is also a working language in nonprofit organisations such as the Red Cross (alongside English, German, Spanish, Portuguese, Arabic and Russian), Amnesty International (alongside 32 other languages of which English is the most used, followed by Spanish, Portuguese, German, and Italian), Médecins sans Frontières (used alongside English, Spanish, Portuguese and Arabic), and Médecins du Monde (used alongside English). Given the demographic prospects of the French-speaking nations of Africa, researcher Pascal-Emmanuel Gobry wrote in 2014 that French \"could be the language of the future\". However, some African countries such as Algeria intermittently attempted to eradicate the use of French, and as of 2024 it was removed as an official language in Mali and Burkina Faso.\nSignificant as a judicial language, French is one of the official languages of such major international and regional courts, tribunals, and dispute-settlement bodies as the African Court on Human and Peoples' Rights, the Caribbean Court of Justice, the Court of Justice for the Economic Community of West African States, the Inter-American Court of Human Rights, the International Court of Justice, the International Criminal Tribunal for the former Yugoslavia, International Criminal Tribunal for Rwanda, the International Tribunal for the Law of the Sea the International Criminal Court and the World Trade Organization Appellate Body. It is the sole internal working language of the Court of Justice of the European Union, and makes with English the European Court of Human Rights's two working languages.\nIn 1997, George Weber published, in Language Today, a comprehensive academic study entitled \"The World's 10 most influential languages\". In the article, Weber ranked French as, after English, the second-most influential language of the world, ahead of Spanish. His criteria were the numbers of native speakers, the number of secondary speakers (especially high for French among fellow world languages), the number of countries using the language and their respective populations, the economic power of the countries using the language, the number of major areas in which the language is used, and the linguistic prestige associated with the mastery of the language (Weber highlighted that French in particular enjoys considerable linguistic prestige). In a 2008 reassessment of his article, Weber concluded that his findings were still correct since \"the situation among the top ten remains unchanged.\"\nKnowledge of French is often considered to be a useful skill by business owners in the United Kingdom; a 2014 study found that 50% of British managers considered French to be a valuable asset for their business, thus ranking French as the most sought-after foreign language there, ahead of German (49%) and Spanish (44%). MIT economist Albert Saiz calculated a 2.3% premium for those who have French as a foreign language in the workplace.\nIn 2011, Bloomberg Businessweek ranked French the third most useful language for business, after English and Standard Mandarin Chinese.\nIn English-speaking Canada, the United Kingdom, and Ireland, French is the first foreign language taught and in number of pupils is far ahead of other languages. In the United States, French is the second-most commonly taught foreign language in schools and universities, although well behind Spanish. In some areas of the country near French-speaking Quebec, however, it is the foreign language more commonly taught.\n\n\n== Phonology ==\n\nVowel phonemes in French\n\nAlthough there are many French regional accents, foreign learners normally use only one variety of the language.\n\nThere are a maximum of 17 vowels in French, not all of which are used in every dialect: /a/, /ɑ/, /e/, /ɛ/, /ɛː/, /ə/, /i/, /o/, /ɔ/, /y/, /u/, /œ/, /ø/, plus the nasalized vowels /ɑ̃/, /ɛ̃/, /ɔ̃/ and /œ̃/. In France, the vowels /ɑ/, /ɛː/ and /œ̃/ are tending to be replaced by /a/, /ɛ/ and /ɛ̃/ in many people's speech, but the distinction of /ɛ̃/ and /œ̃/ is present in Meridional French. In Quebec and Belgian French, the vowels /ɑ/, /ə/, /ɛː/ and /œ̃/ are present.\nVoiced stops (i.e., /b, d, ɡ/) are typically produced fully voiced throughout.\nVoiceless stops (i.e., /p, t, k/) are unaspirated.\nThe velar nasal /ŋ/ can occur in final position in borrowed (usually English) words: parking, camping, swing.\nThe palatal nasal /ɲ/, which is written ⟨gn⟩, can occur in word initial position (e.g., gnon), but it is most frequently found in intervocalic, onset position or word-finally (e.g., montagne).\nFrench has three pairs of homorganic fricatives distinguished by voicing, i.e., labiodental /f/~/v/, dental /s/~/z/, and palato-alveolar /ʃ/~/ʒ/. /s/~/z/ are dental, like the plosives /t/~/d/ and the nasal /n/.\nFrench has one rhotic whose pronunciation varies considerably among speakers and phonetic contexts. In general, it is described as a voiced uvular fricative, as in [ʁu] roue, \"wheel\". Vowels are often lengthened before this segment. It can be reduced to an approximant, particularly in final position (e.g., fort), or reduced to zero in some word-final positions. For other speakers, a uvular trill is also common, and an apical trill [r] occurs in some dialects. The cluster /ʁw/ is generally pronounced as a labialised voiced uvular fricative [ʁʷ], such as in [ʁʷa] roi, \"king\", or [kʁʷaʁ] croire, \"to believe\".\nLateral and central approximants: The lateral approximant /l/ is unvelarised in both onset (lire) and coda position (il). In the onset, the central approximants [w], [ɥ], and [j] each correspond to a high vowel, /u/, /y/, and /i/ respectively. There are a few minimal pairs where the approximant and corresponding vowel contrast, but there are also many cases where they are in free variation. Contrasts between /j/ and /i/ occur in final position as in /pɛj/ paye, \"pay\", vs. /pɛi/ pays, \"country\".\nThe lateral approximant /l/ can be delateralised when word- or morpheme-final and preceded by /i/, such as in /tʁavaj/ travail, \"work\", or when a word ending in ⟨al⟩ is pluralised, giving ⟨aux⟩ /o/.\nFrench pronunciation follows strict rules based on spelling, but French spelling is often based more on history than phonology. The rules for pronunciation vary between dialects, but the standard rules are:\n\nFinal single consonants, in particular s, x, z, t, d, n, p and g, are normally silent. (A consonant is considered \"final\" when no vowel follows it even if one or more consonants follow it.) The final letters f, k, q, and l, however, are normally pronounced. The final c is sometimes pronounced, as in bac, sac, roc, but can also be silent, as in blanc or estomac. The final r is usually silent when it follows an e in a word of two or more syllables, but it is pronounced in some words (hiver, super, cancer etc.).\nWhen the following word begins with a vowel, however, a silent consonant may once again be pronounced, to provide a liaison or \"link\" between the two words. Some liaisons are mandatory, for example the s in les amants or vous avez; some are optional, depending on dialect and register, for example, the first s in deux cents euros or euros irlandais; and some are forbidden, for example, the s in beaucoup d'hommes aiment. The t of et is never pronounced and the silent final consonant of a noun is only pronounced in the plural and in set phrases like pied-à-terre.\nDoubling a final n and adding a silent e at the end of a word (e.g., chien → chienne) makes it clearly pronounced. Doubling a final l and adding a silent e (e.g., gentil → gentille) adds a [j] sound if the l is preceded by the letter i.\nSome monosyllabic function words ending in a or e, such as je and que, drop their final vowel when placed before a word that begins with a vowel sound (thus avoiding a hiatus). The missing vowel is replaced by an apostrophe. (e.g., *je ai is instead pronounced and spelled j'ai). This gives, for example, the same pronunciation for l'homme qu'il a vu (\"the man whom he saw\") and l'homme qui l'a vu (\"the man who saw him\"). However, in Belgian French the sentences are pronounced differently; in the first sentence the syllable break is as \"qu'il-a\", while the second breaks as \"qui-l'a\". It can also be noted that, in Quebec French, the second example (l'homme qui l'a vu) has more emphasis on l'a vu.\n\n\n== Writing system ==\n\n\n=== Alphabet ===\n\nFrench is written with the 26 letters of the basic Latin script, with four diacritics appearing on vowels (circumflex accent, acute accent, grave accent, diaeresis) and the cedilla appearing in \"ç\".\nThere are two ligatures, \"œ\" and \"æ\", but they are often replaced in contemporary French with \"oe\" and \"ae\", because the ligatures do not appear on the AZERTY keyboard layout used in French-speaking countries. However this is nonstandard in formal and literary texts.\n\n\n=== Orthography ===\n\nFrench spelling, like English spelling, tends to preserve obsolete pronunciation rules. This is mainly due to extreme phonetic changes since the Old French period, without a corresponding change in spelling. Moreover, some conscious changes were made to restore Latin orthography (as with some English words such as \"debt\"):\n\nOld French doit > French doigt \"finger\" (Latin digitus)\nOld French pie > French pied \"foot\" [Latin pes (stem: ped-)]\nFrench orthography is morphophonemic. While it contains 130 graphemes that denote only 36 phonemes, many of its spelling rules are likely due to a consistency in morphemic patterns such as adding suffixes and prefixes. Many given spellings of common morphemes usually lead to a predictable sound. In particular, a given vowel combination or diacritic generally leads to one phoneme. However, there is not a one-to-one relation of a phoneme and a single related grapheme, which can be seen in how tomber and tombé both end with the /e/ phoneme. Additionally, there are many variations in the pronunciation of consonants at the end of words, demonstrated by how the x in paix is not pronounced though at the end of Aix it is.\nAs a result, it can be difficult to predict the spelling of a word based on the sound. Final consonants are generally silent, except when the following word begins with a vowel (see Liaison (French)). For example, the following words end in a vowel sound: pied, aller, les, finit, beaux. The same words followed by a vowel, however, may sound the consonants, as they do in these examples: beaux-arts, les amis, pied-à-terre.\nFrench writing, as with any language, is affected by the spoken language. In Old French, the plural for animal was animals. The /als/ sequence was unstable and was turned into a diphthong /aus/. This change was then reflected in the orthography: animaus. The us ending, very common in Latin, was then abbreviated by copyists (monks) to the letter x, resulting in a written form animax. As the French language further evolved, the pronunciation of au turned into /o/ so that the u was reestablished in orthography for consistency, resulting in modern French animaux (pronounced first /animos/ before the final /s/ was dropped in contemporary French). The same is true for cheval pluralized as chevaux and many others. In addition, castel pl. castels became château pl. châteaux.\n\nNasal: n and m. When n or m follows a vowel or diphthong, the n or m becomes silent and causes the preceding vowel to become nasalized (i.e., pronounced with the soft palate extended downward so as to allow part of the air to leave through the nostrils). Exceptions are when the n or m is doubled, or immediately followed by a vowel. The prefixes en- and em- are always nasalized. The rules are more complex than this but may vary between dialects.\nDigraphs: French uses not only diacritics to specify its large range of vowel sounds and diphthongs, but also specific combinations of vowels, sometimes with following consonants, to show which sound is intended.\nGemination: Within words, double consonants are generally not pronounced as geminates in modern French (but geminates can be heard in the cinema or TV news from as recently as the 1970s, and in very refined elocution they may still occur). For example, illusion is pronounced [ilyzjɔ̃] and not [ilːyzjɔ̃]. However, gemination does occur between words; for example, une info (\"a news item\" or \"a piece of information\") is pronounced [ynɛ̃fo], whereas une nympho (\"a nymphomaniac\") is pronounced [ynːɛ̃fo].\nAccents are used sometimes for pronunciation, sometimes to distinguish similar words, and sometimes based on etymology alone.\nAccents that affect pronunciation\nThe acute accent (l'accent aigu) é (e.g., école—school) means that the vowel is pronounced /e/ instead of the default /ə/.\nThe grave accent (l'accent grave) è (e.g., élève—pupil) means that the vowel is pronounced /ɛ/ instead of the default /ə/.\nThe circumflex (l'accent circonflexe) ê (e.g. forêt—forest) shows that an e is pronounced /ɛ/ and that an ô is pronounced /o/. In standard French, it also signifies a pronunciation of /ɑ/ for the letter â, but this differentiation is disappearing. In the mid-18th century, the circumflex was used in place of s after a vowel, where that letter s was not pronounced. Thus, forest became forêt, hospital became hôpital, and hostel became hôtel.\nDiaeresis or tréma (ë, ï, ü, ÿ): over e, i, u or y, indicates that a vowel is to be pronounced separately from the preceding one: naïve, Noël.\nö \nThe combination of e with diaeresis following o (Noël [ɔɛ]) is nasalized in the regular way if followed by n (Samoëns [wɛ̃])\nThe combination of e with diaeresis following a is either pronounced [ɛ] (Raphaël, Israël [aɛ]) or not pronounced, leaving only the a (Staël [a]) and the a is nasalized in the regular way if aë is followed by n (Saint-Saëns [ɑ̃])\nA diaeresis on y only occurs in some proper names and in modern editions of old French texts. Some proper names in which ÿ appears include Aÿ (a commune in Marne, formerly Aÿ-Champagne), Rue des Cloÿs (an alley in Paris), Croÿ (family name and hotel on the Boulevard Raspail, Paris), Château du Faÿ (near Pontoise), Ghÿs (name of Flemish origin spelt Ghĳs where ĳ in handwriting looked like ÿ to French clerks), L'Haÿ-les-Roses (commune near Paris), Pierre Louÿs (author), Moÿ-de-l'Aisne (commune in Aisne and a family name), and Le Blanc de Nicolaÿ (an insurance company in eastern France).\nThe diaeresis on u appears in the Biblical proper names Archélaüs, Capharnaüm, Emmaüs, Ésaü, and Saül, as well as French names such as Haüy. Nevertheless, since the 1990 orthographic changes, the diaeresis in words containing guë (such as aiguë or ciguë) may be moved onto the u: aigüe, cigüe, and by analogy may be used in verbs such as j'argüe.\nIn addition, words coming from German retain their umlaut (ä, ö and ü) if applicable but use often French pronunciation, such as Kärcher (trademark of a pressure washer).\nThe cedilla (la cédille) ç (e.g., garçon—boy) means that the letter ç is pronounced /s/ in front of the back vowels a, o and u (c is otherwise /k/ before a back vowel). C is always pronounced /s/ in front of the front vowels e, i, and y, thus ç is never found in front of front vowels. This letter is used when a front vowel after ⟨c⟩, such as in France or placer, is replaced with a back vowel. To retain the pronunciation of the ⟨c⟩, it is given a cedilla, as in français or plaçons.\nAccents with no pronunciation effect\nThe circumflex does not affect the pronunciation of the letters i or u, nor, in most dialects, a. It usually indicates that an s came after it long ago, as in île (from former isle, compare with English word \"isle\"). The explanation is that some words share the same orthography, so the circumflex is put here to mark the difference between the two words. For example, dites (you say) / dîtes (you said), or even du (of the) / dû (past participle for the verb devoir = must, have to, owe; in this case, the circumflex disappears in the plural and the feminine).\nAll other accents are used only to distinguish similar words, as in the case of distinguishing the adverbs là and où (\"there\", \"where\") from the article la (\"the\" feminine singular) and the conjunction ou (\"or\"), respectively.\nSome proposals exist to simplify the existing writing system, but they still fail to gather interest.\nIn 1990, a reform accepted some changes to French orthography. At the time the proposed changes were considered to be suggestions. In 2016, schoolbooks in France began to use the newer recommended spellings, with instruction to teachers that both old and new spellings be deemed correct.\n\n\n== Grammar ==\n\nFrench is a moderately inflected language. Nouns and most pronouns are inflected for number (singular or plural, though in most nouns the plural is pronounced the same as the singular even if spelled differently); adjectives, for number and gender (masculine or feminine) of their nouns; personal pronouns and a few other pronouns, for person, number, gender, and case; and verbs, for tense, aspect, mood, and the person and number of their subjects. Case is primarily marked using word order and prepositions, while certain verb features are marked using auxiliary verbs. According to the French lexicogrammatical system, French has a rank-scale hierarchy with clause as the top rank, which is followed by group rank, word rank, and morpheme rank. A French clause is made up of groups, groups are made up of words, and lastly, words are made up of morphemes.\nFrench grammar shares several notable features with most other Romance languages, including\n\nthe loss of Latin declensions\nthe loss of the neuter gender\nthe development of grammatical articles from Latin demonstratives\nthe loss of certain Latin tenses and the creation of new tenses from auxiliaries.\n\n\n=== Nouns ===\nEvery French noun is either masculine or feminine. Because French nouns are not inflected for gender, a noun's form cannot specify its gender. For nouns regarding the living, their grammatical genders often correspond to that which they refer to. For example, a male teacher is an enseignant while a female teacher is an enseignante. However, plural nouns that refer to a group that includes both masculine and feminine entities are always masculine. So a group of two male teachers would be enseignants. A group of two male teachers and two female teachers would still be enseignants. However, a group of two female teachers would be enseignantes. In many situations, including in the case of enseignant, both the singular and plural form of a noun are pronounced identically. The article used for singular nouns is different from that used for plural nouns and the article provides a distinguishing factor between the two in speech. For example, the singular le professeur or la professeure (the male or female teacher, professor) can be distinguished from the plural les professeur(e)s because le /lə/, la /la/, and les /le(s)/ are all pronounced differently. With enseignant, however, for both singular forms the le/la becomes l', and so the only difference in pronunciation is that the ⟨t⟩ on the end of masculine form is silent, whereas it is pronounced in the feminine. If the word was to be followed by a word starting with a vowel, then liaison would cause the ⟨t⟩ to be pronounced in both forms, resulting in identical pronunciation. There are also some situations where both the feminine and masculine form of a noun are the same and the article provides the only difference. For example, le dentiste refers to a male dentist while la dentiste refers to a female dentist. Furthermore, a few nouns' meanings depend on their gender. For example, un livre (masculine) refers to a book, while une livre a (feminine) is a pound.\n\n\n=== Verbs ===\n\n\n==== Moods and tense-aspect forms ====\nThe French language consists of both finite and non-finite moods. The finite moods include the indicative mood (indicatif), the subjunctive mood (subjonctif), the imperative mood (impératif), and the conditional mood (conditionnel). The non-finite moods include the infinitive mood (infinitif), the present participle (participe présent), and the past participle (participe passé).\n\n\n===== Finite moods =====\n\n\n====== Indicative (indicatif) ======\nThe indicative mood makes use of eight tense-aspect forms. These include the present (présent), the simple past (passé composé and passé simple), the past imperfective (imparfait), the pluperfect (plus-que-parfait), the simple future (futur simple), the future perfect (futur antérieur), and the past perfect (passé antérieur). Some forms are less commonly used today. In today's spoken French, the passé composé is used while the passé simple is reserved for formal situations or for literary purposes. Similarly, the plus-que-parfait is used for speaking rather than the older passé antérieur seen in literary works.\nWithin the indicative mood, the passé composé, plus-que-parfait, futur antérieur, and passé antérieur all use auxiliary verbs in their forms.\n\n\n====== Subjunctive (subjonctif) ======\nThe subjunctive mood only includes four of the tense-aspect forms found in the indicative: present (présent), simple past (passé composé), past imperfective (imparfait), and pluperfect (plus-que-parfait).\nWithin the subjunctive mood, the passé composé and plus-que-parfait use auxiliary verbs in their forms.\n\n\n====== Imperative (imperatif) ======\nThe imperative is used in the present tense (with the exception of a few instances where it is used in the perfect tense). The imperative is used to give commands to you (tu), we/us (nous), and plural you (vous).\n\n\n====== Conditional (conditionnel) ======\nThe conditional makes use of the present (présent) and the past (passé).\nThe passé uses auxiliary verbs in its forms.\n\n\n==== Voice ====\nFrench uses both the active voice and the passive voice. The active voice is unmarked while the passive voice is formed by using a form of verb être (\"to be\") and the past participle.\nExample of the active voice:\n\n\"Elle aime le chien.\" She loves the dog.\n\"Marc a conduit la voiture.\" Marc drove the car.\nExample of the passive voice:\n\n\"Le chien est aimé par elle.\" The dog is loved by her.\n\"La voiture a été conduite par Marc.\" The car was driven by Marc.\nHowever, unless the subject of the sentence is specified, generally the pronoun on \"one\" is used:\n\n\"On aime le chien.\" The dog is loved. (Literally \"one loves the dog.\")\n\"On conduit la voiture.\" The car is (being) driven. (Literally \"one drives the car.\")\nWord order is subject–verb–object although a pronoun object precedes the verb. Some types of sentences allow for or require different word orders, in particular inversion of the subject and verb, as in \"Parlez-vous français ?\" when asking a question rather than \"Vous parlez français ?\" Both formulations are used, and carry a rising inflection on the last word. The literal English translations are \"Do you speak French?\" and \"You speak French?\", respectively. To avoid inversion while asking a question, \"Est-ce que\" (literally \"is it that\") may be placed at the beginning of the sentence. \"Parlez-vous français ?\" may become \"Est-ce que vous parlez français ?\" French also uses verb–object–subject (VOS) and object–subject–verb (OSV) word order. OSV word order is not used often and VOS is reserved for formal writings.\n\n\n== Vocabulary ==\n\nThe majority of French words derive from Vulgar Latin or were constructed from Latin or Greek roots. In many cases, a single etymological root appears in French in a \"popular\" or native form, inherited from Vulgar Latin, and a learned form, borrowed later from Classical Latin. The following pairs consist of a native noun and a learned adjective:\n\nbrother: frère / fraternel from Latin frater / fraternalis\nfinger: doigt / digital from Latin digitus / digitalis\nfaith: foi / fidèle from Latin fides / fidelis\neye: œil / oculaire from Latin oculus / ocularis\nHowever, a historical tendency to Gallicise Latin roots can be identified, whereas English conversely leans towards a more direct incorporation of the Latin:\n\nrayonnement / radiation from Latin radiatio\néteindre / extinguish from Latin exstinguere\nnoyau / nucleus from Latin nucleus\nensoleillement / insolation from Latin insolatio\nThere are also noun-noun and adjective-adjective pairs:\n\nthing/cause: chose / cause from Latin causa\ncold: froid / frigide from Latin frigidum\nIt can be difficult to identify the Latin source of native French words because in the evolution from Vulgar Latin, unstressed syllables were severely reduced and the remaining vowels and consonants underwent significant modifications.\nMore recently (1994) the linguistic policy (Toubon Law) of the French language academies of France and Quebec has been to provide French equivalents to (mainly English) imported words, either by using existing vocabulary, extending its meaning or deriving a new word according to French morphological rules. The result is often two (or more) co-existing terms for describing the same phenomenon.\n\nmercatique / marketing\nfinance fantôme / shadow banking\nbloc-notes / notepad\nailière / wingsuit\ntiers-lieu / coworking\nIt is estimated that 12% (4,200) of common French words found in a typical dictionary such as the Petit Larousse or Micro-Robert Plus (35,000 words) are of foreign origin (where Greek and Latin learned words are not seen as foreign). About 25% (1,054) of these foreign words come from English and are fairly recent borrowings. The others are some 707 words from Italian, 550 from ancient Germanic languages, 481 from other Gallo-Romance languages, 215 from Arabic, 164 from German, 160 from Celtic languages, 159 from Spanish, 153 from Dutch, 112 from Persian and Sanskrit, 101 from Native American languages, 89 from other Asian languages, 56 from other Afro-Asiatic languages, 55 from Balto-Slavic languages, 10 from Basque and 144 (about 3%) from other languages.\nOne study analyzing the degree of differentiation of Romance languages in comparison to Latin estimated that among the languages analyzed French has the greatest distance from Latin. The French language's lexical similarity to a selection of other Romance languages is 89% with Italian, 80% with Sardinian, 78% with Rhaeto-Romance, and 75% with Romanian, Spanish and Portuguese.\n\n\n=== Numerals ===\nThe numeral system used in the majority of Francophone countries employs both decimal and vigesimal counting. After the use of unique names for the numbers 1–16, those from 17 to 69 are counted by tens, while twenty (vingt) is used as a base number in the names of numbers from 70 to 99. The French word for 80 is quatre-vingts, literally \"four twenties\", and the word for 75 is soixante-quinze, literally \"sixty-fifteen\". The vigesimal method of counting is analogous to the archaic English use of score, as in \"fourscore and seven\" (87), or \"threescore and ten\" (70).\nBelgian, Swiss, and Aostan French as well as that used in the Democratic Republic of the Congo, Rwanda and Burundi, use different names for 70 and 90, namely septante and nonante. In Switzerland, depending on the local dialect, 80 can be quatre-vingts (Geneva, Neuchâtel, Jura) or huitante (Vaud, Valais, Fribourg). The Aosta Valley similarly uses huitante for 80. Conversely, Belgium and in its former African colonies use quatre-vingts for 80.\nIn Old French (during the Middle Ages), all numbers from 30 to 99 could be said in either base 10 or base 20, e.g. vint et doze (twenty and twelve) for 32, dous vinz et diz (two twenties and ten) for 50, uitante for 80, or nonante for 90.\nThe term octante was historically used in Switzerland for 80, but is now considered archaic.\nFrench, like most European languages, uses a space to separate thousands. The comma (French: virgule) is used in French numbers as a decimal point, i.e. \"2,5\" instead of \"2.5\". In the case of currencies, the currency markers are substituted for decimal point, i.e. \"5$7\" for \"5 dollars and 7 cents\".\n\n\n== Example text ==\nArticle 1 of the Universal Declaration of Human Rights in French:\n\nTous les êtres humains naissent libres et égaux en dignité et en droits. Ils sont doués de raison et de conscience et doivent agir les uns envers les autres dans un esprit de fraternité.\nArticle 1 of the Universal Declaration of Human Rights in English:\n\nAll human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\n\n\n== See also ==\n\nAlliance Française\nAZERTY\nFrançais fondamental\nFrancization\nFrancophile\nFrancophobia\nFrancophonie\nFrench language in the United States\nFrench language in Canada\nFrench poetry\nGlossary of French expressions in English\nInfluence of French on English\nLanguage education\nList of countries where French is an official language\nList of English words of French origin\nList of French loanwords in Persian\nList of French words and phrases used by English speakers\nList of German words of French origin\nOfficial bilingualism in Canada\nVarieties of French\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Works cited ===\nLa langue française dans le monde 2014 (PDF) (in French). Nathan. 2014. ISBN 978-2-09-882654-0. Archived from the original (PDF) on 12 April 2015. Retrieved 5 April 2015.\nRoegiest, Eugeen (2006). Vers les sources des langues romanes: Un itinéraire linguistique à travers la Romania. Leuven, Belgium: Acco.\n\n\n== Further reading ==\nMarc Fumaroli (2011). When the World Spoke French. Translated by Richard Howard. New York Review of Books. ISBN 978-1-59017-375-6.\nNadeau, Jean-Benoît, and Julie Barlow (2006). The Story of French. (First U.S. ed.) New York: St. Martin's Press. ISBN 0-312-34183-0.\nUrsula Reutner (2017). Manuel des francophonies. Berlin/Boston: de Gruyter. ISBN 978-3-11-034670-1\n\n\n== External links ==\n\n\n=== Organisations ===\nFondation Alliance française: an international organisation for the promotion of French language and culture (in French)\nAgence de promotion du FLE: Agency for promoting French as a foreign language\n\n\n=== Courses and tutorials ===\nFrançais interactif: interactive French program, University of Texas at Austin\nTex's French Grammar, University of Texas at Austin\nLingopolo French\nFrench lessons in London, The Language machine\n\n\n=== Online dictionaries ===\nOxford Dictionaries French Dictionary\nCollins Online English↔French Dictionary\nCentre national de ressources textuelles et lexicales: monolingual dictionaries (including the Trésor de la langue française), language corpora, etc.\n\n\n=== Grammar ===\n\n\n==== Verbs ====\nFrench verb conjugation at Verbix\n\n\n=== Vocabulary ===\nSwadesh list in English and French\n\n\n==== Numbers ====\nSmith, Paul. \"French, Numbers\". Numberphile. Brady Haran. Archived from the original on 2 March 2017. Retrieved 7 April 2013.\n\n\n==== Books ====\n(in French) La langue française dans le monde 2010 (Full book freely accessible)\n\n\n==== Articles ====\n\"The status of French in the world\". French Ministry of Foreign Affairs"
    },
    {
        "id": "music_theory",
        "name": "Music theory",
        "text": "Music theory is the study of theoretical frameworks for understanding the practices and possibilities of music. The Oxford Companion to Music describes three interrelated uses of the term \"music theory\": The first is the \"rudiments\", that are needed to understand music notation (key signatures, time signatures, and rhythmic notation); the second is learning scholars' views on music from antiquity to the present; the third is a sub-topic of musicology that \"seeks to define processes and general principles in music\". The musicological approach to theory differs from music analysis \"in that it takes as its starting-point not the individual work or performance but the fundamental materials from which it is built.\"\nMusic theory is frequently concerned with describing how musicians and composers make music, including tuning systems and composition methods among other topics. Because of the ever-expanding conception of what constitutes music, a more inclusive definition could be the consideration of any sonic phenomena, including silence. This is not an absolute guideline, however; for example, the study of \"music\" in the Quadrivium liberal arts university curriculum, that was common in medieval Europe, was an abstract system of proportions that was carefully studied at a distance from actual musical practice. But this medieval discipline became the basis for tuning systems in later centuries and is generally included in modern scholarship on the history of music theory.\nMusic theory as a practical discipline encompasses the methods and concepts that composers and other musicians use in creating and performing music. The development, preservation, and transmission of music theory in this sense may be found in oral and written music-making traditions, musical instruments, and other artifacts. For example, ancient instruments from prehistoric sites around the world reveal details about the music they produced and potentially something of the musical theory that might have been used by their makers. In ancient and living cultures around the world, the deep and long roots of music theory are visible in instruments, oral traditions, and current music-making. Many cultures have also considered music theory in more formal ways such as written treatises and music notation. Practical and scholarly traditions overlap, as many practical treatises about music place themselves within a tradition of other treatises, which are cited regularly just as scholarly writing cites earlier research.\nIn modern academia, music theory is a subfield of musicology, the wider study of musical cultures and history. Guido Adler, however, in one of the texts that founded musicology in the late 19th century, wrote that \"the science of music originated at the same time as the art of sounds\", where \"the science of music\" (Musikwissenschaft) obviously meant \"music theory\". Adler added that music only could exist when one began measuring pitches and comparing them to each other. He concluded that \"all people for which one can speak of an art of sounds also have a science of sounds\". One must deduce that music theory exists in all musical cultures of the world.\nMusic theory is often concerned with abstract musical aspects such as tuning and tonal systems, scales, consonance and dissonance, and rhythmic relationships. There is also a body of theory concerning practical aspects, such as the creation or the performance of music, orchestration, ornamentation, improvisation, and electronic sound production. A person who researches or teaches music theory is a music theorist. University study, typically to the MA or PhD level, is required to teach as a tenure-track music theorist in a US or Canadian university. Methods of analysis include mathematics, graphic analysis, and especially analysis enabled by western music notation. Comparative, descriptive, statistical, and other methods are also used. Music theory textbooks, especially in the United States of America, often include elements of musical acoustics, considerations of musical notation, and techniques of tonal composition (harmony and counterpoint), among other topics.\n\n\n== History ==\n\n\n=== Antiquity ===\n\n\n==== Mesopotamia ====\n\nSeveral surviving Sumerian and Akkadian clay tablets include musical information of a theoretical nature, mainly lists of intervals and tunings. The scholar Sam Mirelman reports that the earliest of these texts dates from before 1500 BCE, a millennium earlier than surviving evidence from any other culture of comparable musical thought. Further, \"All the Mesopotamian texts [about music] are united by the use of a terminology for music that, according to the approximate dating of the texts, was in use for over 1,000 years.\"\n\n\n==== China ====\n\nMuch of Chinese music history and theory remains unclear.\nChinese theory starts from numbers, the main musical numbers being twelve, five and eight. Twelve refers to the number of pitches on which the scales can be constructed, Five refers to the Pentatonic Scale (primarily uses a 5-note scale), And Eight refers to the eight categories of Chinese Music Instruments; classified by the material they are made from: (Metal, Stone, Silk, Bamboo, Gourd, Clay, Leather, and Wood). The Lüshi chunqiu from about 238 BCE recalls the legend of Ling Lun. On order of the Yellow Emperor, Ling Lun collected twelve bamboo lengths with thick and even nodes. Blowing on one of these like a pipe, he found its sound agreeable and named it huangzhong, the \"Yellow Bell.\" He then heard phoenixes singing. The male and female phoenix each sang six tones. Ling Lun cut his bamboo pipes to match the pitches of the phoenixes, producing twelve pitch pipes in two sets: six from the male phoenix and six from the female: these were called the lülü or later the shierlü.\n\nApart from technical and structural aspects, ancient Chinese music theory also discusses topics such as the nature and functions of music. The Yueji (\"Record of music\", c1st and 2nd centuries BCE), for example, manifests Confucian moral theories of understanding music in its social context. Studied and implemented by Confucian scholar-officials [...], these theories helped form a musical Confucianism that overshadowed but did not erase rival approaches. These include the assertion of Mozi (c. 468 – c. 376 BCE) that music wasted human and material resources, and Laozi's claim that the greatest music had no sounds. [...] Even the music of the qin zither, a genre closely affiliated with Confucian scholar-officials, includes many works with Daoist references, such as Tianfeng huanpei (\"Heavenly Breeze and Sounds of Jade Pendants\").\n\n\n==== India ====\n\nThe Samaveda and Yajurveda (c. 1200 – 1000 BCE) are among the earliest testimonies of Indian music, but properly speaking, they contain no theory. The Natya Shastra, written between 200 BCE to 200 CE, discusses intervals (Śrutis), scales (Grāmas), consonances and dissonances, classes of melodic structure (Mūrchanās, modes?), melodic types (Jātis), instruments, etc.\n\n\n==== Greece ====\n\nEarly preserved Greek writings on music theory include two types of works:\n\ntechnical manuals describing the Greek musical system including notation, scales, consonance and dissonance, rhythm, and types of musical compositions;\ntreatises on the way in which music reveals universal patterns of order leading to the highest levels of knowledge and understanding.\nSeveral names of theorists are known before these works, including Pythagoras (c. 570 ~ c. 495 BCE), Philolaus (c. 470 ~ (c. 385 BCE), Archytas (428–347 BCE), and others.\nWorks of the first type (technical manuals) include\n\nAnonymous (erroneously attributed to Euclid) (1989) [4th–3rd century BCE]. Barker, Andrew (ed.). Κατατομή κανόνος [Division of the Canon]. Greek Musical Writings. Vol. 2: Harmonic and Acoustic Theory. Cambridge, UK: Cambridge University Press. pp. 191–208. English trans.\nTheon of Smyrna. Τωv κατά τό μαθηματικόν χρησίμων είς τήν Πλάτωνος άνάγνωσις [On the Mathematics Useful for Understanding Plato] (in Greek). 115–140 CE.\nNicomachus of Gerasa. Άρμονικόν έγχειρίδιον [Manual of Harmonics]. 100–150 CE.\nCleonides. Είσαγωγή άρμονική [Introduction to Harmonics] (in Greek). 2nd century CE.\nGaudentius. Άρμονική είσαγωγή [Harmonic Introduction] (in Greek). 3rd or 4th century CE.\nBacchius Geron. Είσαγωγή τέχνης μουσικής [Introduction to the Art of Music]. 4th century CE or later.\nAlypius of Alexandria. Είσαγωγή μουσική [Introduction to Music] (in Greek). 4th–5th century CE.\nMore philosophical treatises of the second type include\n\nAristoxenus. Άρμονικά στοιχεία [Harmonic Elements] (in Greek). 375~360 BCE, before 320 BCE.\nAristoxenus. Ρυθμικά στοιχεία [Rhythmic Elements] (in Greek).\nPtolemaios (Πτολεμαίος), Claudius. Άρμονικά [Harmonics] (in Greek). 127–148 CE.\nPorphyrius. Είς τά άρμονικά Πτολεμαίον ύπόμνημα [On Ptolemy's Harmonics] (in Greek). c. 232~233 – c. 305 CE.\n\n\n=== Post-classical or Medieval Period ===\n\n\n==== China ====\nThe pipa instrument carried with it a theory of musical modes that subsequently led to the Sui and Tang theory of 84 musical modes.\n\n\n==== Arabic countries / Persian countries ====\nMedieval Arabic music theorists include:\n\nAbū Yūsuf Ya'qūb al-Kindi (Bagdad, 873 CE), who uses the first twelve letters of the alphabet to describe the twelve frets on five strings of the oud, producing a chromatic scale of 25 degrees.\n[Yaḥyā ibn] al-Munajjim (Baghdad, 856–912), author of Risāla fī al-mūsīqī (\"Treatise on music\", MS GB-Lbl Oriental 2361) which describes a Pythagorean tuning of the oud and a system of eight modes perhaps inspired by Ishaq al-Mawsili (767–850).\nAbū n-Nașr Muḥammad al-Fārābi (Persia, 872? – Damas, 950 or 951 CE), author of Kitab al-Musiqa al-Kabir (\"The Great Book of Music\").\n'Ali ibn al-Husayn ul-Isfahānī (897–967), known as Abu al-Faraj al-Isfahani, author of Kitāb al-Aghānī (\"The Book of Songs\").\nAbū 'Alī al-Ḥusayn ibn ʿAbd-Allāh ibn Sīnā, known as Avicenna (c. 980 – 1037), whose contribution to music theory consists mainly in Chapter 12 of the section on mathematics of his Kitab Al-Shifa (\"The Book of Healing\").\nal-Ḥasan ibn Aḥmad ibn 'Ali al-Kātib, author of Kamāl adab al Ghinā' (\"The Perfection of Musical Knowledge\"), copied in 1225 (Istanbul, Topkapi Museum, Ms 1727).\nSafi al-Din al-Urmawi (1216–1294 CE), author of the Kitabu al-Adwār (\"Treatise of musical cycles\") and ar-Risālah aš-Šarafiyyah (\"Epistle to Šaraf\").\nMubārak Šāh, commentator of Safi al-Din's Kitāb al-Adwār (British Museum, Ms 823).\nAnon. LXI, Anonymous commentary on Safi al-Din's Kitāb al-Adwār.\nShams al-dῑn al-Saydᾱwῑ Al-Dhahabῑ (14th century CE (?)), music theorist. Author of Urjῡza fi'l-mῡsῑqᾱ (\"A Didactic Poem on Music\").\n\n\n==== Europe ====\nThe Latin treatise De institutione musica by the Roman philosopher Boethius (written c. 500, translated as Fundamentals of Music) was a touchstone for other writings on music in medieval Europe. Boethius represented Classical authority on music during the Middle Ages, as the Greek writings on which he based his work were not read or translated by later Europeans until the 15th century. This treatise carefully maintains distance from the actual practice of music, focusing mostly on the mathematical proportions involved in tuning systems and on the moral character of particular modes. Several centuries later, treatises began to appear which dealt with the actual composition of pieces of music in the plainchant tradition. At the end of the ninth century, Hucbald worked towards more precise pitch notation for the neumes used to record plainchant.\nGuido d'Arezzo wrote a letter to Michael of Pomposa in 1028, entitled Epistola de ignoto cantu, in which he introduced the practice of using syllables to describe notes and intervals. This was the source of the hexachordal solmization that was to be used until the end of the Middle Ages. Guido also wrote about emotional qualities of the modes, the phrase structure of plainchant, the temporal meaning of the neumes, etc.; his chapters on polyphony \"come closer to describing and illustrating real music than any previous account\" in the Western tradition.\nDuring the thirteenth century, a new rhythm system called mensural notation grew out of an earlier, more limited method of notating rhythms in terms of fixed repetitive patterns, the so-called rhythmic modes, which were developed in France around 1200. An early form of mensural notation was first described and codified in the treatise Ars cantus mensurabilis (\"The art of measured chant\") by Franco of Cologne (c. 1280). Mensural notation used different note shapes to specify different durations, allowing scribes to capture rhythms which varied instead of repeating the same fixed pattern; it is a proportional notation, in the sense that each note value is equal to two or three times the shorter value, or half or a third of the longer value. This same notation, transformed through various extensions and improvements during the Renaissance, forms the basis for rhythmic notation in European classical music today.\n\n\n=== Modern ===\n\n\n==== Middle Eastern and Central Asian countries ====\nBāqiyā Nāyinῑ (Uzbekistan, 17th century CE), Uzbek author and music theorist. Author of Zamzama e wahdat-i-mῡsῑqῑ [\"The Chanting of Unity in Music\"].\nBaron Francois Rodolphe d'Erlanger (Tunis, Tunisia, 1910–1932 CE), French musicologist. Author of La musique arabe and Ta'rῑkh al-mῡsῑqᾱ al-arabiyya wa-usῡluha wa-tatawwurᾱtuha [\"A History of Arabian Music, its principles and its Development\"]\nD'Erlanger divulges that the Arabic music scale is derived from the Greek music scale, and that Arabic music is connected to certain features of Arabic culture, such as astrology.\n\n\n==== Europe ====\nRenaissance\n\nBaroque\n\n1750–1900\nAs Western musical influence spread throughout the world in the 1800s, musicians adopted Western theory as an international standard—but other theoretical traditions in both textual and oral traditions remain in use. For example, the long and rich musical traditions unique to ancient and current cultures of Africa are primarily oral, but describe specific forms, genres, performance practices, tunings, and other aspects of music theory.\nSacred harp music uses a different kind of scale and theory in practice. The music focuses on the solfege \"fa, sol, la\" on the music scale. Sacred Harp also employs a different notation involving \"shape notes\", or notes that are shaped to correspond to a certain solfege syllable on the music scale. Sacred Harp music and its music theory originated with Reverend Thomas Symmes in 1720, where he developed a system for \"singing by note\" to help his church members with note accuracy.\n\n\n=== Contemporary ===\n\n\n== Fundamentals of music ==\n\nMusic is composed of aural phenomena; \"music theory\" considers how those phenomena apply in music. Music theory considers melody, rhythm, counterpoint, harmony, form, tonal systems, scales, tuning, intervals, consonance, dissonance, durational proportions, the acoustics of pitch systems, composition, performance, orchestration, ornamentation, improvisation, electronic sound production, etc.\n\n\n=== Pitch ===\n\nPitch is the lowness or highness of a tone, for example the difference between middle C and a higher C. The frequency of the sound waves producing a pitch can be measured precisely, but the perception of pitch is more complex because single notes from natural sources are usually a complex mix of many frequencies. Accordingly, theorists often describe pitch as a subjective sensation rather than an objective measurement of sound.\nSpecific frequencies are often assigned letter names. Today most orchestras assign concert A (the A above middle C on the piano) to the frequency of 440 Hz. This assignment is somewhat arbitrary; for example, in 1859 France, the same A was tuned to 435 Hz. Such differences can have a noticeable effect on the timbre of instruments and other phenomena. Thus, in historically informed performance of older music, tuning is often set to match the tuning used in the period when it was written. Additionally, many cultures do not attempt to standardize pitch, often considering that it should be allowed to vary depending on genre, style, mood, etc.\nThe difference in pitch between two notes is called an interval. The most basic interval is the unison, which is simply two notes of the same pitch. The octave interval is two pitches that are either double or half the frequency of one another. The unique characteristics of octaves gave rise to the concept of pitch class: pitches of the same letter name that occur in different octaves may be grouped into a single \"class\" by ignoring the difference in octave. For example, a high C and a low C are members of the same pitch class—the class that contains all C's.\nMusical tuning systems, or temperaments, determine the precise size of intervals. Tuning systems vary widely within and between world cultures. In Western culture, there have long been several competing tuning systems, all with different qualities. Internationally, the system known as equal temperament is most commonly used today because it is considered the most satisfactory compromise that allows instruments of fixed tuning (e.g. the piano) to sound acceptably in tune in all keys.\n\n\n=== Scales and modes ===\n\nNotes can be arranged in a variety of scales and modes. Western music theory generally divides the octave into a series of twelve pitches, called a chromatic scale, within which the interval between adjacent tones is called a semitone, or half step. Selecting tones from this set of 12 and arranging them in patterns of semitones and whole tones creates other scales.\nThe most commonly encountered scales are the seven-toned major, the harmonic minor, the melodic minor, and the natural minor. Other examples of scales are the octatonic scale and the pentatonic or five-tone scale, which is common in folk music and blues. Non-Western cultures often use scales that do not correspond with an equally divided twelve-tone division of the octave. For example, classical Ottoman, Persian, Indian and Arabic musical systems often make use of multiples of quarter tones (half the size of a semitone, as the name indicates), for instance in 'neutral' seconds (three quarter tones) or 'neutral' thirds (seven quarter tones)—they do not normally use the quarter tone itself as a direct interval.\nIn traditional Western notation, the scale used for a composition is usually indicated by a key signature at the beginning to designate the pitches that make up that scale. As the music progresses, the pitches used may change and introduce a different scale. Music can be transposed from one scale to another for various purposes, often to accommodate the range of a vocalist. Such transposition raises or lowers the overall pitch range, but preserves the intervallic relationships of the original scale. For example, transposition from the key of C major to D major raises all pitches of the scale of C major equally by a whole tone. Since the interval relationships remain unchanged, transposition may be unnoticed by a listener, however other qualities may change noticeably because transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music. This often affects the music's overall sound, as well as having technical implications for the performers.\nThe interrelationship of the keys most commonly used in Western tonal music is conveniently shown by the circle of fifths. Unique key signatures are also sometimes devised for a particular composition. During the Baroque period, emotional associations with specific keys, known as the doctrine of the affections, were an important topic in music theory, but the unique tonal colorings of keys that gave rise to that doctrine were largely erased with the adoption of equal temperament. However, many musicians continue to feel that certain keys are more appropriate to certain emotions than others. Indian classical music theory continues to strongly associate keys with emotional states, times of day, and other extra-musical concepts and notably, does not employ equal temperament.\n\n\n=== Consonance and dissonance ===\n\nConsonance and dissonance are subjective qualities of the sonority of intervals that vary widely in different cultures and over the ages. Consonance (or concord) is the quality of an interval or chord that seems stable and complete in itself. Dissonance (or discord) is the opposite in that it feels incomplete and \"wants to\" resolve to a consonant interval. Dissonant intervals seem to clash. Consonant intervals seem to sound comfortable together. Commonly, perfect fourths, fifths, and octaves and all major and minor thirds and sixths are considered consonant. All others are dissonant to a greater or lesser degree.\nContext and many other aspects can affect apparent dissonance and consonance. For example, in a Debussy prelude, a major second may sound stable and consonant, while the same interval may sound dissonant in a Bach fugue. In the Common practice era, the perfect fourth is considered dissonant when not supported by a lower third or fifth. Since the early 20th century, Arnold Schoenberg's concept of \"emancipated\" dissonance, in which traditionally dissonant intervals can be treated as \"higher,\" more remote consonances, has become more widely accepted.\n\n\n=== Rhythm ===\n\nRhythm is produced by the sequential arrangement of sounds and silences in time. Meter measures music in regular pulse groupings, called measures or bars. The time signature or meter signature specifies how many beats are in a measure, and which value of written note is counted or felt as a single beat.\nThrough increased stress, or variations in duration or articulation, particular tones may be accented. There are conventions in most musical traditions for regular and hierarchical accentuation of beats to reinforce a given meter. Syncopated rhythms contradict those conventions by accenting unexpected parts of the beat. Playing simultaneous rhythms in more than one time signature is called polyrhythm.\nIn recent years, rhythm and meter have become an important area of research among music scholars. The most highly cited of these recent scholars are Maury Yeston, Fred Lerdahl and Ray Jackendoff, Jonathan Kramer, and Justin London.\n\n\n=== Melody ===\n\nA melody is a group of musical sounds in agreeable succession or arrangement. Because melody is such a prominent aspect in so much music, its construction and other qualities are a primary interest of music theory.\nThe basic elements of melody are pitch, duration, rhythm, and tempo. The tones of a melody are usually drawn from pitch systems such as scales or modes. Melody may consist, to increasing degree, of the figure, motive, semi-phrase, antecedent and consequent phrase, and period or sentence. The period may be considered the complete melody, however some examples combine two periods, or use other combinations of constituents to create larger form melodies.\n\n\n=== Chord ===\n\nA chord, in music, is any harmonic set of three or more notes that is heard as if sounding simultaneously.: pp. 67, 359 : p. 63  These need not actually be played together: arpeggios and broken chords may, for many practical and theoretical purposes, constitute chords. Chords and sequences of chords are frequently used in modern Western, West African, and Oceanian music, whereas they are absent from the music of many other parts of the world.: p. 15 \nThe most frequently encountered chords are triads, so called because they consist of three distinct notes: further notes may be added to give seventh chords, extended chords, or added tone chords. The most common chords are the major and minor triads and then the augmented and diminished triads. The descriptions major, minor, augmented, and diminished are sometimes referred to collectively as chordal quality. Chords are also commonly classed by their root note—so, for instance, the chord C major may be described as a triad of major quality built on the note C. Chords may also be classified by inversion, the order in which the notes are stacked.\nA series of chords is called a chord progression. Although any chord may in principle be followed by any other chord, certain patterns of chords have been accepted as establishing key in common-practice harmony. To describe this, chords are numbered, using Roman numerals (upward from the key-note), per their diatonic function. Common ways of notating or representing chords in western music other than conventional staff notation include Roman numerals, figured bass (much used in the Baroque era), chord letters (sometimes used in modern musicology), and various systems of chord charts typically found in the lead sheets used in popular music to lay out the sequence of chords so that the musician may play accompaniment chords or improvise a solo.\n\n\n=== Harmony ===\n\nIn music, harmony is the use of simultaneous pitches (tones, notes), or chords.: p. 15  The study of harmony involves chords and their construction and chord progressions and the principles of connection that govern them. Harmony is often said to refer to the \"vertical\" aspect of music, as distinguished from melodic line, or the \"horizontal\" aspect. Counterpoint, which refers to the interweaving of melodic lines, and polyphony, which refers to the relationship of separate independent voices, is thus sometimes distinguished from harmony.\nIn popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. For example, a lead sheet may indicate chords such as C major, D minor, and G dominant seventh. In many types of music, notably Baroque, Romantic, modern, and jazz, chords are often augmented with \"tensions\". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. It is part of a chord, but is not one of the chord tones (1 3 5 7). Typically, in the classical common practice period a dissonant chord (chord with tension) \"resolves\" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between \"tense\" and \"relaxed\" moments.\n\n\n=== Timbre ===\n\nTimbre, sometimes called \"color\", or \"tone color,\" is the principal phenomenon that allows us to distinguish one instrument from another when both play at the same pitch and volume, a quality of a voice or instrument often described in terms like bright, dull, shrill, etc. It is of considerable interest in music theory, especially because it is one component of music that has as yet, no standardized nomenclature. It has been called \"... the psychoacoustician's multidimensional waste-basket category for everything that cannot be labeled pitch or loudness,\" but can be accurately described and analyzed by Fourier analysis and other methods because it results from the combination of all sound frequencies, attack and release envelopes, and other qualities that a tone comprises.\nTimbre is principally determined by two things: (1) the relative balance of overtones produced by a given instrument due its construction (e.g. shape, material), and (2) the envelope of the sound (including changes in the overtone structure over time). Timbre varies widely between different instruments, voices, and to lesser degree, between instruments of the same type due to variations in their construction, and significantly, the performer's technique. The timbre of most instruments can be changed by employing different techniques while playing. For example, the timbre of a trumpet changes when a mute is inserted into the bell, the player changes their embouchure, or volume.\nA voice can change its timbre by the way the performer manipulates their vocal apparatus, (e.g. the shape of the vocal cavity or mouth). Musical notation frequently specifies alteration in timbre by changes in sounding technique, volume, accent, and other means. These are indicated variously by symbolic and verbal instruction. For example, the word dolce (sweetly) indicates a non-specific, but commonly understood soft and \"sweet\" timbre. Sul tasto instructs a string player to bow near or over the fingerboard to produce a less brilliant sound. Cuivre instructs a brass player to produce a forced and stridently brassy sound. Accent symbols like marcato (^) and dynamic indications (pp) can also indicate changes in timbre.\n\n\n==== Dynamics ====\n\nIn music, \"dynamics\" normally refers to variations of intensity or volume, as may be measured by physicists and audio engineers in decibels or phons. In music notation, however, dynamics are not treated as absolute values, but as relative ones. Because they are usually measured subjectively, there are factors besides amplitude that affect the performance or perception of intensity, such as timbre, vibrato, and articulation.\nThe conventional indications of dynamics are abbreviations for Italian words like forte (f) for loud and piano (p) for soft. These two basic notations are modified by indications including mezzo piano (mp) for moderately soft (literally \"half soft\") and mezzo forte (mf) for moderately loud, sforzando or sforzato (sfz) for a surging or \"pushed\" attack, or fortepiano (fp) for a loud attack with a sudden decrease to a soft level. The full span of these markings usually range from a nearly inaudible pianissississimo (pppp) to a loud-as-possible fortissississimo (ffff).\nGreater extremes of pppppp and fffff and nuances such as p+ or più piano are sometimes found. Other systems of indicating volume are also used in both notation and analysis: dB (decibels), numerical scales, colored or different sized notes, words in languages other than Italian, and symbols such as those for progressively increasing volume (crescendo) or decreasing volume (diminuendo or decrescendo), often called \"hairpins\" when indicated with diverging or converging lines as shown in the graphic above.\n\n\n==== Articulation ====\n\nArticulation is the way the performer sounds notes. For example, staccato is the shortening of duration compared to the written note value, legato performs the notes in a smoothly joined sequence with no separation. Articulation is often described rather than quantified, therefore there is room to interpret how to execute precisely each articulation.\nFor example, staccato is often referred to as \"separated\" or \"detached\" rather than having a defined or numbered amount by which to reduce the notated duration. Violin players use a variety of techniques to perform different qualities of staccato. The manner in which a performer decides to execute a given articulation is usually based on the context of the piece or phrase, but many articulation symbols and verbal instructions depend on the instrument and musical period (e.g. viol, wind; classical, baroque; etc.).\nThere is a set of articulations that most instruments and voices perform in common. They are—from long to short: legato (smooth, connected); tenuto (pressed or played to full notated duration); marcato (accented and detached); staccato (\"separated\", \"detached\"); martelé (heavily accented or \"hammered\"). Many of these can be combined to create certain \"in-between\" articulations. For example, portato is the combination of tenuto and staccato. Some instruments have unique methods by which to produce sounds, such as spiccato for bowed strings, where the bow bounces off the string.\n\n\n=== Texture ===\n\nIn music, texture is how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall quality of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices. For example, a thick texture contains many \"layers\" of instruments. One of these layers could be a string section, or another brass.\nThe thickness also is affected by the number and the richness of the instruments playing the piece. The thickness varies from light to thick. A lightly textured piece will have light, sparse scoring. A thickly or heavily textured piece will be scored for many instruments. A piece's texture may be affected by the number and character of parts playing at once, the timbre of the instruments or voices playing these parts and the harmony, tempo, and rhythms used. The types categorized by number and relationship of parts are analyzed and determined through the labeling of primary textural elements: primary melody, secondary melody, parallel supporting melody, static support, harmonic support, rhythmic support, and harmonic and rhythmic support.\nCommon types included monophonic texture (a single melodic voice, such as a piece for solo soprano or solo flute), biphonic texture (two melodic voices, such as a duo for bassoon and flute in which the bassoon plays a drone note and the flute plays the melody), polyphonic texture and homophonic texture (chords accompanying a melody).\n\n\n=== Form or structure ===\n\nThe term musical form (or musical architecture) refers to the overall structure or plan of a piece of music, and it describes the layout of a composition as divided into sections. In the tenth edition of The Oxford Companion to Music, Percy Scholes defines musical form as \"a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration.\" According to Richard Middleton, musical form is \"the shape or structure of the work.\" He describes it through difference: the distance moved from a repeat; the latter being the smallest difference. Difference is quantitative and qualitative: how far, and of what type, different. In many cases, form depends on statement and restatement, unity and variety, and contrast and connection.\n\n\n=== Expression ===\n\nMusical expression is the art of playing or singing music with emotional communication. The elements of music that comprise expression include dynamic indications, such as forte or piano, phrasing, differing qualities of timbre and articulation, color, intensity, energy and excitement. All of these devices can be incorporated by the performer. A performer aims to elicit responses of sympathetic feeling in the audience, and to excite, calm or otherwise sway the audience's physical and emotional responses. Musical expression is sometimes thought to be produced by a combination of other parameters, and sometimes described as a transcendent quality that is more than the sum of measurable quantities such as pitch or duration.\nExpression on instruments can be closely related to the role of the breath in singing, and the voice's natural ability to express feelings, sentiment and deep emotions. Whether these can somehow be categorized is perhaps the realm of academics, who view expression as an element of musical performance that embodies a consistently recognizable emotion, ideally causing a sympathetic emotional response in its listeners. The emotional content of musical expression is distinct from the emotional content of specific sounds (e.g., a startlingly-loud 'bang') and of learned associations (e.g., a national anthem), but can rarely be completely separated from its context.\nThe components of musical expression continue to be the subject of extensive and unresolved dispute.\n\n\n=== Notation ===\n\nMusical notation is the written or symbolized representation of music. This is most often achieved by the use of commonly understood graphic symbols and written verbal instructions and their abbreviations. There are many systems of music notation from different cultures and different ages. Traditional Western notation evolved during the Middle Ages and remains an area of experimentation and innovation. In the 2000s, computer file formats have become important as well. Spoken language and hand signs are also used to symbolically represent music, primarily in teaching.\nIn standard Western music notation, tones are represented graphically by symbols (notes) placed on a staff or staves, the vertical axis corresponding to pitch and the horizontal axis corresponding to time. Note head shapes, stems, flags, ties and dots are used to indicate duration. Additional symbols indicate keys, dynamics, accents, rests, etc. Verbal instructions from the conductor are often used to indicate tempo, technique, and other aspects.\nIn Western music, a range of different music notation systems are used. In Western Classical music, conductors use printed scores that show all of the instruments' parts and orchestra members read parts with their musical lines written out. In popular styles of music, much less of the music may be notated. A rock band may go into a recording session with just a handwritten chord chart indicating the song's chord progression using chord names (e.g., C major, D minor, G7, etc.). All of the chord voicings, rhythms and accompaniment figures are improvised by the band members.\n\n\n== As academic discipline ==\nThe scholarly study of music theory in the twentieth century has a number of different subfields, each of which takes a different perspective on what are the primary phenomenon of interest and the most useful methods for investigation.\n\n\n=== Analysis ===\n\nMusical analysis is the attempt to answer the question how does this music work? The method employed to answer this question, and indeed exactly what is meant by the question, differs from analyst to analyst, and according to the purpose of the analysis. According to Ian Bent, \"analysis, as a pursuit in its own right, came to be established only in the late 19th century; its emergence as an approach and method can be traced back to the 1750s. However, it existed as a scholarly tool, albeit an auxiliary one, from the Middle Ages onwards.\" Adolf Bernhard Marx was influential in formalising concepts about composition and music understanding towards the second half of the 19th century. The principle of analysis has been variously criticized, especially by composers, such as Edgard Varèse's claim that, \"to explain by means of [analysis] is to decompose, to mutilate the spirit of a work\".\nSchenkerian analysis is a method of musical analysis of tonal music based on the theories of Heinrich Schenker (1868–1935). The goal of a Schenkerian analysis is to interpret the underlying structure of a tonal work and to help reading the score according to that structure. The theory's basic tenets can be viewed as a way of defining tonality in music. A Schenkerian analysis of a passage of music shows hierarchical relationships among its pitches, and draws conclusions about the structure of the passage from this hierarchy. The analysis makes use of a specialized symbolic form of musical notation that Schenker devised to demonstrate various techniques of elaboration. The most fundamental concept of Schenker's theory of tonality may be that of tonal space. The intervals between the notes of the tonic triad form a tonal space that is filled with passing and neighbour notes, producing new triads and new tonal spaces, open for further elaborations until the surface of the work (the score) is reached.\nAlthough Schenker himself usually presents his analyses in the generative direction, starting from the fundamental structure (Ursatz) to reach the score, the practice of Schenkerian analysis more often is reductive, starting from the score and showing how it can be reduced to its fundamental structure. The graph of the Ursatz is arrhythmic, as is a strict-counterpoint cantus firmus exercise. Even at intermediate levels of the reduction, rhythmic notation (open and closed noteheads, beams and flags) shows not rhythm but the hierarchical relationships between the pitch-events. Schenkerian analysis is subjective. There is no mechanical procedure involved and the analysis reflects the musical intuitions of the analyst. The analysis represents a way of hearing (and reading) a piece of music.\nTransformational theory is a branch of music theory developed by David Lewin in the 1980s, and formally introduced in his 1987 work, Generalized Musical Intervals and Transformations. The theory, which models musical transformations as elements of a mathematical group, can be used to analyze both tonal and atonal music. The goal of transformational theory is to change the focus from musical objects—such as the \"C major chord\" or \"G major chord\"—to relations between objects. Thus, instead of saying that a C major chord is followed by G major, a transformational theorist might say that the first chord has been \"transformed\" into the second by the \"Dominant operation.\" (Symbolically, one might write \"Dominant(C major) = G major.\") While traditional musical set theory focuses on the makeup of musical objects, transformational theory focuses on the intervals or types of musical motion that can occur. According to Lewin's description of this change in emphasis, \"[The transformational] attitude does not ask for some observed measure of extension between reified 'points'; rather it asks: 'If I am at s and wish to get to t, what characteristic gesture should I perform in order to arrive there?'\"\n\n\n=== Music perception and cognition ===\n\nMusic psychology or the psychology of music may be regarded as a branch of both psychology and musicology. It aims to explain and understand musical behavior and experience, including the processes through which music is perceived, created, responded to, and incorporated into everyday life. Modern music psychology is primarily empirical; its knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. Music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.\nMusic psychology can shed light on non-psychological aspects of musicology and musical practice. For example, it contributes to music theory through investigations of the perception and computational modelling of musical structures such as melody, harmony, tonality, rhythm, meter, and form. Research in music history can benefit from systematic study of the history of musical syntax, or from psychological analyses of composers and compositions in relation to perceptual, affective, and social responses to their music.\n\n\n=== Genre and technique ===\n\nA music genre is a conventional category that identifies some pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from musical form and musical style, although in practice these terms are sometimes used interchangeably.\nMusic can be divided into different genres in many different ways. The artistic nature of music means that these classifications are often subjective and controversial, and some genres may overlap. There are even varying academic definitions of the term genre itself. In his book Form in Tonal Music, Douglass M. Green distinguishes between genre and form. He lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. To further clarify the meaning of genre, Green writes, \"Beethoven's Op. 61 and Mendelssohn's Op. 64 are identical in genre—both are violin concertos—but different in form. However, Mozart's Rondo for Piano, K. 511, and the Agnus Dei from his Mass, K. 317 are quite different in genre but happen to be similar in form.\" Some, like Peter van der Merwe, treat the terms genre and style as the same, saying that genre should be defined as pieces of music that came from the same style or \"basic musical language.\"\nOthers, such as Allan F. Moore, state that genre and style are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres. A music genre or subgenre may also be defined by the musical techniques, the style, the cultural context, and the content and spirit of the themes. Geographical origin is sometimes used to identify a music genre, though a single geographical category will often include a wide variety of subgenres. Timothy Laurie argues that \"since the early 1980s, genre has graduated from being a subset of popular music studies to being an almost ubiquitous framework for constituting and evaluating musical research objects\".\nMusical technique is the ability of instrumental and vocal musicians to exert optimal control of their instruments or vocal cords to produce precise musical effects. Improving technique generally entails practicing exercises that improve muscular sensitivity and agility. To improve technique, musicians often practice fundamental patterns of notes such as the natural, minor, major, and chromatic scales, minor and major triads, dominant and diminished sevenths, formula patterns and arpeggios. For example, triads and sevenths teach how to play chords with accuracy and speed. Scales teach how to move quickly and gracefully from one note to another (usually by step). Arpeggios teach how to play broken chords over larger intervals. Many of these components of music are found in compositions, for example, a scale is a very common element of classical and romantic era compositions.\nHeinrich Schenker argued that musical technique's \"most striking and distinctive characteristic\" is repetition. Works known as études (meaning \"study\") are also frequently used for the improvement of technique.\n\n\n=== Mathematics ===\n\nMusic theorists sometimes use mathematics to understand music, and although music has no axiomatic foundation in modern mathematics, mathematics is \"the basis of sound\" and sound itself \"in its musical aspects... exhibits a remarkable array of number properties\", simply because nature itself \"is amazingly mathematical\". The attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work. There is a long history of examining the relationships between music and mathematics. Though ancient Chinese, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios.\n\nIn the modern era, musical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set. Expanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.\n\n\n=== Serial composition and set theory ===\n\nIn music theory, serialism is a method or technique of composition that uses a series of values to manipulate different musical elements. Serialism began primarily with Arnold Schoenberg's twelve-tone technique, though his contemporaries were also working to establish serialism as one example of post-tonal thinking. Twelve-tone technique orders the twelve notes of the chromatic scale, forming a row or series and providing a unifying basis for a composition's melody, harmony, structural progressions, and variations. Other types of serialism also work with sets, collections of objects, but not necessarily with fixed-order series, and extend the technique to other musical dimensions (often called \"parameters\"), such as duration, dynamics, and timbre. The idea of serialism is also applied in various ways in the visual arts, design, and architecture\n\"Integral serialism\" or \"total serialism\" is the use of series for aspects such as duration, dynamics, and register as well as pitch. Other terms, used especially in Europe to distinguish post-World War II serial music from twelve-tone music and its American extensions, are \"general serialism\" and \"multiple serialism\".\nMusical set theory provides concepts for categorizing musical objects and describing their relationships. Many of the notions were first elaborated by Howard Hanson (1960) in connection with tonal music, and then mostly developed in connection with atonal music by theorists such as Allen Forte (1973), drawing on the work in twelve-tone theory of Milton Babbitt. The concepts of set theory are very general and can be applied to tonal and atonal styles in any equally tempered tuning system, and to some extent more generally than that.\nOne branch of musical set theory deals with collections (sets and permutations) of pitches and pitch classes (pitch-class set theory), which may be ordered or unordered, and can be related by musical operations such as transposition, inversion, and complementation. The methods of musical set theory are sometimes applied to the analysis of rhythm as well.\n\n\n=== Musical semiotics ===\n\nMusic semiology (semiotics) is the study of signs as they pertain to music on a variety of levels. Following Roman Jakobson, Kofi Agawu adopts the idea of musical semiosis being introversive or extroversive—that is, musical signs within a text and without. \"Topics\", or various musical conventions (such as horn calls, dance forms, and styles), have been treated suggestively by Agawu, among others. The notion of gesture is beginning to play a large role in musico-semiotic enquiry.\n\n\"There are strong arguments that music inhabits a semiological realm which, on both ontogenetic and phylogenetic levels, has developmental priority over verbal language.\"\nWriters on music semiology include Kofi Agawu (on topical theory, Heinrich Schenker, Robert Hatten (on topic, gesture), Raymond Monelle (on topic, musical meaning), Jean-Jacques Nattiez (on introversive taxonomic analysis and ethnomusicological applications), Anthony Newcomb (on narrativity), and Eero Tarasti.\nRoland Barthes, himself a semiotician and skilled amateur pianist, wrote about music in Image-Music-Text, The Responsibilities of Form, and Eiffel Tower, though he did not consider music to be a semiotic system.\nSigns, meanings in music, happen essentially through the connotations of sounds, and through the social construction, appropriation and amplification of certain meanings associated with these connotations. The work of Philip Tagg (Ten Little Tunes, Fernando the Flute, Music's Meanings) provides one of the most complete and systematic analysis of the relation between musical structures and connotations in western and especially popular, television and film music. The work of Leonard B. Meyer in Style and Music theorizes the relationship between ideologies and musical structures and the phenomena of style change, and focuses on romanticism as a case study.\n\n\n=== Education and careers ===\n\nMusic theory in the practical sense has been a part of education at conservatories and music schools for centuries, but the status music theory currently has within academic institutions is relatively recent. In the 1970s, few universities had dedicated music theory programs, many music theorists had been trained as composers or historians, and there was a belief among theorists that the teaching of music theory was inadequate and that the subject was not properly recognised as a scholarly discipline in its own right. A growing number of scholars began promoting the idea that music theory should be taught by theorists, rather than composers, performers or music historians. This led to the founding of the Society for Music Theory in the United States in 1977. In Europe, the French Société d'Analyse musicale was founded in 1985. It called the First European Conference of Music Analysis for 1989, which resulted in the foundation of the Société belge d'Analyse musicale in Belgium and the Gruppo analisi e teoria musicale in Italy the same year, the Society for Music Analysis in the UK in 1991, the Vereniging voor Muziektheorie in the Netherlands in 1999 and the Gesellschaft für Musiktheorie in Germany in 2000. They were later followed by the Russian Society for Music Theory in 2013, the Polish Society for Music Analysis in 2015 and the Sociedad de Análisis y Teoría Musical in Spain in 2020, and others are in construction. These societies coordinate the publication of music theory scholarship and support the professional development of music theory researchers. They formed in 2018 a network of European societies for Theory and/or Analysis of Music, the EuroT&AM\nAs part of their initial training, music theorists will typically complete a B.Mus or a B.A. in music (or a related field) and in many cases an M.A. in music theory. Some individuals apply directly from a bachelor's degree to a PhD, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for music theory PhD programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus. and a Masters in Music Composition or Philosophy of Music).\nMost music theorists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure-track professor positions is very competitive: with an average of around 25 tenure-track positions advertised per year in the past decade, 80–100 PhD graduates are produced each year (according to the Survey of Earned Doctorates) who compete not only with each other for those positions but with job seekers that received PhD's in previous years who are still searching for a tenure-track job. Applicants must hold a completed PhD or the equivalent degree (or expect to receive one within a year of being hired—called an \"ABD\", for \"All But Dissertation\" stage) and (for more senior positions) have a strong record of publishing in peer-reviewed journals. Some PhD-holding music theorists are only able to find insecure positions as sessional lecturers. The job tasks of a music theorist are the same as those of a professor in any other humanities discipline: teaching undergraduate and/or graduate classes in this area of specialization and, in many cases some general courses (such as Music appreciation or Introduction to Music Theory), conducting research in this area of expertise, publishing research articles in peer-reviewed journals, authoring book chapters, books or textbooks, traveling to conferences to present papers and learn about research in the field, and, if the program includes a graduate school, supervising M.A. and PhD students and giving them guidance on the preparation of their theses and dissertations. Some music theory professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.\n\n\n== See also ==\nList of music theorists\nMusic psychology\nMusicology\nTheory of painting\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\nDillen, Oscar van, Outline of basic music theory (2011)"
    }
]